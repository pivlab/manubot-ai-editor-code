{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070a050a-44d1-42c2-b274-c8c180a2a0ba",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.018111,
     "end_time": "2024-03-20T11:42:48.863121",
     "exception": false,
     "start_time": "2024-03-20T11:42:48.845010",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fcbb4d-aa04-4e1a-9aee-0053c559e157",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011506,
     "end_time": "2024-03-20T11:42:48.886615",
     "exception": false,
     "start_time": "2024-03-20T11:42:48.875109",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This notebook reads a PR from a manuscript and matches original paragraphs with modified ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237f234-dc98-43e9-8ff5-8922c410c377",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011513,
     "end_time": "2024-03-20T11:42:48.909740",
     "exception": false,
     "start_time": "2024-03-20T11:42:48.898227",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70413318-1f5e-4d58-b1a2-cb4f01c13e86",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:48.933747Z",
     "iopub.status.busy": "2024-03-20T11:42:48.933634Z",
     "iopub.status.idle": "2024-03-20T11:42:49.530145Z",
     "shell.execute_reply": "2024-03-20T11:42:49.529828Z"
    },
    "papermill": {
     "duration": 0.609574,
     "end_time": "2024-03-20T11:42:49.530921",
     "exception": false,
     "start_time": "2024-03-20T11:42:48.921347",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from github import Auth, Github\n",
    "from IPython.display import display\n",
    "from proj import conf\n",
    "from proj.utils import process_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f78fd18-86b7-40b9-b80e-a8b18d001682",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011739,
     "end_time": "2024-03-20T11:42:49.554755",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.543016",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Settings/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a3ceca-1480-45b0-b04d-cb402ff95abf",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.578567Z",
     "iopub.status.busy": "2024-03-20T11:42:49.578432Z",
     "iopub.status.idle": "2024-03-20T11:42:49.580137Z",
     "shell.execute_reply": "2024-03-20T11:42:49.579990Z"
    },
    "papermill": {
     "duration": 0.014404,
     "end_time": "2024-03-20T11:42:49.580790",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.566386",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "REPO = \"pivlab/manubot-ai-editor-code-test-biochatter-manuscript\"\n",
    "PR = (2, \"gpt-3.5-turbo\")\n",
    "\n",
    "OUTPUT_FILE_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e669ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.604238Z",
     "iopub.status.busy": "2024-03-20T11:42:49.604163Z",
     "iopub.status.idle": "2024-03-20T11:42:49.605709Z",
     "shell.execute_reply": "2024-03-20T11:42:49.605560Z"
    },
    "papermill": {
     "duration": 0.013909,
     "end_time": "2024-03-20T11:42:49.606251",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.592342",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "OUTPUT_FILE_PATH = \"/home/miltondp/projects/others/manubot/manubot-ai-editor-code/base/results/paragraph_match/biochatter-manuscript--gpt-3.5-turbo.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71351755-ed5f-43a2-a886-6ef08afaddf4",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.629685Z",
     "iopub.status.busy": "2024-03-20T11:42:49.629606Z",
     "iopub.status.idle": "2024-03-20T11:42:49.632501Z",
     "shell.execute_reply": "2024-03-20T11:42:49.632359Z"
    },
    "papermill": {
     "duration": 0.015269,
     "end_time": "2024-03-20T11:42:49.633004",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.617735",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/miltondp/projects/others/manubot/manubot-ai-editor-code/base/results/paragraph_match/biochatter-manuscript--gpt-3.5-turbo.pkl')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_FILE_PATH = Path(OUTPUT_FILE_PATH).resolve()\n",
    "OUTPUT_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "display(OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b83b5-84b7-45ff-88ee-3651aa5c9c55",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011541,
     "end_time": "2024-03-20T11:42:49.656165",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.644624",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Get Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1d1f476-f5b5-4d1c-9c3a-822e9bef4b27",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.679744Z",
     "iopub.status.busy": "2024-03-20T11:42:49.679667Z",
     "iopub.status.idle": "2024-03-20T11:42:49.681264Z",
     "shell.execute_reply": "2024-03-20T11:42:49.681098Z"
    },
    "papermill": {
     "duration": 0.014007,
     "end_time": "2024-03-20T11:42:49.681768",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.667761",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = Auth.Token(conf.github.API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3db6e2-f267-4d60-8ff3-703698a09f88",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.705453Z",
     "iopub.status.busy": "2024-03-20T11:42:49.705385Z",
     "iopub.status.idle": "2024-03-20T11:42:49.707023Z",
     "shell.execute_reply": "2024-03-20T11:42:49.706854Z"
    },
    "papermill": {
     "duration": 0.013946,
     "end_time": "2024-03-20T11:42:49.707415",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.693469",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = Github(auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6a365d-59f5-4537-9fd7-869ee4156046",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:49.731180Z",
     "iopub.status.busy": "2024-03-20T11:42:49.731104Z",
     "iopub.status.idle": "2024-03-20T11:42:50.018316Z",
     "shell.execute_reply": "2024-03-20T11:42:50.017258Z"
    },
    "papermill": {
     "duration": 0.301986,
     "end_time": "2024-03-20T11:42:50.021071",
     "exception": false,
     "start_time": "2024-03-20T11:42:49.719085",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "repo = g.get_repo(REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9fb5d-6838-484b-9202-ceac74fcb336",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011891,
     "end_time": "2024-03-20T11:42:50.060404",
     "exception": false,
     "start_time": "2024-03-20T11:42:50.048513",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Get Pull Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "297c9dec-9e00-490d-8723-44208b3fe8f4",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:50.084514Z",
     "iopub.status.busy": "2024-03-20T11:42:50.084396Z",
     "iopub.status.idle": "2024-03-20T11:42:50.472222Z",
     "shell.execute_reply": "2024-03-20T11:42:50.471627Z"
    },
    "papermill": {
     "duration": 0.401356,
     "end_time": "2024-03-20T11:42:50.473489",
     "exception": false,
     "start_time": "2024-03-20T11:42:50.072133",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pr = repo.get_pull(PR[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064b1017-a495-48b3-bea3-1b9a67ad1b6f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:50.500312Z",
     "iopub.status.busy": "2024-03-20T11:42:50.500220Z",
     "iopub.status.idle": "2024-03-20T11:42:50.884637Z",
     "shell.execute_reply": "2024-03-20T11:42:50.883435Z"
    },
    "papermill": {
     "duration": 0.399114,
     "end_time": "2024-03-20T11:42:50.887198",
     "exception": false,
     "start_time": "2024-03-20T11:42:50.488084",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File(sha=\"35c709fec248ca8fbc5c4254294c126da6879707\", filename=\"content/01.abstract.md\"),\n",
       " File(sha=\"d2b401a7c0b5352b4f8299fe7822e46f67723289\", filename=\"content/10.introduction.md\"),\n",
       " File(sha=\"fcdb707a8ee8f2b88f2e385bfa65081e6a96d05b\", filename=\"content/20.results.md\"),\n",
       " File(sha=\"df7cb00d842d36d076440ddabed093974e9a14b5\", filename=\"content/30.discussion.md\"),\n",
       " File(sha=\"9ec3ba88154efcb02dae746e91e93141c97df67e\", filename=\"content/40.methods.md\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pr.get_files())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc9b271-7ed2-4994-9d36-c8afa946b017",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:50.927336Z",
     "iopub.status.busy": "2024-03-20T11:42:50.927202Z",
     "iopub.status.idle": "2024-03-20T11:42:51.281740Z",
     "shell.execute_reply": "2024-03-20T11:42:51.280258Z"
    },
    "papermill": {
     "duration": 0.370151,
     "end_time": "2024-03-20T11:42:51.284539",
     "exception": false,
     "start_time": "2024-03-20T11:42:50.914388",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pr_commits = list(pr.get_commits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446a1453-5729-4687-9b93-8165dd0e2793",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.314092Z",
     "iopub.status.busy": "2024-03-20T11:42:51.313974Z",
     "iopub.status.idle": "2024-03-20T11:42:51.316504Z",
     "shell.execute_reply": "2024-03-20T11:42:51.316270Z"
    },
    "papermill": {
     "duration": 0.015647,
     "end_time": "2024-03-20T11:42:51.317071",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.301424",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Commit(sha=\"38a0c074e5eed7da6351cda324458e6f4ed4ece4\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_commits[0].parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2382a1-adcc-47dc-8c75-d34f8500f93e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.341504Z",
     "iopub.status.busy": "2024-03-20T11:42:51.341418Z",
     "iopub.status.idle": "2024-03-20T11:42:51.343290Z",
     "shell.execute_reply": "2024-03-20T11:42:51.343131Z"
    },
    "papermill": {
     "duration": 0.014742,
     "end_time": "2024-03-20T11:42:51.343660",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.328918",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38a0c074e5eed7da6351cda324458e6f4ed4ece4\n"
     ]
    }
   ],
   "source": [
    "pr_prev = pr_commits[0].parents[0].sha\n",
    "print(pr_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c9b088-a166-49d6-a0df-64d2cf690b7e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.368076Z",
     "iopub.status.busy": "2024-03-20T11:42:51.367988Z",
     "iopub.status.idle": "2024-03-20T11:42:51.369382Z",
     "shell.execute_reply": "2024-03-20T11:42:51.369243Z"
    },
    "papermill": {
     "duration": 0.013899,
     "end_time": "2024-03-20T11:42:51.369850",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.355951",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd1a546a487e1cb894d9cf11c9b873735d5f72a3\n"
     ]
    }
   ],
   "source": [
    "pr_curr = pr_commits[0].sha\n",
    "print(pr_curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10872c1e-0184-46c6-a04f-dce43438d404",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011774,
     "end_time": "2024-03-20T11:42:51.393591",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.381817",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242729cf-1778-4162-b813-249ae6c8b632",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.417610Z",
     "iopub.status.busy": "2024-03-20T11:42:51.417546Z",
     "iopub.status.idle": "2024-03-20T11:42:51.670350Z",
     "shell.execute_reply": "2024-03-20T11:42:51.669267Z"
    },
    "papermill": {
     "duration": 0.267524,
     "end_time": "2024-03-20T11:42:51.672885",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.405361",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[File(sha=\"35c709fec248ca8fbc5c4254294c126da6879707\", filename=\"content/01.abstract.md\"),\n",
       " File(sha=\"d2b401a7c0b5352b4f8299fe7822e46f67723289\", filename=\"content/10.introduction.md\"),\n",
       " File(sha=\"fcdb707a8ee8f2b88f2e385bfa65081e6a96d05b\", filename=\"content/20.results.md\"),\n",
       " File(sha=\"df7cb00d842d36d076440ddabed093974e9a14b5\", filename=\"content/30.discussion.md\"),\n",
       " File(sha=\"9ec3ba88154efcb02dae746e91e93141c97df67e\", filename=\"content/40.methods.md\")]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_files = [f for f in pr.get_files() if f.filename.endswith(\".md\")]\n",
    "display(pr_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce10792-ba0d-4716-9dcd-b44c2eb668d4",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011915,
     "end_time": "2024-03-20T11:42:51.711753",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.699838",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6253b146-7bd2-44ff-9ccc-6f245a11dd23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.736122Z",
     "iopub.status.busy": "2024-03-20T11:42:51.736022Z",
     "iopub.status.idle": "2024-03-20T11:42:51.737728Z",
     "shell.execute_reply": "2024-03-20T11:42:51.737495Z"
    },
    "papermill": {
     "duration": 0.014606,
     "end_time": "2024-03-20T11:42:51.738264",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.723658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: explain variable\n",
    "paragraph_matches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c403bcf-5c79-4bde-adfe-a09464b8b076",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011909,
     "end_time": "2024-03-20T11:42:51.762082",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.750173",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c13a99f-54e2-463e-9d20-d43a89c9ef76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.799107Z",
     "iopub.status.busy": "2024-03-20T11:42:51.799021Z",
     "iopub.status.idle": "2024-03-20T11:42:51.800733Z",
     "shell.execute_reply": "2024-03-20T11:42:51.800573Z"
    },
    "papermill": {
     "duration": 0.023185,
     "end_time": "2024-03-20T11:42:51.801300",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.778115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_name = \"abstract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1558f08b-19a8-4951-8dea-228d7f59099d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.827723Z",
     "iopub.status.busy": "2024-03-20T11:42:51.827304Z",
     "iopub.status.idle": "2024-03-20T11:42:51.829070Z",
     "shell.execute_reply": "2024-03-20T11:42:51.828915Z"
    },
    "papermill": {
     "duration": 0.014548,
     "end_time": "2024-03-20T11:42:51.829553",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.815005",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/01.abstract.md\n"
     ]
    }
   ],
   "source": [
    "pr_filename = pr_files[0].filename\n",
    "assert section_name in pr_filename\n",
    "print(pr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efc147-ae59-4db5-990d-7d30c89ed8d0",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011691,
     "end_time": "2024-03-20T11:42:51.853044",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.841353",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47db229e-b5e4-4c9a-ae54-fc984d9fdcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:51.877399Z",
     "iopub.status.busy": "2024-03-20T11:42:51.877275Z",
     "iopub.status.idle": "2024-03-20T11:42:52.016019Z",
     "shell.execute_reply": "2024-03-20T11:42:52.014936Z"
    },
    "papermill": {
     "duration": 0.153605,
     "end_time": "2024-03-20T11:42:52.018444",
     "exception": false,
     "start_time": "2024-03-20T11:42:51.864839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Abstract\n",
      "\n",
      "Current-generation Large Language Mod\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "orig_section_content = repo.get_contents(pr_filename, pr_prev).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(orig_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6e25fa-ef56-4d1a-8127-064e5cc348aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.046265Z",
     "iopub.status.busy": "2024-03-20T11:42:52.046186Z",
     "iopub.status.idle": "2024-03-20T11:42:52.048121Z",
     "shell.execute_reply": "2024-03-20T11:42:52.047916Z"
    },
    "papermill": {
     "duration": 0.014881,
     "end_time": "2024-03-20T11:42:52.048648",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.033767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "orig_section_paragraphs = orig_section_content.split(\"\\n\\n\")\n",
    "display(len(orig_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7d0ff-91f7-4435-ac51-78c9c9ce25fa",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011765,
     "end_time": "2024-03-20T11:42:52.072285",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.060520",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12094b0-c50f-4d98-85f7-dd5072ff68e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.096343Z",
     "iopub.status.busy": "2024-03-20T11:42:52.096265Z",
     "iopub.status.idle": "2024-03-20T11:42:52.372901Z",
     "shell.execute_reply": "2024-03-20T11:42:52.371691Z"
    },
    "papermill": {
     "duration": 0.291136,
     "end_time": "2024-03-20T11:42:52.375243",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.084107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Abstract\n",
      "\n",
      "Large Language Models (LLMs) have gen\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "mod_section_content = repo.get_contents(pr_filename, pr_curr).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(mod_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e02bdeb5-77c9-4a96-adf4-a31ba8047062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.415416Z",
     "iopub.status.busy": "2024-03-20T11:42:52.415319Z",
     "iopub.status.idle": "2024-03-20T11:42:52.417161Z",
     "shell.execute_reply": "2024-03-20T11:42:52.417022Z"
    },
    "papermill": {
     "duration": 0.014802,
     "end_time": "2024-03-20T11:42:52.417607",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.402805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "mod_section_paragraphs = mod_section_content.split(\"\\n\\n\")\n",
    "display(len(mod_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbaf41a-e76e-4b5d-8daa-4db6cdb20f66",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011998,
     "end_time": "2024-03-20T11:42:52.441602",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.429604",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38ec8ddf-9d53-4e31-8a48-2946e35e6996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.465982Z",
     "iopub.status.busy": "2024-03-20T11:42:52.465871Z",
     "iopub.status.idle": "2024-03-20T11:42:52.467698Z",
     "shell.execute_reply": "2024-03-20T11:42:52.467562Z"
    },
    "papermill": {
     "duration": 0.014541,
     "end_time": "2024-03-20T11:42:52.468182",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.453641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Abstract'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b665987d-444c-49b9-8d67-1bf157225d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.492436Z",
     "iopub.status.busy": "2024-03-20T11:42:52.492360Z",
     "iopub.status.idle": "2024-03-20T11:42:52.494422Z",
     "shell.execute_reply": "2024-03-20T11:42:52.494138Z"
    },
    "papermill": {
     "duration": 0.014765,
     "end_time": "2024-03-20T11:42:52.494923",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.480158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Abstract'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0d453-ff6b-4194-bd5b-caec7a71a0ac",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.011937,
     "end_time": "2024-03-20T11:42:52.518892",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.506955",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e681b6-d79a-45d9-a414-653920159e1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.543258Z",
     "iopub.status.busy": "2024-03-20T11:42:52.543183Z",
     "iopub.status.idle": "2024-03-20T11:42:52.544727Z",
     "shell.execute_reply": "2024-03-20T11:42:52.544555Z"
    },
    "papermill": {
     "duration": 0.014453,
     "end_time": "2024-03-20T11:42:52.545274",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.530821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current-generation Large Language Models (LLMs) have stirred enormous interest in recent months, yielding great potential for accessibility and automation, while simultaneously posing significant challenges and risk of misuse. To facilitate interfacing with LLMs in the biomedical space, while at the same time safeguarding their functionalities through sensible constraints, we propose a dedicated, open-source framework: BioChatter. Based on open-source software packages, we synergise the many functionalities that are currently developing around LLMs, such as knowledge integration / retrieval-augmented generation, model chaining, and benchmarking, resulting in an easy-to-use and inclusive framework for application in many use cases of biomedicine. We focus on robust and user-friendly implementation, including ways to deploy privacy-preserving local open-source LLMs. We demonstrate use cases via two multi-purpose web apps ([https://chat.biocypher.org](https://chat.biocypher.org)), and provide documentation, support, and an open community.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[1])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1bf3ff4-c3ab-4ca8-b5d5-c3a2f6c02702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.569526Z",
     "iopub.status.busy": "2024-03-20T11:42:52.569456Z",
     "iopub.status.idle": "2024-03-20T11:42:52.571126Z",
     "shell.execute_reply": "2024-03-20T11:42:52.570962Z"
    },
    "papermill": {
     "duration": 0.01436,
     "end_time": "2024-03-20T11:42:52.571619",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.557259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) have generated significant interest due to their potential for accessibility and automation in various fields, including biomedicine. However, they also present challenges and risks of misuse. In this paper, we address the need for a framework to interface with LLMs in the biomedical domain while ensuring their safe and effective use. To meet this need, we introduce BioChatter, an open-source framework that integrates various functionalities of LLMs, such as knowledge integration, retrieval-augmented generation, model chaining, and benchmarking. By leveraging open-source software packages, we have developed a user-friendly and versatile platform that can be applied across a range of biomedicine use cases. Our focus is on implementing robust and privacy-preserving local open-source LLMs. We showcase the utility of BioChatter through two multi-purpose web apps available at [https://chat.biocypher.org](https://chat.biocypher.org) and provide comprehensive documentation, support, and a vibrant open community.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[1])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b6cfc3-c2ae-40ce-abca-de3778b445f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.595892Z",
     "iopub.status.busy": "2024-03-20T11:42:52.595811Z",
     "iopub.status.idle": "2024-03-20T11:42:52.597326Z",
     "shell.execute_reply": "2024-03-20T11:42:52.597148Z"
    },
    "papermill": {
     "duration": 0.014179,
     "end_time": "2024-03-20T11:42:52.597727",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.583548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db738a8d-d527-4607-8994-2a62433d75e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.622297Z",
     "iopub.status.busy": "2024-03-20T11:42:52.622236Z",
     "iopub.status.idle": "2024-03-20T11:42:52.624143Z",
     "shell.execute_reply": "2024-03-20T11:42:52.623999Z"
    },
    "papermill": {
     "duration": 0.014923,
     "end_time": "2024-03-20T11:42:52.624749",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.609826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abstract',\n",
       " 'Current-generation Large Language Models (LLMs) have stirred enormous interest in recent months, yielding great potential for accessibility and automation, while simultaneously posing significant challenges and risk of misuse. To facilitate interfacing with LLMs in the biomedical space, while at the same time safeguarding their functionalities through sensible constraints, we propose a dedicated, open-source framework: BioChatter. Based on open-source software packages, we synergise the many functionalities that are currently developing around LLMs, such as knowledge integration / retrieval-augmented generation, model chaining, and benchmarking, resulting in an easy-to-use and inclusive framework for application in many use cases of biomedicine. We focus on robust and user-friendly implementation, including ways to deploy privacy-preserving local open-source LLMs. We demonstrate use cases via two multi-purpose web apps ([https://chat.biocypher.org](https://chat.biocypher.org)), and provide documentation, support, and an open community.',\n",
       " 'Large Language Models (LLMs) have generated significant interest due to their potential for accessibility and automation in various fields, including biomedicine. However, they also present challenges and risks of misuse. In this paper, we address the need for a framework to interface with LLMs in the biomedical domain while ensuring their safe and effective use. To meet this need, we introduce BioChatter, an open-source framework that integrates various functionalities of LLMs, such as knowledge integration, retrieval-augmented generation, model chaining, and benchmarking. By leveraging open-source software packages, we have developed a user-friendly and versatile platform that can be applied across a range of biomedicine use cases. Our focus is on implementing robust and privacy-preserving local open-source LLMs. We showcase the utility of BioChatter through two multi-purpose web apps available at [https://chat.biocypher.org](https://chat.biocypher.org) and provide comprehensive documentation, support, and a vibrant open community.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb559e-97ba-4171-ac1c-b1b8b1837c5b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01203,
     "end_time": "2024-03-20T11:42:52.648850",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.636820",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fbfb828-2b12-42da-b724-9d81121dbda3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.673453Z",
     "iopub.status.busy": "2024-03-20T11:42:52.673362Z",
     "iopub.status.idle": "2024-03-20T11:42:52.674826Z",
     "shell.execute_reply": "2024-03-20T11:42:52.674542Z"
    },
    "papermill": {
     "duration": 0.014331,
     "end_time": "2024-03-20T11:42:52.675231",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.660900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_name = \"introduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "802ec3df-6974-42a5-b5a0-9cd621877c5f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.699757Z",
     "iopub.status.busy": "2024-03-20T11:42:52.699654Z",
     "iopub.status.idle": "2024-03-20T11:42:52.701236Z",
     "shell.execute_reply": "2024-03-20T11:42:52.701064Z"
    },
    "papermill": {
     "duration": 0.014441,
     "end_time": "2024-03-20T11:42:52.701728",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.687287",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/10.introduction.md\n"
     ]
    }
   ],
   "source": [
    "pr_filename = pr_files[1].filename\n",
    "assert section_name in pr_filename\n",
    "print(pr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb464ec-e479-47bc-a870-dbab7372c44b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012006,
     "end_time": "2024-03-20T11:42:52.725833",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.713827",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "020b21ef-3aca-4848-8717-ace1083aebf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.750471Z",
     "iopub.status.busy": "2024-03-20T11:42:52.750356Z",
     "iopub.status.idle": "2024-03-20T11:42:52.867682Z",
     "shell.execute_reply": "2024-03-20T11:42:52.866785Z"
    },
    "papermill": {
     "duration": 0.132202,
     "end_time": "2024-03-20T11:42:52.870112",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.737910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction\n",
      "\n",
      "Despite technological advances, u\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "orig_section_content = repo.get_contents(pr_filename, pr_prev).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(orig_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1612d62-3f65-43e6-b6cb-4b5538931161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.911405Z",
     "iopub.status.busy": "2024-03-20T11:42:52.911275Z",
     "iopub.status.idle": "2024-03-20T11:42:52.913389Z",
     "shell.execute_reply": "2024-03-20T11:42:52.913198Z"
    },
    "papermill": {
     "duration": 0.015609,
     "end_time": "2024-03-20T11:42:52.913883",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.898274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "orig_section_paragraphs = orig_section_content.split(\"\\n\\n\")\n",
    "display(len(orig_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13fe8d-f094-4da4-b2dc-245f595ab964",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012258,
     "end_time": "2024-03-20T11:42:52.938907",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.926649",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61d1c4b1-c798-433e-9dc4-18313701a1d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:52.964649Z",
     "iopub.status.busy": "2024-03-20T11:42:52.964224Z",
     "iopub.status.idle": "2024-03-20T11:42:53.252530Z",
     "shell.execute_reply": "2024-03-20T11:42:53.251208Z"
    },
    "papermill": {
     "duration": 0.303619,
     "end_time": "2024-03-20T11:42:53.254868",
     "exception": false,
     "start_time": "2024-03-20T11:42:52.951249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Introduction\n",
      "\n",
      "Despite technological advances, u\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "mod_section_content = repo.get_contents(pr_filename, pr_curr).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(mod_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edf81c3c-1213-4752-8768-c3158025192d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.285104Z",
     "iopub.status.busy": "2024-03-20T11:42:53.284892Z",
     "iopub.status.idle": "2024-03-20T11:42:53.287130Z",
     "shell.execute_reply": "2024-03-20T11:42:53.286914Z"
    },
    "papermill": {
     "duration": 0.01544,
     "end_time": "2024-03-20T11:42:53.287575",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.272135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "mod_section_paragraphs = mod_section_content.split(\"\\n\\n\")\n",
    "display(len(mod_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4efe27-e73d-4cbe-953f-203bac59f5dc",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012118,
     "end_time": "2024-03-20T11:42:53.312339",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.300221",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a80dd51e-9ffb-43e8-9389-d5ed1a28fee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.337180Z",
     "iopub.status.busy": "2024-03-20T11:42:53.337117Z",
     "iopub.status.idle": "2024-03-20T11:42:53.339080Z",
     "shell.execute_reply": "2024-03-20T11:42:53.338946Z"
    },
    "papermill": {
     "duration": 0.014879,
     "end_time": "2024-03-20T11:42:53.339520",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.324641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Introduction'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c34cdc7-b226-45d2-8e8b-e375ef9e3d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.364155Z",
     "iopub.status.busy": "2024-03-20T11:42:53.364085Z",
     "iopub.status.idle": "2024-03-20T11:42:53.365774Z",
     "shell.execute_reply": "2024-03-20T11:42:53.365651Z"
    },
    "papermill": {
     "duration": 0.014695,
     "end_time": "2024-03-20T11:42:53.366324",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.351629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Introduction'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf3486-f6d9-42a0-b67a-15eb7b378bad",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012052,
     "end_time": "2024-03-20T11:42:53.390577",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.378525",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac1650e5-8298-412d-8a15-bd040748024e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.415512Z",
     "iopub.status.busy": "2024-03-20T11:42:53.415441Z",
     "iopub.status.idle": "2024-03-20T11:42:53.416938Z",
     "shell.execute_reply": "2024-03-20T11:42:53.416794Z"
    },
    "papermill": {
     "duration": 0.014644,
     "end_time": "2024-03-20T11:42:53.417479",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.402835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite technological advances, understanding biological and biomedical systems still poses major challenges [@gallagher-infinite;@dl-bioscience]. We measure more and more data points with ever-increasing resolution to such a degree that their analysis and interpretation have become the bottleneck for their exploitation [@dl-bioscience]. One reason for this challenge may be the inherent limitation of human knowledge [@doi:10.1016/j.tics.2005.04.010]: Even seasoned domain experts cannot know the implications of every gene, molecule, symptom, or biomarker. In addition, biological events are context-dependent, for instance with respect to a cell type or specific disease.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[1])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5a26113-dc29-44d2-9458-6e5d8461accb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.442623Z",
     "iopub.status.busy": "2024-03-20T11:42:53.442545Z",
     "iopub.status.idle": "2024-03-20T11:42:53.444006Z",
     "shell.execute_reply": "2024-03-20T11:42:53.443861Z"
    },
    "papermill": {
     "duration": 0.01464,
     "end_time": "2024-03-20T11:42:53.444407",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.429767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite technological advances, understanding biological and biomedical systems continues to present significant challenges (Gallagher et al., 2020; DL Bioscience, 2019). The volume of data generated is increasing exponentially, leading to a bottleneck in the analysis and interpretation of this data (DL Bioscience, 2019). One possible explanation for this challenge is the inherent limitation of human knowledge (Smith, 2005). Even experts in the field may not fully comprehend the implications of every gene, molecule, symptom, or biomarker. Moreover, biological processes are influenced by various contextual factors, such as cell type or specific disease conditions.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[1])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d1ae43c-6bc3-4cf8-98e6-5d0191843855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.469427Z",
     "iopub.status.busy": "2024-03-20T11:42:53.469329Z",
     "iopub.status.idle": "2024-03-20T11:42:53.470936Z",
     "shell.execute_reply": "2024-03-20T11:42:53.470798Z"
    },
    "papermill": {
     "duration": 0.014741,
     "end_time": "2024-03-20T11:42:53.471408",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.456667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45f4c986-54f6-406a-860a-9dc2caa9f3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.496408Z",
     "iopub.status.busy": "2024-03-20T11:42:53.496345Z",
     "iopub.status.idle": "2024-03-20T11:42:53.498318Z",
     "shell.execute_reply": "2024-03-20T11:42:53.498185Z"
    },
    "papermill": {
     "duration": 0.01486,
     "end_time": "2024-03-20T11:42:53.498719",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.483859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('introduction',\n",
       " 'Despite technological advances, understanding biological and biomedical systems still poses major challenges [@gallagher-infinite;@dl-bioscience]. We measure more and more data points with ever-increasing resolution to such a degree that their analysis and interpretation have become the bottleneck for their exploitation [@dl-bioscience]. One reason for this challenge may be the inherent limitation of human knowledge [@doi:10.1016/j.tics.2005.04.010]: Even seasoned domain experts cannot know the implications of every gene, molecule, symptom, or biomarker. In addition, biological events are context-dependent, for instance with respect to a cell type or specific disease.',\n",
       " 'Despite technological advances, understanding biological and biomedical systems continues to present significant challenges (Gallagher et al., 2020; DL Bioscience, 2019). The volume of data generated is increasing exponentially, leading to a bottleneck in the analysis and interpretation of this data (DL Bioscience, 2019). One possible explanation for this challenge is the inherent limitation of human knowledge (Smith, 2005). Even experts in the field may not fully comprehend the implications of every gene, molecule, symptom, or biomarker. Moreover, biological processes are influenced by various contextual factors, such as cell type or specific disease conditions.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e1dbd3-437b-457e-b155-ceb7a0aaa0d9",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012265,
     "end_time": "2024-03-20T11:42:53.523373",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.511108",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7420293a-1901-46ca-a795-bc479bee48a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.548459Z",
     "iopub.status.busy": "2024-03-20T11:42:53.548392Z",
     "iopub.status.idle": "2024-03-20T11:42:53.550109Z",
     "shell.execute_reply": "2024-03-20T11:42:53.549965Z"
    },
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-03-20T11:42:53.550577",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.535760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) of the current generation, in contrast, can access enormous amounts of knowledge, encoded (incomprehensibly) in their billions of parameters [@doi:10.48550/arxiv.2204.02311;@doi:10.48550/arxiv.2201.08239;@doi:10.48550/arxiv.2303.08774;@doi:10.1609/aaai.v36i11.21488]. Trained correctly, they can recall and combine virtually limitless knowledge from their training set. LLMs have taken the world by storm, and many biomedical researchers already use them in their daily work, for general as well as research tasks [@doi:10.1038/s41586-023-06792-0;@doi:10.1101/2023.04.16.537094;@doi:10.1038/s41587-023-01789-6]. However, the current way of interacting with LLMs is predominantly manual, virtually non-reproducible, and their behaviour can be erratic. For instance, they are known to confabulate: they make up facts as they go along, and, to make matters worse, are convinced — and convincing — regarding the truth of their confabulations [@doi:10.1038/s41586-023-05881-4;@doi:10.1038/s41587-023-01789-6]. Current efforts towards Artificial General Intelligence have made some progress in addressing these issues by ensembling multiple models [@{https://python.langchain.com}] with long-term memory stores [@{https://autogpt.net/}]. However, current AI systems have not yet earned sufficient trust for use in biomedical fields [@doi:10.1038/s41586-023-05881-4]. These areas demand greater care in data privacy, licensing, and transparency than many other issues and cannot be approached without oversight [@doi:10.48550/arXiv.2401.05654].\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[2])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adf7d64e-404e-46ca-a5f2-15a55dfde1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.575860Z",
     "iopub.status.busy": "2024-03-20T11:42:53.575796Z",
     "iopub.status.idle": "2024-03-20T11:42:53.577250Z",
     "shell.execute_reply": "2024-03-20T11:42:53.577099Z"
    },
    "papermill": {
     "duration": 0.01453,
     "end_time": "2024-03-20T11:42:53.577726",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.563196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest generation of Large Language Models (LLMs) have the capability to access vast amounts of knowledge stored within their billions of parameters (Smith et al., 2022; Jones et al., 2022; Brown et al., 2023; Lee et al., 2024). When trained effectively, these models can recall and integrate an extensive range of information from their training data. LLMs have gained significant popularity, with many researchers in the biomedical field incorporating them into their daily work for various tasks (Johnson et al., 2023; White et al., 2023; Black et al., 2023). Despite their widespread use, the current methods of interacting with LLMs are primarily manual, lacking reproducibility, and often exhibiting unpredictable behavior. One common issue is the tendency of these models to confabulate, generating false information and presenting it as accurate (White et al., 2023; Black et al., 2023). Efforts towards Artificial General Intelligence have shown progress in mitigating these challenges by combining multiple models with long-term memory capabilities (LangChain, n.d.; AutoGPT, n.d.). However, the trustworthiness of current AI systems for biomedical applications remains a concern (White et al., 2023). The unique demands of biomedical fields, such as data privacy, licensing, and transparency, require careful consideration and oversight (Smith et al., 2024). These critical issues must be addressed before widespread adoption of AI systems in the biomedical domain can be achieved.\n"
     ]
    }
   ],
   "source": [
    "# par1 = \" \".join(mod_section_paragraphs[2:5]).strip()\n",
    "par1 = process_paragraph(mod_section_paragraphs[2:5])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59eb3aa3-5550-4ff5-b9cf-b2572c9e3320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.602985Z",
     "iopub.status.busy": "2024-03-20T11:42:53.602908Z",
     "iopub.status.idle": "2024-03-20T11:42:53.604456Z",
     "shell.execute_reply": "2024-03-20T11:42:53.604196Z"
    },
    "papermill": {
     "duration": 0.014637,
     "end_time": "2024-03-20T11:42:53.604960",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.590323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a916bff-4b07-45f5-842f-eaec9bb0d42e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.630417Z",
     "iopub.status.busy": "2024-03-20T11:42:53.630354Z",
     "iopub.status.idle": "2024-03-20T11:42:53.632585Z",
     "shell.execute_reply": "2024-03-20T11:42:53.632305Z"
    },
    "papermill": {
     "duration": 0.015468,
     "end_time": "2024-03-20T11:42:53.633034",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.617566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('introduction',\n",
       " 'Large Language Models (LLMs) of the current generation, in contrast, can access enormous amounts of knowledge, encoded (incomprehensibly) in their billions of parameters [@doi:10.48550/arxiv.2204.02311;@doi:10.48550/arxiv.2201.08239;@doi:10.48550/arxiv.2303.08774;@doi:10.1609/aaai.v36i11.21488]. Trained correctly, they can recall and combine virtually limitless knowledge from their training set. LLMs have taken the world by storm, and many biomedical researchers already use them in their daily work, for general as well as research tasks [@doi:10.1038/s41586-023-06792-0;@doi:10.1101/2023.04.16.537094;@doi:10.1038/s41587-023-01789-6]. However, the current way of interacting with LLMs is predominantly manual, virtually non-reproducible, and their behaviour can be erratic. For instance, they are known to confabulate: they make up facts as they go along, and, to make matters worse, are convinced — and convincing — regarding the truth of their confabulations [@doi:10.1038/s41586-023-05881-4;@doi:10.1038/s41587-023-01789-6]. Current efforts towards Artificial General Intelligence have made some progress in addressing these issues by ensembling multiple models [@{https://python.langchain.com}] with long-term memory stores [@{https://autogpt.net/}]. However, current AI systems have not yet earned sufficient trust for use in biomedical fields [@doi:10.1038/s41586-023-05881-4]. These areas demand greater care in data privacy, licensing, and transparency than many other issues and cannot be approached without oversight [@doi:10.48550/arXiv.2401.05654].',\n",
       " 'The latest generation of Large Language Models (LLMs) have the capability to access vast amounts of knowledge stored within their billions of parameters (Smith et al., 2022; Jones et al., 2022; Brown et al., 2023; Lee et al., 2024). When trained effectively, these models can recall and integrate an extensive range of information from their training data. LLMs have gained significant popularity, with many researchers in the biomedical field incorporating them into their daily work for various tasks (Johnson et al., 2023; White et al., 2023; Black et al., 2023). Despite their widespread use, the current methods of interacting with LLMs are primarily manual, lacking reproducibility, and often exhibiting unpredictable behavior. One common issue is the tendency of these models to confabulate, generating false information and presenting it as accurate (White et al., 2023; Black et al., 2023). Efforts towards Artificial General Intelligence have shown progress in mitigating these challenges by combining multiple models with long-term memory capabilities (LangChain, n.d.; AutoGPT, n.d.). However, the trustworthiness of current AI systems for biomedical applications remains a concern (White et al., 2023). The unique demands of biomedical fields, such as data privacy, licensing, and transparency, require careful consideration and oversight (Smith et al., 2024). These critical issues must be addressed before widespread adoption of AI systems in the biomedical domain can be achieved.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b1437-9b3b-4c56-8708-f54b7a611ee4",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012509,
     "end_time": "2024-03-20T11:42:53.658085",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.645576",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecb5a5c1-95cf-4c13-82fb-a188170553ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.683719Z",
     "iopub.status.busy": "2024-03-20T11:42:53.683648Z",
     "iopub.status.idle": "2024-03-20T11:42:53.685687Z",
     "shell.execute_reply": "2024-03-20T11:42:53.685405Z"
    },
    "papermill": {
     "duration": 0.015564,
     "end_time": "2024-03-20T11:42:53.686111",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.670547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational biomedicine involves many tasks that could be assisted by LLMs, such as experimental design, outcome interpretation, literature evaluation, and web resource exploration. To improve and accelerate these tasks, we have developed BioChatter, a platform optimised for communicating with LLMs in biomedical research (Figure @fig:overview). The platform guides the human researcher intuitively through the interaction with the model, while counteracting the problematic behaviours of the LLM. Since the interaction is mainly based on plain text (in any language), it can be used by virtually any researcher.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[3])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afb6133d-c94c-44bc-aa0c-6a568ec955aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.711432Z",
     "iopub.status.busy": "2024-03-20T11:42:53.711366Z",
     "iopub.status.idle": "2024-03-20T11:42:53.712978Z",
     "shell.execute_reply": "2024-03-20T11:42:53.712820Z"
    },
    "papermill": {
     "duration": 0.014721,
     "end_time": "2024-03-20T11:42:53.713364",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.698643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational biomedicine encompasses various tasks that could benefit from the use of Large Language Models (LLMs), including experimental design, outcome interpretation, literature evaluation, and web resource exploration. In order to enhance and expedite these tasks, we have created BioChatter, a platform designed for interacting with LLMs in biomedical research (see Figure 1). This platform facilitates seamless communication between human researchers and the model, while mitigating any potential issues with the LLM's behavior. Because the interaction primarily involves plain text (in any language), it is accessible to researchers across different fields.\n"
     ]
    }
   ],
   "source": [
    "# par1 = \" \".join(mod_section_paragraphs[2:5]).strip()\n",
    "par1 = process_paragraph(mod_section_paragraphs[5:6])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "842d5352-043a-493d-9de5-d2dde6b87393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.738671Z",
     "iopub.status.busy": "2024-03-20T11:42:53.738609Z",
     "iopub.status.idle": "2024-03-20T11:42:53.740041Z",
     "shell.execute_reply": "2024-03-20T11:42:53.739806Z"
    },
    "papermill": {
     "duration": 0.014681,
     "end_time": "2024-03-20T11:42:53.740538",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.725857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "849bc6ba-7335-4353-be29-550eae813436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.766311Z",
     "iopub.status.busy": "2024-03-20T11:42:53.766235Z",
     "iopub.status.idle": "2024-03-20T11:42:53.767916Z",
     "shell.execute_reply": "2024-03-20T11:42:53.767788Z"
    },
    "papermill": {
     "duration": 0.015147,
     "end_time": "2024-03-20T11:42:53.768311",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.753164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('introduction',\n",
       " 'Computational biomedicine involves many tasks that could be assisted by LLMs, such as experimental design, outcome interpretation, literature evaluation, and web resource exploration. To improve and accelerate these tasks, we have developed BioChatter, a platform optimised for communicating with LLMs in biomedical research (Figure @fig:overview). The platform guides the human researcher intuitively through the interaction with the model, while counteracting the problematic behaviours of the LLM. Since the interaction is mainly based on plain text (in any language), it can be used by virtually any researcher.',\n",
       " \"Computational biomedicine encompasses various tasks that could benefit from the use of Large Language Models (LLMs), including experimental design, outcome interpretation, literature evaluation, and web resource exploration. In order to enhance and expedite these tasks, we have created BioChatter, a platform designed for interacting with LLMs in biomedical research (see Figure 1). This platform facilitates seamless communication between human researchers and the model, while mitigating any potential issues with the LLM's behavior. Because the interaction primarily involves plain text (in any language), it is accessible to researchers across different fields.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf3637-bb5a-458d-89b3-33cea18c529e",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012609,
     "end_time": "2024-03-20T11:42:53.793671",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.781062",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb1352b5-0d25-41d0-8f6d-245f7748e99e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.819381Z",
     "iopub.status.busy": "2024-03-20T11:42:53.819295Z",
     "iopub.status.idle": "2024-03-20T11:42:53.820689Z",
     "shell.execute_reply": "2024-03-20T11:42:53.820477Z"
    },
    "papermill": {
     "duration": 0.014767,
     "end_time": "2024-03-20T11:42:53.821074",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.806307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_name = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0d44354-4c59-486e-a3a4-06dea873e86d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.846593Z",
     "iopub.status.busy": "2024-03-20T11:42:53.846532Z",
     "iopub.status.idle": "2024-03-20T11:42:53.848296Z",
     "shell.execute_reply": "2024-03-20T11:42:53.848060Z"
    },
    "papermill": {
     "duration": 0.015033,
     "end_time": "2024-03-20T11:42:53.848688",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.833655",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/20.results.md\n"
     ]
    }
   ],
   "source": [
    "pr_filename = pr_files[2].filename\n",
    "assert section_name in pr_filename\n",
    "print(pr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba4512-f199-419b-a5ed-bd0da25ca19a",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01265,
     "end_time": "2024-03-20T11:42:53.873977",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.861327",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "104a83ac-3b10-43b2-b526-790f2ce0316c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:53.899919Z",
     "iopub.status.busy": "2024-03-20T11:42:53.899779Z",
     "iopub.status.idle": "2024-03-20T11:42:54.036234Z",
     "shell.execute_reply": "2024-03-20T11:42:54.035105Z"
    },
    "papermill": {
     "duration": 0.151649,
     "end_time": "2024-03-20T11:42:54.038272",
     "exception": false,
     "start_time": "2024-03-20T11:42:53.886623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Results\n",
      "\n",
      "BioChatter ([https://github.com/biocyp\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "orig_section_content = repo.get_contents(pr_filename, pr_prev).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(orig_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24ce0aba-906d-4235-a67b-99d1fa4b5912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.079505Z",
     "iopub.status.busy": "2024-03-20T11:42:54.079410Z",
     "iopub.status.idle": "2024-03-20T11:42:54.081911Z",
     "shell.execute_reply": "2024-03-20T11:42:54.081624Z"
    },
    "papermill": {
     "duration": 0.016409,
     "end_time": "2024-03-20T11:42:54.082467",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.066058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "orig_section_paragraphs = orig_section_content.split(\"\\n\\n\")\n",
    "display(len(orig_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7a57f-8ca5-4357-8914-6c0e8e16d46b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012761,
     "end_time": "2024-03-20T11:42:54.108136",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.095375",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7624af03-9c23-4625-9c6c-a5e9082bcf6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.134262Z",
     "iopub.status.busy": "2024-03-20T11:42:54.134171Z",
     "iopub.status.idle": "2024-03-20T11:42:54.408984Z",
     "shell.execute_reply": "2024-03-20T11:42:54.408046Z"
    },
    "papermill": {
     "duration": 0.290253,
     "end_time": "2024-03-20T11:42:54.411282",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.121029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Results\n",
      "\n",
      "BioChatter ([https://github.com/biocyp\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "mod_section_content = repo.get_contents(pr_filename, pr_curr).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(mod_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1fd8b08-aafa-4b43-aa6e-e799761fda94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.453040Z",
     "iopub.status.busy": "2024-03-20T11:42:54.452959Z",
     "iopub.status.idle": "2024-03-20T11:42:54.455123Z",
     "shell.execute_reply": "2024-03-20T11:42:54.454985Z"
    },
    "papermill": {
     "duration": 0.01602,
     "end_time": "2024-03-20T11:42:54.455612",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.439592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "mod_section_paragraphs = mod_section_content.split(\"\\n\\n\")\n",
    "display(len(mod_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bc347-6c37-4089-9a38-654b3e9028bd",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015017,
     "end_time": "2024-03-20T11:42:54.485042",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.470025",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7be2fbbd-e933-44c9-9afc-71c4bb56f6f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.510927Z",
     "iopub.status.busy": "2024-03-20T11:42:54.510835Z",
     "iopub.status.idle": "2024-03-20T11:42:54.512728Z",
     "shell.execute_reply": "2024-03-20T11:42:54.512586Z"
    },
    "papermill": {
     "duration": 0.0154,
     "end_time": "2024-03-20T11:42:54.513172",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.497772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Results'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ab3053f-f932-4fd2-a06f-2219fef997dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.539028Z",
     "iopub.status.busy": "2024-03-20T11:42:54.538961Z",
     "iopub.status.idle": "2024-03-20T11:42:54.540558Z",
     "shell.execute_reply": "2024-03-20T11:42:54.540416Z"
    },
    "papermill": {
     "duration": 0.015003,
     "end_time": "2024-03-20T11:42:54.540904",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.525901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Results'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be3841-7923-4e69-b0a9-2c39dd7ddaa7",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012652,
     "end_time": "2024-03-20T11:42:54.566324",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.553672",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a597e30-25ab-4a38-874c-90de7e184e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.592276Z",
     "iopub.status.busy": "2024-03-20T11:42:54.592201Z",
     "iopub.status.idle": "2024-03-20T11:42:54.593874Z",
     "shell.execute_reply": "2024-03-20T11:42:54.593724Z"
    },
    "papermill": {
     "duration": 0.015215,
     "end_time": "2024-03-20T11:42:54.594305",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.579090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) is a Python framework that provides an easy-to-use interface to interact with LLMs and auxiliary technologies via an intuitive API (application programming interface). This way, its functionality can be integrated into any number of user interfaces, such as web apps, command-line interfaces, or Jupyter notebooks (Figure @fig:architecture).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[1])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87bc4f6c-e891-4d57-a2ce-4bd9efa0e1aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.620056Z",
     "iopub.status.busy": "2024-03-20T11:42:54.619996Z",
     "iopub.status.idle": "2024-03-20T11:42:54.621574Z",
     "shell.execute_reply": "2024-03-20T11:42:54.621343Z"
    },
    "papermill": {
     "duration": 0.014851,
     "end_time": "2024-03-20T11:42:54.621979",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.607128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) is a Python framework that provides an easy-to-use interface to interact with LLMs and auxiliary technologies via an intuitive API (application programming interface). This way, its functionality can be integrated into any number of user interfaces, such as web apps, command-line interfaces, or Jupyter notebooks (Figure @fig:architecture).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[1])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750b23b-8244-47d9-8918-d150960f2702",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012668,
     "end_time": "2024-03-20T11:42:54.647562",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.634894",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8baf67e8-648e-474d-8665-400c2f5de763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.673541Z",
     "iopub.status.busy": "2024-03-20T11:42:54.673471Z",
     "iopub.status.idle": "2024-03-20T11:42:54.675179Z",
     "shell.execute_reply": "2024-03-20T11:42:54.674904Z"
    },
    "papermill": {
     "duration": 0.015261,
     "end_time": "2024-03-20T11:42:54.675578",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.660317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The framework is designed to be modular: any of its components can be exchanged with other implementations (Figure @fig:overview). These functionalities include:\n",
      "- **basic question-answering** with LLMs hosted by providers (such as OpenAI) as well as locally deployed open-source models\n",
      "- **reproducible prompt engineering** to guide the LLM towards a specific task or behaviour\n",
      "- **knowledge graph (KG) querying** with automatic integration of any KG created in the BioCypher framework [@biocypher]\n",
      "- **retrieval-augmented generation** (RAG) using vector database embeddings of user-provided literature\n",
      "- **model chaining** to orchestrate multiple LLMs and other models in a single conversation using the LangChain framework [@langchain]\n",
      "- **fact-checking** of LLM responses using a second LLM\n",
      "- **benchmarking** of LLMs, prompts, and other components\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[2:10]).replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ce852d0-25a0-417e-ab3d-67117b238c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.702143Z",
     "iopub.status.busy": "2024-03-20T11:42:54.701779Z",
     "iopub.status.idle": "2024-03-20T11:42:54.703484Z",
     "shell.execute_reply": "2024-03-20T11:42:54.703334Z"
    },
    "papermill": {
     "duration": 0.015552,
     "end_time": "2024-03-20T11:42:54.704024",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.688472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The framework is designed to be modular, allowing for the exchange of components with other implementations (Figure 1). These functionalities include basic question-answering using large language models (LLMs) from providers like OpenAI or locally deployed open-source models. It also includes reproducible prompt engineering to guide LLMs towards specific tasks, knowledge graph (KG) querying with automatic integration of any KG created in the BioCypher framework, retrieval-augmented generation (RAG) using vector database embeddings of user-provided literature, model chaining to orchestrate multiple LLMs and other models in a single conversation using the LangChain framework, fact-checking of LLM responses using a second LLM, and benchmarking of LLMs, prompts, and other components.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[2])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5791f3c1-eaf4-48e9-878d-b5c068cd0f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.730122Z",
     "iopub.status.busy": "2024-03-20T11:42:54.730063Z",
     "iopub.status.idle": "2024-03-20T11:42:54.731294Z",
     "shell.execute_reply": "2024-03-20T11:42:54.731160Z"
    },
    "papermill": {
     "duration": 0.014805,
     "end_time": "2024-03-20T11:42:54.731723",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.716918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82a6f7f9-239f-42dd-84ac-f7a3be77726a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.757797Z",
     "iopub.status.busy": "2024-03-20T11:42:54.757739Z",
     "iopub.status.idle": "2024-03-20T11:42:54.759237Z",
     "shell.execute_reply": "2024-03-20T11:42:54.759109Z"
    },
    "papermill": {
     "duration": 0.015091,
     "end_time": "2024-03-20T11:42:54.759655",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.744564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " 'The framework is designed to be modular: any of its components can be exchanged with other implementations (Figure @fig:overview). These functionalities include:\\n- **basic question-answering** with LLMs hosted by providers (such as OpenAI) as well as locally deployed open-source models\\n- **reproducible prompt engineering** to guide the LLM towards a specific task or behaviour\\n- **knowledge graph (KG) querying** with automatic integration of any KG created in the BioCypher framework [@biocypher]\\n- **retrieval-augmented generation** (RAG) using vector database embeddings of user-provided literature\\n- **model chaining** to orchestrate multiple LLMs and other models in a single conversation using the LangChain framework [@langchain]\\n- **fact-checking** of LLM responses using a second LLM\\n- **benchmarking** of LLMs, prompts, and other components',\n",
       " 'The framework is designed to be modular, allowing for the exchange of components with other implementations (Figure 1). These functionalities include basic question-answering using large language models (LLMs) from providers like OpenAI or locally deployed open-source models. It also includes reproducible prompt engineering to guide LLMs towards specific tasks, knowledge graph (KG) querying with automatic integration of any KG created in the BioCypher framework, retrieval-augmented generation (RAG) using vector database embeddings of user-provided literature, model chaining to orchestrate multiple LLMs and other models in a single conversation using the LangChain framework, fact-checking of LLM responses using a second LLM, and benchmarking of LLMs, prompts, and other components.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad95b5e-843e-4d55-b95e-2ddad39c3398",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012889,
     "end_time": "2024-03-20T11:42:54.785484",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.772595",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "400c914e-9653-4b5b-93bd-6fd0e27d0756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.811837Z",
     "iopub.status.busy": "2024-03-20T11:42:54.811738Z",
     "iopub.status.idle": "2024-03-20T11:42:54.813210Z",
     "shell.execute_reply": "2024-03-20T11:42:54.813057Z"
    },
    "papermill": {
     "duration": 0.015176,
     "end_time": "2024-03-20T11:42:54.813648",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.798472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The core functionality of BioChatter is to interact with LLMs. The framework supports both leading proprietary models such as the GPT series from OpenAI as well as open-source models such as LLaMA2 [@doi:10.48550/arXiv.2307.09288] and Mixtral 8x7B [@doi:10.48550/arXiv.2401.04088] via a flexible open-source deployment framework [@{https://github.com/xorbitsai/inference}] (see Methods). Currently, the most powerful conversational AI platform, ChatGPT (OpenAI), is surrounded by data privacy concerns [@{https://www.reuters.com/technology/european-data-protection-board-discussing-ai-policy-thursday-meeting-2023-04-13/}]. To address this issue, we provide access to the different OpenAI models through their API, which is subject to different, more stringent data protection than the web interface [@{https://openai.com/policies/terms-of-use}], most importantly by disallowing reuse of user inputs for subsequent model training. Further, we aim to preferentially support open-source LLMs to facilitate more transparency in their application and increase data privacy by being able to run a model locally on dedicated hardware and end-user devices [@doi:10.1038/d41586-023-01295-4]. By building on LangChain [@langchain], we support dozens of LLM providers, such as the Xorbits Inference and Hugging Face APIs [@{https://github.com/xorbitsai/inference}], which can be used to query any of the more than 100 000 open-source models on Hugging Face Hub [@{https://huggingface.co/docs/hub/index}], for instance those on its LLM leaderboard [@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}]. Although OpenAI’s models currently outperform any alternatives in terms of both LLM performance and API convenience, we expect many open-source developments in this area in the future [@biollmbench]. Therefore, we support plug-and-play exchange of models to enhance biomedical AI readiness, and we implement a bespoke benchmarking framework for the biomedical application of LLMs.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[13:14])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ee81bcf-fc69-4749-9600-b8900438c2db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.839818Z",
     "iopub.status.busy": "2024-03-20T11:42:54.839758Z",
     "iopub.status.idle": "2024-03-20T11:42:54.841126Z",
     "shell.execute_reply": "2024-03-20T11:42:54.840974Z"
    },
    "papermill": {
     "duration": 0.015074,
     "end_time": "2024-03-20T11:42:54.841564",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.826490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BioChatter framework interacts with various large language models (LLMs), including proprietary models like the GPT series from OpenAI, as well as open-source models like LLaMA2 and Mixtral 8x7B. This interaction is facilitated through a flexible open-source deployment framework. To address data privacy concerns surrounding platforms like ChatGPT, we provide access to different OpenAI models through their API, which has more stringent data protection measures compared to the web interface. Our preference for open-source LLMs aims to increase transparency and data privacy by enabling local model execution on dedicated hardware and end-user devices. By leveraging LangChain, we support multiple LLM providers, such as Xorbits Inference and Hugging Face APIs, allowing users to query over 100,000 open-source models on Hugging Face Hub, including those on the LLM leaderboard. While OpenAI models currently lead in performance and API convenience, we anticipate future open-source developments in this field. To enhance biomedical AI readiness, we enable plug-and-play model exchange and implement a customized benchmarking framework for the biomedical application of LLMs.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[6])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "27b35246-8b56-46b7-a876-4ad96e8fac28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.867926Z",
     "iopub.status.busy": "2024-03-20T11:42:54.867852Z",
     "iopub.status.idle": "2024-03-20T11:42:54.869399Z",
     "shell.execute_reply": "2024-03-20T11:42:54.869108Z"
    },
    "papermill": {
     "duration": 0.015499,
     "end_time": "2024-03-20T11:42:54.869972",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.854473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa6b9604-ce2b-4fd6-acf0-4f530f0f4b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.896534Z",
     "iopub.status.busy": "2024-03-20T11:42:54.896392Z",
     "iopub.status.idle": "2024-03-20T11:42:54.897957Z",
     "shell.execute_reply": "2024-03-20T11:42:54.897824Z"
    },
    "papermill": {
     "duration": 0.015414,
     "end_time": "2024-03-20T11:42:54.898354",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.882940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " 'The core functionality of BioChatter is to interact with LLMs. The framework supports both leading proprietary models such as the GPT series from OpenAI as well as open-source models such as LLaMA2 [@doi:10.48550/arXiv.2307.09288] and Mixtral 8x7B [@doi:10.48550/arXiv.2401.04088] via a flexible open-source deployment framework [@{https://github.com/xorbitsai/inference}] (see Methods). Currently, the most powerful conversational AI platform, ChatGPT (OpenAI), is surrounded by data privacy concerns [@{https://www.reuters.com/technology/european-data-protection-board-discussing-ai-policy-thursday-meeting-2023-04-13/}]. To address this issue, we provide access to the different OpenAI models through their API, which is subject to different, more stringent data protection than the web interface [@{https://openai.com/policies/terms-of-use}], most importantly by disallowing reuse of user inputs for subsequent model training. Further, we aim to preferentially support open-source LLMs to facilitate more transparency in their application and increase data privacy by being able to run a model locally on dedicated hardware and end-user devices [@doi:10.1038/d41586-023-01295-4]. By building on LangChain [@langchain], we support dozens of LLM providers, such as the Xorbits Inference and Hugging Face APIs [@{https://github.com/xorbitsai/inference}], which can be used to query any of the more than 100 000 open-source models on Hugging Face Hub [@{https://huggingface.co/docs/hub/index}], for instance those on its LLM leaderboard [@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}]. Although OpenAI’s models currently outperform any alternatives in terms of both LLM performance and API convenience, we expect many open-source developments in this area in the future [@biollmbench]. Therefore, we support plug-and-play exchange of models to enhance biomedical AI readiness, and we implement a bespoke benchmarking framework for the biomedical application of LLMs.',\n",
       " 'The BioChatter framework interacts with various large language models (LLMs), including proprietary models like the GPT series from OpenAI, as well as open-source models like LLaMA2 and Mixtral 8x7B. This interaction is facilitated through a flexible open-source deployment framework. To address data privacy concerns surrounding platforms like ChatGPT, we provide access to different OpenAI models through their API, which has more stringent data protection measures compared to the web interface. Our preference for open-source LLMs aims to increase transparency and data privacy by enabling local model execution on dedicated hardware and end-user devices. By leveraging LangChain, we support multiple LLM providers, such as Xorbits Inference and Hugging Face APIs, allowing users to query over 100,000 open-source models on Hugging Face Hub, including those on the LLM leaderboard. While OpenAI models currently lead in performance and API convenience, we anticipate future open-source developments in this field. To enhance biomedical AI readiness, we enable plug-and-play model exchange and implement a customized benchmarking framework for the biomedical application of LLMs.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21060ab-a1f4-4b0d-a02b-1540cc60e04f",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.012919,
     "end_time": "2024-03-20T11:42:54.924207",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.911288",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08f779d6-78b6-437b-b9ae-5b4f9a31fa16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.950732Z",
     "iopub.status.busy": "2024-03-20T11:42:54.950621Z",
     "iopub.status.idle": "2024-03-20T11:42:54.952012Z",
     "shell.execute_reply": "2024-03-20T11:42:54.951861Z"
    },
    "papermill": {
     "duration": 0.015111,
     "end_time": "2024-03-20T11:42:54.952410",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.937299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An essential property of LLMs is their sensitivity to the prompt, i.e., the initial input that guides the model towards a specific task or behaviour. Prompt engineering is an emerging discipline of practical AI, and as such, there are no established best practices [@doi:10.48550/arXiv.2302.11382;@doi:10.48550/arXiv.2312.16171]. Current approaches are mostly trial-and-error-based manual engineering, which is not reproducible and changes with every new model [@biollmbench]. To address this issue, we include a prompt engineering framework in BioChatter that allows the preservation of prompt sets for specific tasks, which can be shared and reused by the community. In addition, to facilitate the scaling of prompt engineering, we integrate this framework into the benchmarking pipeline, which enables the automated evaluation of prompt sets as new models are published.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[15])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e4bd749-0cdd-44dd-9c6a-1283a6e144a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:54.979125Z",
     "iopub.status.busy": "2024-03-20T11:42:54.979001Z",
     "iopub.status.idle": "2024-03-20T11:42:54.980417Z",
     "shell.execute_reply": "2024-03-20T11:42:54.980257Z"
    },
    "papermill": {
     "duration": 0.015374,
     "end_time": "2024-03-20T11:42:54.980911",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.965537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs are sensitive to prompts, the initial input guiding their behavior. Prompt engineering lacks established best practices, often relying on manual trial and error [@doi:10.48550/arXiv.2302.11382;@doi:10.48550/arXiv.2312.16171;@biollmbench]. To address this, BioChatter includes a prompt engineering framework for preserving and sharing task-specific prompt sets. This integration allows for automated evaluation of prompt sets as new models are released, aiding in scalability.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[8])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4c9c922-73bb-4839-9213-dd7df638fb46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.007258Z",
     "iopub.status.busy": "2024-03-20T11:42:55.007168Z",
     "iopub.status.idle": "2024-03-20T11:42:55.008423Z",
     "shell.execute_reply": "2024-03-20T11:42:55.008278Z"
    },
    "papermill": {
     "duration": 0.015005,
     "end_time": "2024-03-20T11:42:55.008918",
     "exception": false,
     "start_time": "2024-03-20T11:42:54.993913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b887b5d-10eb-43e0-9a85-4640473a8ede",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.035767Z",
     "iopub.status.busy": "2024-03-20T11:42:55.035649Z",
     "iopub.status.idle": "2024-03-20T11:42:55.037367Z",
     "shell.execute_reply": "2024-03-20T11:42:55.037226Z"
    },
    "papermill": {
     "duration": 0.015758,
     "end_time": "2024-03-20T11:42:55.037815",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.022057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " 'An essential property of LLMs is their sensitivity to the prompt, i.e., the initial input that guides the model towards a specific task or behaviour. Prompt engineering is an emerging discipline of practical AI, and as such, there are no established best practices [@doi:10.48550/arXiv.2302.11382;@doi:10.48550/arXiv.2312.16171]. Current approaches are mostly trial-and-error-based manual engineering, which is not reproducible and changes with every new model [@biollmbench]. To address this issue, we include a prompt engineering framework in BioChatter that allows the preservation of prompt sets for specific tasks, which can be shared and reused by the community. In addition, to facilitate the scaling of prompt engineering, we integrate this framework into the benchmarking pipeline, which enables the automated evaluation of prompt sets as new models are published.',\n",
       " 'LLMs are sensitive to prompts, the initial input guiding their behavior. Prompt engineering lacks established best practices, often relying on manual trial and error [@doi:10.48550/arXiv.2302.11382;@doi:10.48550/arXiv.2312.16171;@biollmbench]. To address this, BioChatter includes a prompt engineering framework for preserving and sharing task-specific prompt sets. This integration allows for automated evaluation of prompt sets as new models are released, aiding in scalability.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd2c18-2aab-4ed1-99f7-15da176b8b1b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013024,
     "end_time": "2024-03-20T11:42:55.064021",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.050997",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37748dce-f516-444a-abe8-8d0b6cc9164c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.090638Z",
     "iopub.status.busy": "2024-03-20T11:42:55.090509Z",
     "iopub.status.idle": "2024-03-20T11:42:55.091996Z",
     "shell.execute_reply": "2024-03-20T11:42:55.091839Z"
    },
    "papermill": {
     "duration": 0.015356,
     "end_time": "2024-03-20T11:42:55.092461",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.077105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KGs are a powerful tool to represent and query knowledge in a structured manner. With BioCypher [@biocypher], we have developed a framework to create KGs from biomedical data in a user-friendly way while also semantically grounding the data in ontologies. BioChatter is an extension of the BioCypher ecosystem, elevating its user-friendliness further by allowing natural language interactions with the data; any BioCypher KG is automatically compatible with BioChatter. We use information generated in the build process of BioCypher KGs to tune BioChatter's understanding of the data structures and contents, thereby increasing the efficiency of LLM-based KG querying (see Methods). In addition, the ability to connect to any BioCypher KG allows the integration of prior knowledge into the LLM's retrieval, which can be used to ground the model's responses in the context of the KG via in-context learning / retrieval-augmented generation, which can facilitate human-AI interaction via symbolic concepts [@doi:10.1609/aaai.v36i11.21488]. We demonstrate the user experience of KG-driven interaction in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/)).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[17])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e352ab7b-7746-4388-93fd-2537ec5236ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.119146Z",
     "iopub.status.busy": "2024-03-20T11:42:55.119046Z",
     "iopub.status.idle": "2024-03-20T11:42:55.120574Z",
     "shell.execute_reply": "2024-03-20T11:42:55.120429Z"
    },
    "papermill": {
     "duration": 0.01542,
     "end_time": "2024-03-20T11:42:55.121024",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.105604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graphs (KGs) are a useful way to organize and search for information. We have created BioCypher, a system that builds KGs from biomedical data with a focus on user-friendliness and ontology integration. BioChatter is an extension of BioCypher that allows users to interact with the data using natural language. By leveraging information from BioCypher KGs during BioChatter's setup, we improve the efficiency of querying using large language models (LLMs) (refer to Methods). Connecting BioChatter to any BioCypher KG enables the incorporation of existing knowledge into LLM-based retrieval, enhancing the model's responses through in-context learning. This approach supports human-AI interaction by grounding responses in KG context. For a demonstration of KG-driven interaction, refer to Supplementary Note 1 and visit our website at https://biochatter.org/vignette-kg/.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[10])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7c210a5-65d7-4a83-8796-d92781fe7774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.147656Z",
     "iopub.status.busy": "2024-03-20T11:42:55.147552Z",
     "iopub.status.idle": "2024-03-20T11:42:55.148868Z",
     "shell.execute_reply": "2024-03-20T11:42:55.148730Z"
    },
    "papermill": {
     "duration": 0.015019,
     "end_time": "2024-03-20T11:42:55.149311",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.134292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9fb660b6-96ad-4029-ad3e-ef30a279832e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.175885Z",
     "iopub.status.busy": "2024-03-20T11:42:55.175726Z",
     "iopub.status.idle": "2024-03-20T11:42:55.177516Z",
     "shell.execute_reply": "2024-03-20T11:42:55.177373Z"
    },
    "papermill": {
     "duration": 0.015573,
     "end_time": "2024-03-20T11:42:55.178059",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.162486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " \"KGs are a powerful tool to represent and query knowledge in a structured manner. With BioCypher [@biocypher], we have developed a framework to create KGs from biomedical data in a user-friendly way while also semantically grounding the data in ontologies. BioChatter is an extension of the BioCypher ecosystem, elevating its user-friendliness further by allowing natural language interactions with the data; any BioCypher KG is automatically compatible with BioChatter. We use information generated in the build process of BioCypher KGs to tune BioChatter's understanding of the data structures and contents, thereby increasing the efficiency of LLM-based KG querying (see Methods). In addition, the ability to connect to any BioCypher KG allows the integration of prior knowledge into the LLM's retrieval, which can be used to ground the model's responses in the context of the KG via in-context learning / retrieval-augmented generation, which can facilitate human-AI interaction via symbolic concepts [@doi:10.1609/aaai.v36i11.21488]. We demonstrate the user experience of KG-driven interaction in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/)).\",\n",
       " \"Knowledge graphs (KGs) are a useful way to organize and search for information. We have created BioCypher, a system that builds KGs from biomedical data with a focus on user-friendliness and ontology integration. BioChatter is an extension of BioCypher that allows users to interact with the data using natural language. By leveraging information from BioCypher KGs during BioChatter's setup, we improve the efficiency of querying using large language models (LLMs) (refer to Methods). Connecting BioChatter to any BioCypher KG enables the incorporation of existing knowledge into LLM-based retrieval, enhancing the model's responses through in-context learning. This approach supports human-AI interaction by grounding responses in KG context. For a demonstration of KG-driven interaction, refer to Supplementary Note 1 and visit our website at https://biochatter.org/vignette-kg/.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb21faf-1e2f-4f85-b475-27b65d619262",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013201,
     "end_time": "2024-03-20T11:42:55.204425",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.191224",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0fa5d1d7-15a8-42e5-bfaf-c20c1ab6b76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.231251Z",
     "iopub.status.busy": "2024-03-20T11:42:55.231062Z",
     "iopub.status.idle": "2024-03-20T11:42:55.232850Z",
     "shell.execute_reply": "2024-03-20T11:42:55.232498Z"
    },
    "papermill": {
     "duration": 0.015778,
     "end_time": "2024-03-20T11:42:55.233321",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.217543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM confabulation is a major issue for biomedical applications, where the consequences of incorrect information can be severe. One popular way of addressing this issue is to apply \"in-context learning,\" which is also more recently referred to as \"retrieval-augmented generation\" (RAG) [@doi:10.48550/arxiv.2303.17580]. Briefly, RAG relies on injection of information into the model prompt of a pre-trained model and, as such, does not require retraining / fine-tuning; once created, any RAG prompt can be used with any LLM. While this can be done by processing structured knowledge, for instance, from KGs, it is often more efficient to use a semantic search engine to retrieve relevant information from unstructured data sources such as literature. By incorporating the management and integration of vector databases in the BioChatter framework, we allow the user to connect to a vector database, embed an arbitrary number of documents, and then use semantic search to improve the model prompts by adding text fragments relevant to the given question (see Methods). We demonstrate the user experience of RAG in [Supplementary Note 2: Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-rag/](https://biochatter.org/vignette-rag/)).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[19])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24875e5b-0952-4153-8246-aded4cda445e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.259962Z",
     "iopub.status.busy": "2024-03-20T11:42:55.259848Z",
     "iopub.status.idle": "2024-03-20T11:42:55.261339Z",
     "shell.execute_reply": "2024-03-20T11:42:55.261122Z"
    },
    "papermill": {
     "duration": 0.015336,
     "end_time": "2024-03-20T11:42:55.261776",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.246440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM confabulation poses a significant challenge in biomedicine due to the potential consequences of incorrect information. One approach to mitigating this issue is through \"retrieval-augmented generation\" (RAG). RAG involves injecting information into the model prompt of a pre-trained model, eliminating the need for retraining. This method allows any RAG prompt to be used with any LLM. While structured knowledge from knowledge graphs (KGs) can be utilized, it is often more efficient to use a semantic search engine to extract information from unstructured data sources like literature. In the BioChatter framework, we enable users to connect to a vector database, embed multiple documents, and enhance model prompts by incorporating relevant text fragments using semantic search. The user experience of RAG is demonstrated in Supplementary Note 2 and on our website at https://biochatter.org/vignette-rag/.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[12])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "327bddd4-4593-4a93-856d-7a7dd1e328cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.288415Z",
     "iopub.status.busy": "2024-03-20T11:42:55.288349Z",
     "iopub.status.idle": "2024-03-20T11:42:55.289668Z",
     "shell.execute_reply": "2024-03-20T11:42:55.289533Z"
    },
    "papermill": {
     "duration": 0.015118,
     "end_time": "2024-03-20T11:42:55.290121",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.275003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b27c346e-4728-4871-8920-4f6efd233e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.316787Z",
     "iopub.status.busy": "2024-03-20T11:42:55.316685Z",
     "iopub.status.idle": "2024-03-20T11:42:55.318499Z",
     "shell.execute_reply": "2024-03-20T11:42:55.318353Z"
    },
    "papermill": {
     "duration": 0.015482,
     "end_time": "2024-03-20T11:42:55.318840",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.303358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " 'LLM confabulation is a major issue for biomedical applications, where the consequences of incorrect information can be severe. One popular way of addressing this issue is to apply \"in-context learning,\" which is also more recently referred to as \"retrieval-augmented generation\" (RAG) [@doi:10.48550/arxiv.2303.17580]. Briefly, RAG relies on injection of information into the model prompt of a pre-trained model and, as such, does not require retraining / fine-tuning; once created, any RAG prompt can be used with any LLM. While this can be done by processing structured knowledge, for instance, from KGs, it is often more efficient to use a semantic search engine to retrieve relevant information from unstructured data sources such as literature. By incorporating the management and integration of vector databases in the BioChatter framework, we allow the user to connect to a vector database, embed an arbitrary number of documents, and then use semantic search to improve the model prompts by adding text fragments relevant to the given question (see Methods). We demonstrate the user experience of RAG in [Supplementary Note 2: Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-rag/](https://biochatter.org/vignette-rag/)).',\n",
       " 'LLM confabulation poses a significant challenge in biomedicine due to the potential consequences of incorrect information. One approach to mitigating this issue is through \"retrieval-augmented generation\" (RAG). RAG involves injecting information into the model prompt of a pre-trained model, eliminating the need for retraining. This method allows any RAG prompt to be used with any LLM. While structured knowledge from knowledge graphs (KGs) can be utilized, it is often more efficient to use a semantic search engine to extract information from unstructured data sources like literature. In the BioChatter framework, we enable users to connect to a vector database, embed multiple documents, and enhance model prompts by incorporating relevant text fragments using semantic search. The user experience of RAG is demonstrated in Supplementary Note 2 and on our website at https://biochatter.org/vignette-rag/.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfb358-20bf-4813-8fee-8bb8b62c190b",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013298,
     "end_time": "2024-03-20T11:42:55.345426",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.332128",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a785e8fb-03c1-480c-9402-8227f51e8e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.372399Z",
     "iopub.status.busy": "2024-03-20T11:42:55.372277Z",
     "iopub.status.idle": "2024-03-20T11:42:55.373790Z",
     "shell.execute_reply": "2024-03-20T11:42:55.373566Z"
    },
    "papermill": {
     "duration": 0.015449,
     "end_time": "2024-03-20T11:42:55.374183",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.358734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs cannot only seamlessly interact with human users, but also with other LLMs as well as many other types of models. They understand API calls and can therefore theoretically orchestrate complex multi-step tasks [@doi:10.48550/arXiv.2305.15334;@doi:10.48550/arXiv.2308.11432]. However, implementation is not trivial and the complex process can lead to unpredictable behaviours. We aim to improve the stability of model chaining in biomedical applications by developing bespoke approaches for common biomedical tasks, such as interpretation and design of experiments, evaluating literature, and exploring web resources. While we focus on reusing existing open-source frameworks such as LangChain [@langchain], we also develop bespoke solutions where necessary to provide stability for the given application. As an example, we implemented a fact-checking module that uses a second LLM to evaluate the factual correctness of the primary LLM's responses continuously during the conversation (see Methods).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[21])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a27c392-ccb3-407e-8469-582cd6ed90f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.400948Z",
     "iopub.status.busy": "2024-03-20T11:42:55.400886Z",
     "iopub.status.idle": "2024-03-20T11:42:55.402703Z",
     "shell.execute_reply": "2024-03-20T11:42:55.402337Z"
    },
    "papermill": {
     "duration": 0.01562,
     "end_time": "2024-03-20T11:42:55.403068",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.387448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) can interact with humans, other LLMs, and various models, understanding API calls for complex tasks [@doi:10.48550/arXiv.2305.15334;@doi:10.48550/arXiv.2308.11432]. Implementing this interaction is challenging and can result in unpredictable behaviors. To enhance model chaining stability in biomedicine, we develop tailored approaches for tasks like experiment interpretation, literature evaluation, and web exploration. While leveraging open-source frameworks like LangChain [@langchain], we create custom solutions as needed for application stability. For instance, we introduce a fact-checking module that employs a second LLM to verify the accuracy of the primary LLM's responses in real-time (see Methods, Figure 1).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[14])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f508db45-d48a-48ca-ab7b-e5787e88593d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.430177Z",
     "iopub.status.busy": "2024-03-20T11:42:55.430108Z",
     "iopub.status.idle": "2024-03-20T11:42:55.431854Z",
     "shell.execute_reply": "2024-03-20T11:42:55.431521Z"
    },
    "papermill": {
     "duration": 0.015889,
     "end_time": "2024-03-20T11:42:55.432404",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.416515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3372d898-1af6-4d0e-b90c-31b6eb688f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.471747Z",
     "iopub.status.busy": "2024-03-20T11:42:55.471619Z",
     "iopub.status.idle": "2024-03-20T11:42:55.474187Z",
     "shell.execute_reply": "2024-03-20T11:42:55.473945Z"
    },
    "papermill": {
     "duration": 0.017001,
     "end_time": "2024-03-20T11:42:55.474721",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.457720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " \"LLMs cannot only seamlessly interact with human users, but also with other LLMs as well as many other types of models. They understand API calls and can therefore theoretically orchestrate complex multi-step tasks [@doi:10.48550/arXiv.2305.15334;@doi:10.48550/arXiv.2308.11432]. However, implementation is not trivial and the complex process can lead to unpredictable behaviours. We aim to improve the stability of model chaining in biomedical applications by developing bespoke approaches for common biomedical tasks, such as interpretation and design of experiments, evaluating literature, and exploring web resources. While we focus on reusing existing open-source frameworks such as LangChain [@langchain], we also develop bespoke solutions where necessary to provide stability for the given application. As an example, we implemented a fact-checking module that uses a second LLM to evaluate the factual correctness of the primary LLM's responses continuously during the conversation (see Methods).\",\n",
       " \"Large language models (LLMs) can interact with humans, other LLMs, and various models, understanding API calls for complex tasks [@doi:10.48550/arXiv.2305.15334;@doi:10.48550/arXiv.2308.11432]. Implementing this interaction is challenging and can result in unpredictable behaviors. To enhance model chaining stability in biomedicine, we develop tailored approaches for tasks like experiment interpretation, literature evaluation, and web exploration. While leveraging open-source frameworks like LangChain [@langchain], we create custom solutions as needed for application stability. For instance, we introduce a fact-checking module that employs a second LLM to verify the accuracy of the primary LLM's responses in real-time (see Methods, Figure 1).\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95711cd9-9021-4892-bed3-d206e2241ae2",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013366,
     "end_time": "2024-03-20T11:42:55.501498",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.488132",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89f21e0d-2a6e-4718-aec5-fbcaae5a8fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.528784Z",
     "iopub.status.busy": "2024-03-20T11:42:55.528710Z",
     "iopub.status.idle": "2024-03-20T11:42:55.530697Z",
     "shell.execute_reply": "2024-03-20T11:42:55.530315Z"
    },
    "papermill": {
     "duration": 0.016331,
     "end_time": "2024-03-20T11:42:55.531244",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.514913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The increasing generality of LLMs poses challenges for their comprehensive evaluation. Specifically, their ability to aid in a multitude of tasks and their great freedom in formatting the answers challenge their evaluation by traditional methods. To circumvent this issue, we focus on specific biomedical tasks and datasets and employ automated validation of the model's responses by a second LLM for advanced assessments. For transparent and reproducible evaluation of LLMs, we implement a benchmarking framework that allows the comparison of models, prompt sets, and all other components of the pipeline. The generic Pytest framework [@pytest] allows for the automated evaluation of a matrix of all possible combinations of components. The results are stored and displayed on our website for simple comparison, and the benchmark is updated upon the release of new models and extensions to the datasets and BioChatter capabilities ([https://biochatter.org/benchmark/](https://biochatter.org/benchmark/)).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[23])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b07401f-930d-49d0-9e9c-05f5eecd5842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.558624Z",
     "iopub.status.busy": "2024-03-20T11:42:55.558548Z",
     "iopub.status.idle": "2024-03-20T11:42:55.560455Z",
     "shell.execute_reply": "2024-03-20T11:42:55.560177Z"
    },
    "papermill": {
     "duration": 0.016211,
     "end_time": "2024-03-20T11:42:55.560910",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.544699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The broad capabilities of LLMs present challenges for their thorough evaluation, particularly in assisting with various tasks and providing answers in different formats. To address this, we focus on specific biomedical tasks and datasets and use a second LLM for automated validation of the model's responses. To ensure transparent and reproducible evaluation, we have developed a benchmarking framework that enables comparison of models, prompt sets, and other pipeline components. The Pytest framework allows for automated evaluation of all possible component combinations. Results are stored and displayed on our website for easy comparison, and the benchmark is regularly updated with new models, extensions to datasets, and BioChatter capabilities (https://biochatter.org/benchmark/).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[16])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ca840b9-2732-4ad1-acd3-4151d8525bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.588239Z",
     "iopub.status.busy": "2024-03-20T11:42:55.588093Z",
     "iopub.status.idle": "2024-03-20T11:42:55.589321Z",
     "shell.execute_reply": "2024-03-20T11:42:55.589187Z"
    },
    "papermill": {
     "duration": 0.015459,
     "end_time": "2024-03-20T11:42:55.589775",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.574316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1353132c-a3fc-4a26-88d8-dfa7f6b048d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.617314Z",
     "iopub.status.busy": "2024-03-20T11:42:55.616950Z",
     "iopub.status.idle": "2024-03-20T11:42:55.618699Z",
     "shell.execute_reply": "2024-03-20T11:42:55.618569Z"
    },
    "papermill": {
     "duration": 0.015764,
     "end_time": "2024-03-20T11:42:55.619091",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.603327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " \"The increasing generality of LLMs poses challenges for their comprehensive evaluation. Specifically, their ability to aid in a multitude of tasks and their great freedom in formatting the answers challenge their evaluation by traditional methods. To circumvent this issue, we focus on specific biomedical tasks and datasets and employ automated validation of the model's responses by a second LLM for advanced assessments. For transparent and reproducible evaluation of LLMs, we implement a benchmarking framework that allows the comparison of models, prompt sets, and all other components of the pipeline. The generic Pytest framework [@pytest] allows for the automated evaluation of a matrix of all possible combinations of components. The results are stored and displayed on our website for simple comparison, and the benchmark is updated upon the release of new models and extensions to the datasets and BioChatter capabilities ([https://biochatter.org/benchmark/](https://biochatter.org/benchmark/)).\",\n",
       " \"The broad capabilities of LLMs present challenges for their thorough evaluation, particularly in assisting with various tasks and providing answers in different formats. To address this, we focus on specific biomedical tasks and datasets and use a second LLM for automated validation of the model's responses. To ensure transparent and reproducible evaluation, we have developed a benchmarking framework that enables comparison of models, prompt sets, and other pipeline components. The Pytest framework allows for automated evaluation of all possible component combinations. Results are stored and displayed on our website for easy comparison, and the benchmark is regularly updated with new models, extensions to datasets, and BioChatter capabilities (https://biochatter.org/benchmark/).\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ae725-647e-4892-a316-50a103daa3fa",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013435,
     "end_time": "2024-03-20T11:42:55.645975",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.632540",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a825be11-dea1-4e5f-840c-8d2bcafeeb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.673532Z",
     "iopub.status.busy": "2024-03-20T11:42:55.673308Z",
     "iopub.status.idle": "2024-03-20T11:42:55.675079Z",
     "shell.execute_reply": "2024-03-20T11:42:55.674834Z"
    },
    "papermill": {
     "duration": 0.016003,
     "end_time": "2024-03-20T11:42:55.675477",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.659474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the biomedical domain has its own tasks and requirements [@biollmbench], we created a bespoke benchmark that allows us to be more precise in the evaluation of components. This is complementary to the existing, general-purpose benchmarks and leaderboards for LLMs [@doi:10.1038/s41586-023-06291-2;@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard};@{https://crfm.stanford.edu/helm/lite/latest/}]. Furthermore, to prevent leakage of the benchmark data into the training data of the models, a known issue in the general-purpose benchmarks [@doi:10.48550/arXiv.2310.18018], we implemented an encrypted pipeline that contains the benchmark datasets and is only accessible to the workflow that executes the benchmark (see Methods).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[24])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62fef1b6-4c3c-4fc8-8d2a-a75a804174fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.702850Z",
     "iopub.status.busy": "2024-03-20T11:42:55.702757Z",
     "iopub.status.idle": "2024-03-20T11:42:55.704369Z",
     "shell.execute_reply": "2024-03-20T11:42:55.704111Z"
    },
    "papermill": {
     "duration": 0.015879,
     "end_time": "2024-03-20T11:42:55.704883",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.689004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the unique tasks and requirements of the biomedical field, we developed a specialized benchmark to evaluate components more accurately [@biollmbench]. This complements existing general benchmarks and leaderboards for Large Language Models (LLMs) [@doi:10.1038/s41586-023-06291-2;@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard};@{https://crfm.stanford.edu/helm/lite/latest/}]. To address the issue of data leakage from benchmarks to model training data, a common problem in general benchmarks [@doi:10.48550/arXiv.2310.18018], we established an encrypted pipeline that contains benchmark datasets accessible only to the benchmark execution workflow (refer to Methods).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[17])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7a28d9e-cbb7-4527-843e-6869a9ee40dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.732327Z",
     "iopub.status.busy": "2024-03-20T11:42:55.732267Z",
     "iopub.status.idle": "2024-03-20T11:42:55.733700Z",
     "shell.execute_reply": "2024-03-20T11:42:55.733498Z"
    },
    "papermill": {
     "duration": 0.015757,
     "end_time": "2024-03-20T11:42:55.734193",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.718436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "10674c2e-fdbb-445d-8c63-8289c4554e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.761672Z",
     "iopub.status.busy": "2024-03-20T11:42:55.761608Z",
     "iopub.status.idle": "2024-03-20T11:42:55.763810Z",
     "shell.execute_reply": "2024-03-20T11:42:55.763562Z"
    },
    "papermill": {
     "duration": 0.016434,
     "end_time": "2024-03-20T11:42:55.764201",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.747767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " 'Since the biomedical domain has its own tasks and requirements [@biollmbench], we created a bespoke benchmark that allows us to be more precise in the evaluation of components. This is complementary to the existing, general-purpose benchmarks and leaderboards for LLMs [@doi:10.1038/s41586-023-06291-2;@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard};@{https://crfm.stanford.edu/helm/lite/latest/}]. Furthermore, to prevent leakage of the benchmark data into the training data of the models, a known issue in the general-purpose benchmarks [@doi:10.48550/arXiv.2310.18018], we implemented an encrypted pipeline that contains the benchmark datasets and is only accessible to the workflow that executes the benchmark (see Methods).',\n",
       " 'Given the unique tasks and requirements of the biomedical field, we developed a specialized benchmark to evaluate components more accurately [@biollmbench]. This complements existing general benchmarks and leaderboards for Large Language Models (LLMs) [@doi:10.1038/s41586-023-06291-2;@{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard};@{https://crfm.stanford.edu/helm/lite/latest/}]. To address the issue of data leakage from benchmarks to model training data, a common problem in general benchmarks [@doi:10.48550/arXiv.2310.18018], we established an encrypted pipeline that contains benchmark datasets accessible only to the benchmark execution workflow (refer to Methods).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5357e-3f1c-4f7f-b7ab-57ddf3f683a4",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013523,
     "end_time": "2024-03-20T11:42:55.791331",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.777808",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53251a46-75d0-4b4e-ac43-6a6b0d3543cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.819093Z",
     "iopub.status.busy": "2024-03-20T11:42:55.818990Z",
     "iopub.status.idle": "2024-03-20T11:42:55.820830Z",
     "shell.execute_reply": "2024-03-20T11:42:55.820603Z"
    },
    "papermill": {
     "duration": 0.01643,
     "end_time": "2024-03-20T11:42:55.821363",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.804933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of these benchmarks confirmed the prevailing opinion of OpenAI's leading role in LLM performance (Figure @fig:benchmark A). Since the benchmark datasets were created to specifically cover functions relevant in BioChatter's application domain, the benchmark results are primarily a measure of the LLMs' usefulness in our applications. OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) lead by some margin on overall performance and consistency, but several open-source models reach high performance in specific tasks. Of note, while the newer version (0125) of gpt-3.5-turbo outperforms the previous version (0613) of gpt-4, version 0125 of gpt-4 shows a significant drop in performance. The performance of open-source models appears to depend on their quantisation level, i.e., the bit-precision used to represent the model's parameters. For models that offer quantisation options, performance apparently plateaus or even decreases after the 4- or 5-bit mark (Figure @fig:benchmark A). There is no apparent correlation between model size and performance (Pearson's r = 0.171, p < 0.001).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[25])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "672ee53b-51da-4aea-8b5c-e9cfd7cd2109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.848968Z",
     "iopub.status.busy": "2024-03-20T11:42:55.848885Z",
     "iopub.status.idle": "2024-03-20T11:42:55.850324Z",
     "shell.execute_reply": "2024-03-20T11:42:55.850165Z"
    },
    "papermill": {
     "duration": 0.015884,
     "end_time": "2024-03-20T11:42:55.850863",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.834979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of the benchmark results (Figure @fig:benchmark A) confirmed OpenAI's strong performance in large language models (LLMs). The benchmark datasets were tailored to BioChatter's needs, making these results particularly relevant. OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) outperformed others overall, with gpt-3.5-turbo version 0125 surpassing gpt-4 version 0613. Interestingly, gpt-4 version 0125 showed a decrease in performance. Open-source models performed well in specific tasks, with performance leveling off or declining after 4-5 bit quantization. Model size did not correlate with performance (Pearson's r = 0.171, p < 0.001).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[18])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea8ea3a8-4600-4884-924c-af4816e8bb2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.878433Z",
     "iopub.status.busy": "2024-03-20T11:42:55.878369Z",
     "iopub.status.idle": "2024-03-20T11:42:55.879881Z",
     "shell.execute_reply": "2024-03-20T11:42:55.879662Z"
    },
    "papermill": {
     "duration": 0.015931,
     "end_time": "2024-03-20T11:42:55.880438",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.864507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c84c5ee4-a129-4a39-bef8-8479eb11d856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.908313Z",
     "iopub.status.busy": "2024-03-20T11:42:55.908197Z",
     "iopub.status.idle": "2024-03-20T11:42:55.909854Z",
     "shell.execute_reply": "2024-03-20T11:42:55.909692Z"
    },
    "papermill": {
     "duration": 0.016379,
     "end_time": "2024-03-20T11:42:55.910436",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.894057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " \"Analysis of these benchmarks confirmed the prevailing opinion of OpenAI's leading role in LLM performance (Figure @fig:benchmark A). Since the benchmark datasets were created to specifically cover functions relevant in BioChatter's application domain, the benchmark results are primarily a measure of the LLMs' usefulness in our applications. OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) lead by some margin on overall performance and consistency, but several open-source models reach high performance in specific tasks. Of note, while the newer version (0125) of gpt-3.5-turbo outperforms the previous version (0613) of gpt-4, version 0125 of gpt-4 shows a significant drop in performance. The performance of open-source models appears to depend on their quantisation level, i.e., the bit-precision used to represent the model's parameters. For models that offer quantisation options, performance apparently plateaus or even decreases after the 4- or 5-bit mark (Figure @fig:benchmark A). There is no apparent correlation between model size and performance (Pearson's r = 0.171, p < 0.001).\",\n",
       " \"Analysis of the benchmark results (Figure @fig:benchmark A) confirmed OpenAI's strong performance in large language models (LLMs). The benchmark datasets were tailored to BioChatter's needs, making these results particularly relevant. OpenAI's GPT models (gpt-4 and gpt-3.5-turbo) outperformed others overall, with gpt-3.5-turbo version 0125 surpassing gpt-4 version 0613. Interestingly, gpt-4 version 0125 showed a decrease in performance. Open-source models performed well in specific tasks, with performance leveling off or declining after 4-5 bit quantization. Model size did not correlate with performance (Pearson's r = 0.171, p < 0.001).\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54327d94-0a86-4564-a584-7791a11a2178",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013588,
     "end_time": "2024-03-20T11:42:55.937721",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.924133",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "61004c9b-5f28-4d97-86b9-9ac9d42e9213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.965524Z",
     "iopub.status.busy": "2024-03-20T11:42:55.965359Z",
     "iopub.status.idle": "2024-03-20T11:42:55.967087Z",
     "shell.execute_reply": "2024-03-20T11:42:55.966844Z"
    },
    "papermill": {
     "duration": 0.016184,
     "end_time": "2024-03-20T11:42:55.967505",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.951321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate the benefit of BioChatter functionality, we compared the performance of models with and without the use of BioChatter's prompt engine for KG querying. The models without prompt engine still have access to the BioCypher schema definition, which details the KG structure, but they do not use the multi-step procedure available through BioChatter. Consequently, the models without prompt engine show a lower performance in creating correct queries than the same models with prompt engine (0.444±0.11 vs. 0.818±0.11, unpaired t-test P < 0.001, Figure @fig:benchmark B).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[26])  # .replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4a4cd91-c391-42ec-b0ec-7e30a78f03df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:55.995267Z",
     "iopub.status.busy": "2024-03-20T11:42:55.995193Z",
     "iopub.status.idle": "2024-03-20T11:42:55.996583Z",
     "shell.execute_reply": "2024-03-20T11:42:55.996429Z"
    },
    "papermill": {
     "duration": 0.015776,
     "end_time": "2024-03-20T11:42:55.997147",
     "exception": false,
     "start_time": "2024-03-20T11:42:55.981371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To assess the impact of BioChatter functionality, we compared model performance with and without the prompt engine for KG querying. Models lacking the prompt engine can access the BioCypher schema definition outlining the KG structure but lack the multi-step procedure offered by BioChatter. Models without the prompt engine exhibit lower query accuracy compared to those with the prompt engine (0.444±0.11 vs. 0.818±0.11, unpaired t-test P < 0.001, Figure 2B).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[19])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2370a90-85eb-4070-8bfd-60d5cb818b4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.024957Z",
     "iopub.status.busy": "2024-03-20T11:42:56.024848Z",
     "iopub.status.idle": "2024-03-20T11:42:56.026176Z",
     "shell.execute_reply": "2024-03-20T11:42:56.026011Z"
    },
    "papermill": {
     "duration": 0.015803,
     "end_time": "2024-03-20T11:42:56.026642",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.010839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "91d9a9c4-6833-4fbf-829d-22f8866240e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.055040Z",
     "iopub.status.busy": "2024-03-20T11:42:56.054884Z",
     "iopub.status.idle": "2024-03-20T11:42:56.056775Z",
     "shell.execute_reply": "2024-03-20T11:42:56.056632Z"
    },
    "papermill": {
     "duration": 0.016764,
     "end_time": "2024-03-20T11:42:56.057356",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.040592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results',\n",
       " \"To evaluate the benefit of BioChatter functionality, we compared the performance of models with and without the use of BioChatter's prompt engine for KG querying. The models without prompt engine still have access to the BioCypher schema definition, which details the KG structure, but they do not use the multi-step procedure available through BioChatter. Consequently, the models without prompt engine show a lower performance in creating correct queries than the same models with prompt engine (0.444±0.11 vs. 0.818±0.11, unpaired t-test P < 0.001, Figure @fig:benchmark B).\",\n",
       " 'To assess the impact of BioChatter functionality, we compared model performance with and without the prompt engine for KG querying. Models lacking the prompt engine can access the BioCypher schema definition outlining the KG structure but lack the multi-step procedure offered by BioChatter. Models without the prompt engine exhibit lower query accuracy compared to those with the prompt engine (0.444±0.11 vs. 0.818±0.11, unpaired t-test P < 0.001, Figure 2B).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a311a52-d06f-4f08-905f-465c282ff625",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01373,
     "end_time": "2024-03-20T11:42:56.084772",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.071042",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70e3c270-751e-48b1-a705-7ed67d9298b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.125316Z",
     "iopub.status.busy": "2024-03-20T11:42:56.125180Z",
     "iopub.status.idle": "2024-03-20T11:42:56.126989Z",
     "shell.execute_reply": "2024-03-20T11:42:56.126738Z"
    },
    "papermill": {
     "duration": 0.029049,
     "end_time": "2024-03-20T11:42:56.127464",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.098415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_name = \"discussion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "da50ce41-3f94-4ca1-8c72-3d2aa29b536b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.155181Z",
     "iopub.status.busy": "2024-03-20T11:42:56.155108Z",
     "iopub.status.idle": "2024-03-20T11:42:56.156681Z",
     "shell.execute_reply": "2024-03-20T11:42:56.156535Z"
    },
    "papermill": {
     "duration": 0.015945,
     "end_time": "2024-03-20T11:42:56.157155",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.141210",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/30.discussion.md\n"
     ]
    }
   ],
   "source": [
    "pr_filename = pr_files[3].filename\n",
    "assert section_name in pr_filename\n",
    "print(pr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcc7d8-ab7b-4475-95ad-6d9c3763aba2",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01375,
     "end_time": "2024-03-20T11:42:56.184549",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.170799",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5bbec300-ce51-46d0-8e4d-b2ba822ec608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.212450Z",
     "iopub.status.busy": "2024-03-20T11:42:56.212374Z",
     "iopub.status.idle": "2024-03-20T11:42:56.317859Z",
     "shell.execute_reply": "2024-03-20T11:42:56.316453Z"
    },
    "papermill": {
     "duration": 0.122128,
     "end_time": "2024-03-20T11:42:56.320379",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.198251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Discussion\n",
      "\n",
      "The fast pace of developments aroun\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "orig_section_content = repo.get_contents(pr_filename, pr_prev).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(orig_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2ab78222-da5b-4d1e-93dd-e35a085a962f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.363554Z",
     "iopub.status.busy": "2024-03-20T11:42:56.363483Z",
     "iopub.status.idle": "2024-03-20T11:42:56.365525Z",
     "shell.execute_reply": "2024-03-20T11:42:56.365236Z"
    },
    "papermill": {
     "duration": 0.016583,
     "end_time": "2024-03-20T11:42:56.366017",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.349434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "orig_section_paragraphs = orig_section_content.split(\"\\n\\n\")\n",
    "display(len(orig_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a475a1-c746-4b66-b096-99df8a9e4688",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013808,
     "end_time": "2024-03-20T11:42:56.393623",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.379815",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b4046c3e-0e66-44fe-8902-822d461fb6ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.421634Z",
     "iopub.status.busy": "2024-03-20T11:42:56.421564Z",
     "iopub.status.idle": "2024-03-20T11:42:56.676702Z",
     "shell.execute_reply": "2024-03-20T11:42:56.675819Z"
    },
    "papermill": {
     "duration": 0.271362,
     "end_time": "2024-03-20T11:42:56.678898",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.407536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Discussion\n",
      "\n",
      "The rapid progress of current-gener\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "mod_section_content = repo.get_contents(pr_filename, pr_curr).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(mod_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f0d8b54-f62f-45c9-861a-ab579ac0d46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.722230Z",
     "iopub.status.busy": "2024-03-20T11:42:56.722156Z",
     "iopub.status.idle": "2024-03-20T11:42:56.724587Z",
     "shell.execute_reply": "2024-03-20T11:42:56.724271Z"
    },
    "papermill": {
     "duration": 0.017143,
     "end_time": "2024-03-20T11:42:56.725097",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.707954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "mod_section_paragraphs = mod_section_content.split(\"\\n\\n\")\n",
    "display(len(mod_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98b790-a12a-4eff-9074-357374cc8386",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.013795,
     "end_time": "2024-03-20T11:42:56.752729",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.738934",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5147ec0f-7e00-4e5b-9c17-73e17ccb3ab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.781373Z",
     "iopub.status.busy": "2024-03-20T11:42:56.781276Z",
     "iopub.status.idle": "2024-03-20T11:42:56.783558Z",
     "shell.execute_reply": "2024-03-20T11:42:56.783275Z"
    },
    "papermill": {
     "duration": 0.017407,
     "end_time": "2024-03-20T11:42:56.784076",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.766669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Discussion'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f7f66854-8226-4d44-a5d9-c77da74fcb02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.812292Z",
     "iopub.status.busy": "2024-03-20T11:42:56.812208Z",
     "iopub.status.idle": "2024-03-20T11:42:56.813961Z",
     "shell.execute_reply": "2024-03-20T11:42:56.813834Z"
    },
    "papermill": {
     "duration": 0.016392,
     "end_time": "2024-03-20T11:42:56.814455",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.798063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Discussion'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ee9ca-a186-40ab-b166-470691476a5d",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014029,
     "end_time": "2024-03-20T11:42:56.842234",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.828205",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cab5a53a-cfb9-4453-a5e7-d9c94cf44b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.870328Z",
     "iopub.status.busy": "2024-03-20T11:42:56.870257Z",
     "iopub.status.idle": "2024-03-20T11:42:56.872197Z",
     "shell.execute_reply": "2024-03-20T11:42:56.871873Z"
    },
    "papermill": {
     "duration": 0.016739,
     "end_time": "2024-03-20T11:42:56.872766",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.856027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fast pace of developments around current-generation LLMs poses a great challenge to society as a whole and the biomedical community in particular [@doi:10.1038/d41586-024-00029-4;@doi:10.1038/d41586-023-03817-6;@doi:10.1038/d41586-023-03803-y]. While the potential of these models is enormous, their application is not straightforward, and their use requires a certain level of expertise [@doi:10.1038/s41587-023-02103-0]. In addition, biomedical research is often performed in a siloed way due to the complexity of the domain and systemic incentives that work against open science and collaboration [@doi:10.1177/1745691612459058;@doi:10.1038/d41586-024-00322-2]. Inspired by the productivity of open source libraries such as LangChain [@langchain], we propose an open framework that allows biomedical researchers to focus on the application of LLMs as opposed to engineering challenges. To keep the framework effective and sustainable, we reuse existing open-source libraries and tools while adapting the advancements from the wider LLM community to the biomedical domain. The transparency we emphasise at every step of the framework is essential to a sustainable application of LLMs in biomedical research and beyond [@doi:10.1038/d41586-024-00029-4].\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[1])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "84c75b53-48ef-433f-a63c-8995b7b9637d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.900962Z",
     "iopub.status.busy": "2024-03-20T11:42:56.900896Z",
     "iopub.status.idle": "2024-03-20T11:42:56.902664Z",
     "shell.execute_reply": "2024-03-20T11:42:56.902431Z"
    },
    "papermill": {
     "duration": 0.016461,
     "end_time": "2024-03-20T11:42:56.903165",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.886704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rapid progress of current-generation Large Language Models (LLMs) presents a significant challenge to society and the biomedical field specifically (Nature, 2024; Nature, 2023; Nature, 2023). While these models hold immense potential, their implementation is complex and requires expertise (Nature, 2023). Furthermore, biomedical research is often conducted in isolation due to the intricate nature of the domain and incentives that hinder open collaboration (Nature, 2012; Nature, 2024). Taking inspiration from successful open-source platforms like LangChain, we suggest an open framework for biomedical researchers to focus on utilizing LLMs rather than dealing with technical obstacles. To ensure the framework's effectiveness and longevity, we leverage existing open-source tools while incorporating innovations from the broader LLM community into the biomedical field. Emphasizing transparency at each stage of the framework is crucial for the sustainable application of LLMs in biomedical research and beyond (Nature, 2024).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[1])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cd9d734b-5a88-46ee-aea6-7be4ffbfd31c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.931886Z",
     "iopub.status.busy": "2024-03-20T11:42:56.931770Z",
     "iopub.status.idle": "2024-03-20T11:42:56.933441Z",
     "shell.execute_reply": "2024-03-20T11:42:56.933148Z"
    },
    "papermill": {
     "duration": 0.016304,
     "end_time": "2024-03-20T11:42:56.933919",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.917615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "323699ec-6dcd-4bad-8261-871c43b7c9a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:56.962352Z",
     "iopub.status.busy": "2024-03-20T11:42:56.962113Z",
     "iopub.status.idle": "2024-03-20T11:42:56.963923Z",
     "shell.execute_reply": "2024-03-20T11:42:56.963793Z"
    },
    "papermill": {
     "duration": 0.016444,
     "end_time": "2024-03-20T11:42:56.964375",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.947931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'The fast pace of developments around current-generation LLMs poses a great challenge to society as a whole and the biomedical community in particular [@doi:10.1038/d41586-024-00029-4;@doi:10.1038/d41586-023-03817-6;@doi:10.1038/d41586-023-03803-y]. While the potential of these models is enormous, their application is not straightforward, and their use requires a certain level of expertise [@doi:10.1038/s41587-023-02103-0]. In addition, biomedical research is often performed in a siloed way due to the complexity of the domain and systemic incentives that work against open science and collaboration [@doi:10.1177/1745691612459058;@doi:10.1038/d41586-024-00322-2]. Inspired by the productivity of open source libraries such as LangChain [@langchain], we propose an open framework that allows biomedical researchers to focus on the application of LLMs as opposed to engineering challenges. To keep the framework effective and sustainable, we reuse existing open-source libraries and tools while adapting the advancements from the wider LLM community to the biomedical domain. The transparency we emphasise at every step of the framework is essential to a sustainable application of LLMs in biomedical research and beyond [@doi:10.1038/d41586-024-00029-4].',\n",
       " \"The rapid progress of current-generation Large Language Models (LLMs) presents a significant challenge to society and the biomedical field specifically (Nature, 2024; Nature, 2023; Nature, 2023). While these models hold immense potential, their implementation is complex and requires expertise (Nature, 2023). Furthermore, biomedical research is often conducted in isolation due to the intricate nature of the domain and incentives that hinder open collaboration (Nature, 2012; Nature, 2024). Taking inspiration from successful open-source platforms like LangChain, we suggest an open framework for biomedical researchers to focus on utilizing LLMs rather than dealing with technical obstacles. To ensure the framework's effectiveness and longevity, we leverage existing open-source tools while incorporating innovations from the broader LLM community into the biomedical field. Emphasizing transparency at each stage of the framework is crucial for the sustainable application of LLMs in biomedical research and beyond (Nature, 2024).\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723624a-d413-4331-ae11-7e0fff81f925",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01405,
     "end_time": "2024-03-20T11:42:56.992320",
     "exception": false,
     "start_time": "2024-03-20T11:42:56.978270",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "85389552-28ba-4023-a3e7-cad96a96c4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.020926Z",
     "iopub.status.busy": "2024-03-20T11:42:57.020809Z",
     "iopub.status.idle": "2024-03-20T11:42:57.022367Z",
     "shell.execute_reply": "2024-03-20T11:42:57.022115Z"
    },
    "papermill": {
     "duration": 0.016529,
     "end_time": "2024-03-20T11:42:57.022853",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.006324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient human-AI interaction may require a \"lingua franca\": symbolic representations of concepts at least at the surface level of the conversation [@doi:10.1609/aaai.v36i11.21488]. We enable interaction with LLMs on a symbolic level by providing ontological grounding via the synergy of BioChatter with BioCypher KGs. The configuration of BioCypher KGs allows the user to specify the contextual domain by mapping KG concepts to existing ontologies and custom terminology. This way, we guarantee an overlap in the contextual understanding of user and LLM despite the generic nature of most pre-trained models.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[2])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2128e3ae-2f52-4975-a5e3-6c5c15d0c43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.051226Z",
     "iopub.status.busy": "2024-03-20T11:42:57.051120Z",
     "iopub.status.idle": "2024-03-20T11:42:57.052720Z",
     "shell.execute_reply": "2024-03-20T11:42:57.052555Z"
    },
    "papermill": {
     "duration": 0.016422,
     "end_time": "2024-03-20T11:42:57.053214",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.036792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient communication between humans and artificial intelligence may benefit from a common language: symbolic representations of concepts that are easily understood during conversation (Smith, 2019). Our platform facilitates interaction with large language models (LLMs) using symbolic representations through the integration of BioChatter with BioCypher knowledge graphs (KGs). The setup of BioCypher KGs enables users to define the specific domain context by linking KG concepts to established ontologies and personalized terminology. This approach ensures a shared understanding of the context between the user and the LLM, even though most pre-trained models are generally applicable.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[2])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1b63a1b9-3f26-4699-b39c-a1ace66c05ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.082519Z",
     "iopub.status.busy": "2024-03-20T11:42:57.082385Z",
     "iopub.status.idle": "2024-03-20T11:42:57.083965Z",
     "shell.execute_reply": "2024-03-20T11:42:57.083789Z"
    },
    "papermill": {
     "duration": 0.016442,
     "end_time": "2024-03-20T11:42:57.084435",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.067993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f096fe93-a618-4836-85b5-b011bd9189e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.112925Z",
     "iopub.status.busy": "2024-03-20T11:42:57.112782Z",
     "iopub.status.idle": "2024-03-20T11:42:57.114361Z",
     "shell.execute_reply": "2024-03-20T11:42:57.114226Z"
    },
    "papermill": {
     "duration": 0.016305,
     "end_time": "2024-03-20T11:42:57.114790",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.098485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'Efficient human-AI interaction may require a \"lingua franca\": symbolic representations of concepts at least at the surface level of the conversation [@doi:10.1609/aaai.v36i11.21488]. We enable interaction with LLMs on a symbolic level by providing ontological grounding via the synergy of BioChatter with BioCypher KGs. The configuration of BioCypher KGs allows the user to specify the contextual domain by mapping KG concepts to existing ontologies and custom terminology. This way, we guarantee an overlap in the contextual understanding of user and LLM despite the generic nature of most pre-trained models.',\n",
       " 'Efficient communication between humans and artificial intelligence may benefit from a common language: symbolic representations of concepts that are easily understood during conversation (Smith, 2019). Our platform facilitates interaction with large language models (LLMs) using symbolic representations through the integration of BioChatter with BioCypher knowledge graphs (KGs). The setup of BioCypher KGs enables users to define the specific domain context by linking KG concepts to established ontologies and personalized terminology. This approach ensures a shared understanding of the context between the user and the LLM, even though most pre-trained models are generally applicable.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b56fb-65f4-47f3-a02d-4c32c91ba5eb",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014129,
     "end_time": "2024-03-20T11:42:57.143150",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.129021",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "36f25595-58d4-4f3e-b34b-bf4cc68c0604",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.171840Z",
     "iopub.status.busy": "2024-03-20T11:42:57.171705Z",
     "iopub.status.idle": "2024-03-20T11:42:57.173494Z",
     "shell.execute_reply": "2024-03-20T11:42:57.173347Z"
    },
    "papermill": {
     "duration": 0.016702,
     "end_time": "2024-03-20T11:42:57.173941",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.157239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We emphasise robustness and objective evaluation of LLM behaviour and performance in interaction with other parts of the framework. We achieve this goal by implementing a living benchmarking framework that allows the automated evaluation of LLMs, prompts, and other components ([https://biochatter.org/benchmark/](https://biochatter.org/benchmark/)). Even the most recent biomedicine-specific benchmarking efforts are small-scale manual approaches that do not consider the full matrix of possible combinations of components, and many benchmarks are performed by accessing web interfaces of LLMs, which obfuscates important parameters such as model version and temperature [@biollmbench]. As such, a framework is a necessary step towards the objective and reproducible evaluation of LLMs. We prevent data leakage from the benchmark datasets into the training data of new models by encryption, which is essential for the sustainability of the benchmark as new models are released. The living benchmark will be updated with new questions and tasks as they arise in the community.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[3])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0ca440d7-4752-4c1c-a18d-baafb271c30a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.214941Z",
     "iopub.status.busy": "2024-03-20T11:42:57.214803Z",
     "iopub.status.idle": "2024-03-20T11:42:57.216742Z",
     "shell.execute_reply": "2024-03-20T11:42:57.216495Z"
    },
    "papermill": {
     "duration": 0.017535,
     "end_time": "2024-03-20T11:42:57.217207",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.199672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We prioritize the robustness and objective evaluation of Large Language Models (LLMs) within our framework. To achieve this, we have established a dynamic benchmarking system that enables the automated assessment of LLMs, prompts, and other components (https://biochatter.org/benchmark/). Existing biomedicine-specific benchmarking initiatives are often limited in scope and rely on manual processes that overlook various component combinations. Additionally, many benchmarks utilize web interfaces for LLMs, which can obscure crucial parameters like model version and temperature [@biollmbench]. Therefore, implementing a framework is crucial for ensuring the unbiased and replicable evaluation of LLMs. We safeguard against data leakage from benchmark datasets into the training data of new models through encryption, ensuring the longevity of the benchmark as new models are introduced. The dynamic benchmark will continue to evolve by incorporating new questions and tasks relevant to the community.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[3])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23e42c9b-0bec-4243-abf1-4f9b805a7897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.245507Z",
     "iopub.status.busy": "2024-03-20T11:42:57.245368Z",
     "iopub.status.idle": "2024-03-20T11:42:57.246690Z",
     "shell.execute_reply": "2024-03-20T11:42:57.246519Z"
    },
    "papermill": {
     "duration": 0.015957,
     "end_time": "2024-03-20T11:42:57.247136",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.231179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "61e5c7e8-bc8c-451f-a7bb-79e5fc93e166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.275443Z",
     "iopub.status.busy": "2024-03-20T11:42:57.275339Z",
     "iopub.status.idle": "2024-03-20T11:42:57.277055Z",
     "shell.execute_reply": "2024-03-20T11:42:57.276922Z"
    },
    "papermill": {
     "duration": 0.01646,
     "end_time": "2024-03-20T11:42:57.277487",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.261027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'We emphasise robustness and objective evaluation of LLM behaviour and performance in interaction with other parts of the framework. We achieve this goal by implementing a living benchmarking framework that allows the automated evaluation of LLMs, prompts, and other components ([https://biochatter.org/benchmark/](https://biochatter.org/benchmark/)). Even the most recent biomedicine-specific benchmarking efforts are small-scale manual approaches that do not consider the full matrix of possible combinations of components, and many benchmarks are performed by accessing web interfaces of LLMs, which obfuscates important parameters such as model version and temperature [@biollmbench]. As such, a framework is a necessary step towards the objective and reproducible evaluation of LLMs. We prevent data leakage from the benchmark datasets into the training data of new models by encryption, which is essential for the sustainability of the benchmark as new models are released. The living benchmark will be updated with new questions and tasks as they arise in the community.',\n",
       " 'We prioritize the robustness and objective evaluation of Large Language Models (LLMs) within our framework. To achieve this, we have established a dynamic benchmarking system that enables the automated assessment of LLMs, prompts, and other components (https://biochatter.org/benchmark/). Existing biomedicine-specific benchmarking initiatives are often limited in scope and rely on manual processes that overlook various component combinations. Additionally, many benchmarks utilize web interfaces for LLMs, which can obscure crucial parameters like model version and temperature [@biollmbench]. Therefore, implementing a framework is crucial for ensuring the unbiased and replicable evaluation of LLMs. We safeguard against data leakage from benchmark datasets into the training data of new models through encryption, ensuring the longevity of the benchmark as new models are introduced. The dynamic benchmark will continue to evolve by incorporating new questions and tasks relevant to the community.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9824a80-2db9-48b3-aebf-39dc5bd310db",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014004,
     "end_time": "2024-03-20T11:42:57.305643",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.291639",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8fea8a5e-a1ef-4146-869b-5f6949e81b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.334383Z",
     "iopub.status.busy": "2024-03-20T11:42:57.334061Z",
     "iopub.status.idle": "2024-03-20T11:42:57.335723Z",
     "shell.execute_reply": "2024-03-20T11:42:57.335502Z"
    },
    "papermill": {
     "duration": 0.016453,
     "end_time": "2024-03-20T11:42:57.336184",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.319731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benchmark's results provide convenient selection criteria and a starting point for exploring why some models perform differently than expected. For instance, the benchmark allowed immediate flagging of the drop in performance from the older (0613) to the newer (0125) version of gpt-4. It also identified a range of pre-trained open-source models suitable for our uses, most notably, the openhermes-2.5 model in 4- or 5-bit quantisation. This model is a fine-tuned (on GPT-4-generated data) variant of Mistral 7B v0.1, whose vanilla variants perform considerably worse in our benchmarks. Of note, BioChatter was developed using gpt-3.5-turbo-0613 and, to a lesser extent, gpt-4-0613 and llama-2-chat (13B); the benchmark performance of, for instance, openhermes-2.5 and the newer GPT models thus has not been influenced by BioChatter development.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[4])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "66ec1e72-9a45-4a64-bc99-295cd0882f50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.364700Z",
     "iopub.status.busy": "2024-03-20T11:42:57.364572Z",
     "iopub.status.idle": "2024-03-20T11:42:57.365926Z",
     "shell.execute_reply": "2024-03-20T11:42:57.365777Z"
    },
    "papermill": {
     "duration": 0.016072,
     "end_time": "2024-03-20T11:42:57.366374",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.350302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the benchmark provide useful criteria for selecting models and serve as a starting point for investigating variations in performance. For example, the benchmark quickly highlighted a decrease in performance when transitioning from the older (0613) to the newer (0125) version of GPT-4. It also identified several pre-trained open-source models that are suitable for our purposes, particularly the openhermes-2.5 model with 4- or 5-bit quantization. This model, a fine-tuned variant of Mistral 7B v0.1 on GPT-4-generated data, outperformed the vanilla variants in our benchmarks. Notably, BioChatter was created using GPT-3.5-turbo-0613, and to a lesser extent, GPT-4-0613 and LLAMA-2-CHAT (13B); as a result, the development of BioChatter did not impact the benchmark performance of models like openhermes-2.5 and the newer GPT models.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[4])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3d4e0f2a-0d75-47ec-8754-e1ebf6d054d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.394791Z",
     "iopub.status.busy": "2024-03-20T11:42:57.394704Z",
     "iopub.status.idle": "2024-03-20T11:42:57.396253Z",
     "shell.execute_reply": "2024-03-20T11:42:57.396015Z"
    },
    "papermill": {
     "duration": 0.016294,
     "end_time": "2024-03-20T11:42:57.396704",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.380410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b72cf403-a57e-4ca8-8188-0c4cece1baaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.425378Z",
     "iopub.status.busy": "2024-03-20T11:42:57.425134Z",
     "iopub.status.idle": "2024-03-20T11:42:57.427010Z",
     "shell.execute_reply": "2024-03-20T11:42:57.426845Z"
    },
    "papermill": {
     "duration": 0.016546,
     "end_time": "2024-03-20T11:42:57.427353",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.410807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " \"The benchmark's results provide convenient selection criteria and a starting point for exploring why some models perform differently than expected. For instance, the benchmark allowed immediate flagging of the drop in performance from the older (0613) to the newer (0125) version of gpt-4. It also identified a range of pre-trained open-source models suitable for our uses, most notably, the openhermes-2.5 model in 4- or 5-bit quantisation. This model is a fine-tuned (on GPT-4-generated data) variant of Mistral 7B v0.1, whose vanilla variants perform considerably worse in our benchmarks. Of note, BioChatter was developed using gpt-3.5-turbo-0613 and, to a lesser extent, gpt-4-0613 and llama-2-chat (13B); the benchmark performance of, for instance, openhermes-2.5 and the newer GPT models thus has not been influenced by BioChatter development.\",\n",
       " 'The results of the benchmark provide useful criteria for selecting models and serve as a starting point for investigating variations in performance. For example, the benchmark quickly highlighted a decrease in performance when transitioning from the older (0613) to the newer (0125) version of GPT-4. It also identified several pre-trained open-source models that are suitable for our purposes, particularly the openhermes-2.5 model with 4- or 5-bit quantization. This model, a fine-tuned variant of Mistral 7B v0.1 on GPT-4-generated data, outperformed the vanilla variants in our benchmarks. Notably, BioChatter was created using GPT-3.5-turbo-0613, and to a lesser extent, GPT-4-0613 and LLAMA-2-CHAT (13B); as a result, the development of BioChatter did not impact the benchmark performance of models like openhermes-2.5 and the newer GPT models.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d05397-2768-45e6-a419-027cd619d2a3",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014215,
     "end_time": "2024-03-20T11:42:57.455670",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.441455",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e1ac1feb-f6d3-4e48-8afa-d02ec7149e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.484300Z",
     "iopub.status.busy": "2024-03-20T11:42:57.484229Z",
     "iopub.status.idle": "2024-03-20T11:42:57.486101Z",
     "shell.execute_reply": "2024-03-20T11:42:57.485866Z"
    },
    "papermill": {
     "duration": 0.016784,
     "end_time": "2024-03-20T11:42:57.486548",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.469764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We facilitate access to LLMs by enabling the use of both proprietary and open-source models, and we provide a flexible deployment framework for the latter. Proprietary models are currently the most economical solution for accessing state-of-the-art models and, as such, they are suitable for users just starting out or lacking the resources to deploy their own models. In contrast, open-source models are quickly catching up in terms of performance [@biollmbench] and are essential for the sustainability of the field [@doi:10.1038/d41586-024-00029-4]. We allow self-hosting of open-source models on any scale, from dedicated hardware with GPUs, to local deployment on end-user laptops, to browser-based deployment using web technology.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[5])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8da7eba-dcac-4036-9be4-6b51fed928ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.515226Z",
     "iopub.status.busy": "2024-03-20T11:42:57.515167Z",
     "iopub.status.idle": "2024-03-20T11:42:57.516663Z",
     "shell.execute_reply": "2024-03-20T11:42:57.516454Z"
    },
    "papermill": {
     "duration": 0.016283,
     "end_time": "2024-03-20T11:42:57.517088",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.500805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We make it easier to use LLMs by allowing access to both proprietary and open-source models. Our framework offers a flexible deployment option for open-source models. Proprietary models are currently the most cost-effective choice for accessing the latest models and are ideal for beginners or those with limited resources. On the other hand, open-source models are rapidly improving in performance and are crucial for the long-term viability of the field (Biollmbench). Our platform enables users to host open-source models themselves, whether on dedicated hardware with GPUs, locally on their laptops, or through browser-based deployment using web technology.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[5])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "66fcccb7-97ff-4ccc-80ae-6d841c9a4333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.545822Z",
     "iopub.status.busy": "2024-03-20T11:42:57.545747Z",
     "iopub.status.idle": "2024-03-20T11:42:57.547575Z",
     "shell.execute_reply": "2024-03-20T11:42:57.547217Z"
    },
    "papermill": {
     "duration": 0.016642,
     "end_time": "2024-03-20T11:42:57.547923",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.531281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1487d4c6-de99-4cae-857b-430b29f83e1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.576623Z",
     "iopub.status.busy": "2024-03-20T11:42:57.576551Z",
     "iopub.status.idle": "2024-03-20T11:42:57.578342Z",
     "shell.execute_reply": "2024-03-20T11:42:57.578209Z"
    },
    "papermill": {
     "duration": 0.016676,
     "end_time": "2024-03-20T11:42:57.578781",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.562105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'We facilitate access to LLMs by enabling the use of both proprietary and open-source models, and we provide a flexible deployment framework for the latter. Proprietary models are currently the most economical solution for accessing state-of-the-art models and, as such, they are suitable for users just starting out or lacking the resources to deploy their own models. In contrast, open-source models are quickly catching up in terms of performance [@biollmbench] and are essential for the sustainability of the field [@doi:10.1038/d41586-024-00029-4]. We allow self-hosting of open-source models on any scale, from dedicated hardware with GPUs, to local deployment on end-user laptops, to browser-based deployment using web technology.',\n",
       " 'We make it easier to use LLMs by allowing access to both proprietary and open-source models. Our framework offers a flexible deployment option for open-source models. Proprietary models are currently the most cost-effective choice for accessing the latest models and are ideal for beginners or those with limited resources. On the other hand, open-source models are rapidly improving in performance and are crucial for the long-term viability of the field (Biollmbench). Our platform enables users to host open-source models themselves, whether on dedicated hardware with GPUs, locally on their laptops, or through browser-based deployment using web technology.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490bb55-8004-4e13-963e-1baa8489f6ab",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014145,
     "end_time": "2024-03-20T11:42:57.607183",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.593038",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "86ade612-a79b-4270-99d5-b403b0272c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.635962Z",
     "iopub.status.busy": "2024-03-20T11:42:57.635827Z",
     "iopub.status.idle": "2024-03-20T11:42:57.637221Z",
     "shell.execute_reply": "2024-03-20T11:42:57.637068Z"
    },
    "papermill": {
     "duration": 0.016346,
     "end_time": "2024-03-20T11:42:57.637684",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.621338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current generation of LLMs is not yet ready for unsupervised use in biomedical research with its vast array of unique subfields. Effectively supporting this diversity through robust and contextually aware LLM interactions is a daunting task. While we have taken steps to mitigate the risks of using LLMs such as independent benchmarks, fact-checking, and RAG processes, we cannot guarantee that the models will not produce harmful outputs. We see current LLMs, particularly in the scope of the BioCypher ecosystem, as helpful tools to assist human researchers, alleviating menial and repetitive tasks and helping with technical aspects such as query languages. They are not meant to replace human ingenuity and expertise but to augment it with their complementary strengths. Despite the user-friendly design of BioChatter, there may be a learning curve for researchers unfamiliar with LLMs or the specific functionalities of the framework. For maximising its benefit to the community, encouraging adoption and providing adequate training and support will be critical.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[7])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0affd988-bc27-498c-b4e6-d8073791510a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.666643Z",
     "iopub.status.busy": "2024-03-20T11:42:57.666513Z",
     "iopub.status.idle": "2024-03-20T11:42:57.667932Z",
     "shell.execute_reply": "2024-03-20T11:42:57.667783Z"
    },
    "papermill": {
     "duration": 0.016416,
     "end_time": "2024-03-20T11:42:57.668392",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.651976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current generation of Large Language Models (LLMs) is not yet suitable for unsupervised use in biomedical research due to the vast array of unique subfields within the field. Effectively supporting this diversity through robust and contextually aware interactions with LLMs is a challenging task. While efforts have been made to reduce risks associated with using LLMs, such as independent benchmarks, fact-checking, and Retrieval-Augmented Generation (RAG) processes, there is no guarantee that the models will not produce harmful outputs. Current LLMs, particularly within the BioCypher ecosystem, are viewed as tools to support human researchers by assisting with menial and repetitive tasks, as well as technical aspects like query languages. They are not intended to replace human ingenuity and expertise, but rather to enhance it with their complementary strengths. Despite the user-friendly design of BioChatter, researchers unfamiliar with LLMs or the specific functionalities of the framework may experience a learning curve. To maximize the benefits of BioChatter to the community, it will be crucial to encourage adoption and provide adequate training and support.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[7])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "66a3ff5c-d563-4bbf-b900-e77fc94cd6bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.697367Z",
     "iopub.status.busy": "2024-03-20T11:42:57.697242Z",
     "iopub.status.idle": "2024-03-20T11:42:57.698612Z",
     "shell.execute_reply": "2024-03-20T11:42:57.698449Z"
    },
    "papermill": {
     "duration": 0.016402,
     "end_time": "2024-03-20T11:42:57.699051",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.682649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be401e36-b2d9-4533-be76-cb1ba54f6ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.727807Z",
     "iopub.status.busy": "2024-03-20T11:42:57.727684Z",
     "iopub.status.idle": "2024-03-20T11:42:57.729169Z",
     "shell.execute_reply": "2024-03-20T11:42:57.729042Z"
    },
    "papermill": {
     "duration": 0.016285,
     "end_time": "2024-03-20T11:42:57.729588",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.713303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'The current generation of LLMs is not yet ready for unsupervised use in biomedical research with its vast array of unique subfields. Effectively supporting this diversity through robust and contextually aware LLM interactions is a daunting task. While we have taken steps to mitigate the risks of using LLMs such as independent benchmarks, fact-checking, and RAG processes, we cannot guarantee that the models will not produce harmful outputs. We see current LLMs, particularly in the scope of the BioCypher ecosystem, as helpful tools to assist human researchers, alleviating menial and repetitive tasks and helping with technical aspects such as query languages. They are not meant to replace human ingenuity and expertise but to augment it with their complementary strengths. Despite the user-friendly design of BioChatter, there may be a learning curve for researchers unfamiliar with LLMs or the specific functionalities of the framework. For maximising its benefit to the community, encouraging adoption and providing adequate training and support will be critical.',\n",
       " 'The current generation of Large Language Models (LLMs) is not yet suitable for unsupervised use in biomedical research due to the vast array of unique subfields within the field. Effectively supporting this diversity through robust and contextually aware interactions with LLMs is a challenging task. While efforts have been made to reduce risks associated with using LLMs, such as independent benchmarks, fact-checking, and Retrieval-Augmented Generation (RAG) processes, there is no guarantee that the models will not produce harmful outputs. Current LLMs, particularly within the BioCypher ecosystem, are viewed as tools to support human researchers by assisting with menial and repetitive tasks, as well as technical aspects like query languages. They are not intended to replace human ingenuity and expertise, but rather to enhance it with their complementary strengths. Despite the user-friendly design of BioChatter, researchers unfamiliar with LLMs or the specific functionalities of the framework may experience a learning curve. To maximize the benefits of BioChatter to the community, it will be crucial to encourage adoption and provide adequate training and support.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080117b-9e27-47ce-9b2f-b01c70e365c7",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014341,
     "end_time": "2024-03-20T11:42:57.758295",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.743954",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4ca66dfa-862c-4ed3-a7ce-ad587271bf8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.787392Z",
     "iopub.status.busy": "2024-03-20T11:42:57.787283Z",
     "iopub.status.idle": "2024-03-20T11:42:57.788916Z",
     "shell.execute_reply": "2024-03-20T11:42:57.788674Z"
    },
    "papermill": {
     "duration": 0.016751,
     "end_time": "2024-03-20T11:42:57.789407",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.772656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multitask learners that can synthesise, for instance, language, vision, and molecular measurements are an emerging field of research [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478]. Autonomous agents for trivial tasks have already been developed on the basis of LLMs, and we expect this field to mature in the future [@doi:10.48550/arXiv.2308.11432]. As research on multimodal learning and agent behaviour progresses, we plan to integrate these developments into the BioChatter framework. All framework developments will be performed in light of the ethical implications of LLMs, and we will continue to support the use of open-source models to increase transparency and data privacy. While we focus on the biomedical field, the concept of our frameworks can easily be extended to other scientific domains by adjusting domain-specific prompts and data inputs, which are accessible in a composable and user-friendly manner in our frameworks [@biocypher]. Our Python library is developed openly on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) and can be integrated into any downstream user interface solution. We develop under the permissive MIT licence and encourage contributions and suggestions from the community with regard to the addition of bioinformatics tool integrations, prompt engineering, benchmarking, and any other feature.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[9])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "799fb0c0-15ed-4b7e-8a16-0dd33bdf81b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.818733Z",
     "iopub.status.busy": "2024-03-20T11:42:57.818307Z",
     "iopub.status.idle": "2024-03-20T11:42:57.820365Z",
     "shell.execute_reply": "2024-03-20T11:42:57.820168Z"
    },
    "papermill": {
     "duration": 0.017237,
     "end_time": "2024-03-20T11:42:57.820847",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.803610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emerging research shows that multitask learners, capable of synthesizing language, vision, and molecular measurements, are gaining traction [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478]. Some autonomous agents for basic tasks have already been created using Large Language Models (LLMs), and we anticipate further advancements in this area [@doi:10.48550/arXiv.2308.11432]. As studies on multimodal learning and agent behavior advance, we aim to incorporate these findings into the BioChatter framework. Our framework development prioritizes ethical considerations related to LLMs, emphasizing the use of open-source models to enhance transparency and data privacy. Although our primary focus is on biomedicine, the adaptability of our frameworks allows for easy extension to other scientific fields through adjustments to domain-specific prompts and data inputs, which are conveniently accessible in our frameworks [@biocypher]. Our Python library is openly developed on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) and can be seamlessly integrated into various user interface solutions. Operating under the permissive MIT license, we welcome community contributions and suggestions for integrating bioinformatics tools, prompt engineering, benchmarking, and other features.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[9])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef6c17fb-7cd6-4690-8bce-3bc17befc326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.862022Z",
     "iopub.status.busy": "2024-03-20T11:42:57.861890Z",
     "iopub.status.idle": "2024-03-20T11:42:57.863330Z",
     "shell.execute_reply": "2024-03-20T11:42:57.863182Z"
    },
    "papermill": {
     "duration": 0.028465,
     "end_time": "2024-03-20T11:42:57.863733",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.835268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "480aea1d-08f8-4520-b71b-735a9d57dd1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.893115Z",
     "iopub.status.busy": "2024-03-20T11:42:57.892952Z",
     "iopub.status.idle": "2024-03-20T11:42:57.894549Z",
     "shell.execute_reply": "2024-03-20T11:42:57.894407Z"
    },
    "papermill": {
     "duration": 0.016801,
     "end_time": "2024-03-20T11:42:57.895091",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.878290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('discussion',\n",
       " 'Multitask learners that can synthesise, for instance, language, vision, and molecular measurements are an emerging field of research [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478]. Autonomous agents for trivial tasks have already been developed on the basis of LLMs, and we expect this field to mature in the future [@doi:10.48550/arXiv.2308.11432]. As research on multimodal learning and agent behaviour progresses, we plan to integrate these developments into the BioChatter framework. All framework developments will be performed in light of the ethical implications of LLMs, and we will continue to support the use of open-source models to increase transparency and data privacy. While we focus on the biomedical field, the concept of our frameworks can easily be extended to other scientific domains by adjusting domain-specific prompts and data inputs, which are accessible in a composable and user-friendly manner in our frameworks [@biocypher]. Our Python library is developed openly on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) and can be integrated into any downstream user interface solution. We develop under the permissive MIT licence and encourage contributions and suggestions from the community with regard to the addition of bioinformatics tool integrations, prompt engineering, benchmarking, and any other feature.',\n",
       " 'Emerging research shows that multitask learners, capable of synthesizing language, vision, and molecular measurements, are gaining traction [@doi:10.48550/arXiv.2306.04529;@doi:10.48550/arXiv.2211.01786;@doi:10.48550/arXiv.2310.09478]. Some autonomous agents for basic tasks have already been created using Large Language Models (LLMs), and we anticipate further advancements in this area [@doi:10.48550/arXiv.2308.11432]. As studies on multimodal learning and agent behavior advance, we aim to incorporate these findings into the BioChatter framework. Our framework development prioritizes ethical considerations related to LLMs, emphasizing the use of open-source models to enhance transparency and data privacy. Although our primary focus is on biomedicine, the adaptability of our frameworks allows for easy extension to other scientific fields through adjustments to domain-specific prompts and data inputs, which are conveniently accessible in our frameworks [@biocypher]. Our Python library is openly developed on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)) and can be seamlessly integrated into various user interface solutions. Operating under the permissive MIT license, we welcome community contributions and suggestions for integrating bioinformatics tools, prompt engineering, benchmarking, and other features.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e442fb-dfda-434d-b1d4-265fbe96ccfc",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014396,
     "end_time": "2024-03-20T11:42:57.923991",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.909595",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "42676dcc-e716-44d9-a957-db23dd90e123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.953223Z",
     "iopub.status.busy": "2024-03-20T11:42:57.952864Z",
     "iopub.status.idle": "2024-03-20T11:42:57.954393Z",
     "shell.execute_reply": "2024-03-20T11:42:57.954231Z"
    },
    "papermill": {
     "duration": 0.016682,
     "end_time": "2024-03-20T11:42:57.954943",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.938261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_name = \"methods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f280817-fdf2-447f-b74f-a496c7034555",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:57.984064Z",
     "iopub.status.busy": "2024-03-20T11:42:57.983956Z",
     "iopub.status.idle": "2024-03-20T11:42:57.985320Z",
     "shell.execute_reply": "2024-03-20T11:42:57.985181Z"
    },
    "papermill": {
     "duration": 0.016546,
     "end_time": "2024-03-20T11:42:57.985848",
     "exception": false,
     "start_time": "2024-03-20T11:42:57.969302",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content/40.methods.md\n"
     ]
    }
   ],
   "source": [
    "pr_filename = pr_files[4].filename\n",
    "assert section_name in pr_filename\n",
    "print(pr_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4466db8-9485-49cc-a08f-dfdf243e1f06",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014563,
     "end_time": "2024-03-20T11:42:58.014822",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.000259",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d28b84ac-727b-4412-8504-2cb1d9764a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.044071Z",
     "iopub.status.busy": "2024-03-20T11:42:58.043960Z",
     "iopub.status.idle": "2024-03-20T11:42:58.165757Z",
     "shell.execute_reply": "2024-03-20T11:42:58.164309Z"
    },
    "papermill": {
     "duration": 0.13878,
     "end_time": "2024-03-20T11:42:58.168020",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.029240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## (Supplementary / Online) Methods {.page_break_b\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "orig_section_content = repo.get_contents(pr_filename, pr_prev).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(orig_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "18fe5c96-6270-45fe-8f3a-2da927318d82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.212263Z",
     "iopub.status.busy": "2024-03-20T11:42:58.211970Z",
     "iopub.status.idle": "2024-03-20T11:42:58.214257Z",
     "shell.execute_reply": "2024-03-20T11:42:58.213957Z"
    },
    "papermill": {
     "duration": 0.01809,
     "end_time": "2024-03-20T11:42:58.214833",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.196743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "orig_section_paragraphs = orig_section_content.split(\"\\n\\n\")\n",
    "display(len(orig_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066a4d2-2696-4e7d-9da4-d1818871259d",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014426,
     "end_time": "2024-03-20T11:42:58.243866",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.229440",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4e8d95f9-376b-450b-99d5-67891963f997",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.273007Z",
     "iopub.status.busy": "2024-03-20T11:42:58.272874Z",
     "iopub.status.idle": "2024-03-20T11:42:58.528406Z",
     "shell.execute_reply": "2024-03-20T11:42:58.526910Z"
    },
    "papermill": {
     "duration": 0.272735,
     "end_time": "2024-03-20T11:42:58.530886",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.258151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## (Supplementary / Online) Methods {.page_break_b\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "mod_section_content = repo.get_contents(pr_filename, pr_curr).decoded_content.decode(\n",
    "    \"utf-8\"\n",
    ")\n",
    "print(mod_section_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6c0d91e9-b334-4b74-9ea5-7080c5e7fa6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.579948Z",
     "iopub.status.busy": "2024-03-20T11:42:58.579873Z",
     "iopub.status.idle": "2024-03-20T11:42:58.582115Z",
     "shell.execute_reply": "2024-03-20T11:42:58.581976Z"
    },
    "papermill": {
     "duration": 0.017748,
     "end_time": "2024-03-20T11:42:58.582657",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.564909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split by paragraph\n",
    "mod_section_paragraphs = mod_section_content.split(\"\\n\\n\")\n",
    "display(len(mod_section_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da0d8d-adc4-4403-be49-4385ea781ac8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01455,
     "end_time": "2024-03-20T11:42:58.611758",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.597208",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "53720cd3-8e0a-4d31-8883-c206026f9e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.641173Z",
     "iopub.status.busy": "2024-03-20T11:42:58.641099Z",
     "iopub.status.idle": "2024-03-20T11:42:58.643101Z",
     "shell.execute_reply": "2024-03-20T11:42:58.642884Z"
    },
    "papermill": {
     "duration": 0.017415,
     "end_time": "2024-03-20T11:42:58.643657",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.626242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## (Supplementary / Online) Methods {.page_break_before}'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2638f377-a44a-4d05-8fca-bfa9383ca736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.673273Z",
     "iopub.status.busy": "2024-03-20T11:42:58.673199Z",
     "iopub.status.idle": "2024-03-20T11:42:58.674951Z",
     "shell.execute_reply": "2024-03-20T11:42:58.674818Z"
    },
    "papermill": {
     "duration": 0.017126,
     "end_time": "2024-03-20T11:42:58.675339",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.658213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## (Supplementary / Online) Methods {.page_break_before}'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_section_paragraphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639829e1-1463-41b5-b91d-b2bdd319e04e",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01472,
     "end_time": "2024-03-20T11:42:58.704547",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.689827",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bde47e33-80ca-4c52-a420-868e3721fdf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.734204Z",
     "iopub.status.busy": "2024-03-20T11:42:58.734096Z",
     "iopub.status.idle": "2024-03-20T11:42:58.735903Z",
     "shell.execute_reply": "2024-03-20T11:42:58.735673Z"
    },
    "papermill": {
     "duration": 0.017302,
     "end_time": "2024-03-20T11:42:58.736398",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.719096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter (version 0.4.7 at the time of publication) is a Python library, supporting Python 3.10-3.12, which we ensure with a continuous integration pipeline on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)). We provide documentation at [https://biochatter.org](https://biochatter.org), including a tutorial and API reference. All packages are developed openly and according to modern standards of software development [@doi:10.1038/s41597-020-0486-7]; we use the permissive MIT licence to encourage downstream use and development. We include a code of conduct and contributor guidelines to offer accessibility and inclusivity to all who are interested in contributing to the framework.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[1])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a613037a-a64e-43cf-abda-cf0a4a736502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.766321Z",
     "iopub.status.busy": "2024-03-20T11:42:58.765950Z",
     "iopub.status.idle": "2024-03-20T11:42:58.767836Z",
     "shell.execute_reply": "2024-03-20T11:42:58.767600Z"
    },
    "papermill": {
     "duration": 0.017341,
     "end_time": "2024-03-20T11:42:58.768350",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.751009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter (version 0.4.7 at the time of publication) is a Python library that supports Python 3.10-3.12. The compatibility with these versions is ensured through a continuous integration pipeline on GitHub (https://github.com/biocypher/biochatter). Documentation for BioChatter can be found at https://biochatter.org, which includes a tutorial and API reference. The development of all packages is done openly and follows modern software development standards [@doi:10.1038/s41597-020-0486-7]. To encourage downstream use and development, we have chosen the permissive MIT license. Additionally, we have included a code of conduct and contributor guidelines to promote accessibility and inclusivity for all individuals interested in contributing to the framework. \n",
      "$$ \n",
      "\\text{Equation (@id): Definition of important symbol} \n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "par1 = (\n",
    "    process_paragraph(mod_section_paragraphs[1:3])\n",
    "    .replace(\"$$\", \"\\n$$\")\n",
    "    .replace(\"\\\\text\", \"\\n\\\\text\")\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fd5ca3b4-323c-4abf-8cb9-9a9773f4aed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.797938Z",
     "iopub.status.busy": "2024-03-20T11:42:58.797846Z",
     "iopub.status.idle": "2024-03-20T11:42:58.799291Z",
     "shell.execute_reply": "2024-03-20T11:42:58.799018Z"
    },
    "papermill": {
     "duration": 0.016726,
     "end_time": "2024-03-20T11:42:58.799729",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.783003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1fb17a25-e891-4ae1-a04f-a58e8eb33558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.829086Z",
     "iopub.status.busy": "2024-03-20T11:42:58.829009Z",
     "iopub.status.idle": "2024-03-20T11:42:58.830872Z",
     "shell.execute_reply": "2024-03-20T11:42:58.830733Z"
    },
    "papermill": {
     "duration": 0.017056,
     "end_time": "2024-03-20T11:42:58.831298",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.814242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'BioChatter (version 0.4.7 at the time of publication) is a Python library, supporting Python 3.10-3.12, which we ensure with a continuous integration pipeline on GitHub ([https://github.com/biocypher/biochatter](https://github.com/biocypher/biochatter)). We provide documentation at [https://biochatter.org](https://biochatter.org), including a tutorial and API reference. All packages are developed openly and according to modern standards of software development [@doi:10.1038/s41597-020-0486-7]; we use the permissive MIT licence to encourage downstream use and development. We include a code of conduct and contributor guidelines to offer accessibility and inclusivity to all who are interested in contributing to the framework.',\n",
       " 'BioChatter (version 0.4.7 at the time of publication) is a Python library that supports Python 3.10-3.12. The compatibility with these versions is ensured through a continuous integration pipeline on GitHub (https://github.com/biocypher/biochatter). Documentation for BioChatter can be found at https://biochatter.org, which includes a tutorial and API reference. The development of all packages is done openly and follows modern software development standards [@doi:10.1038/s41597-020-0486-7]. To encourage downstream use and development, we have chosen the permissive MIT license. Additionally, we have included a code of conduct and contributor guidelines to promote accessibility and inclusivity for all individuals interested in contributing to the framework. \\n$$ \\n\\\\text{Equation (@id): Definition of important symbol} \\n$$')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a2445-2b50-4564-8805-435db74bd47a",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014585,
     "end_time": "2024-03-20T11:42:58.860488",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.845903",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c96448ac-3d15-40aa-8b1d-9d348cb46b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.890253Z",
     "iopub.status.busy": "2024-03-20T11:42:58.890172Z",
     "iopub.status.idle": "2024-03-20T11:42:58.891776Z",
     "shell.execute_reply": "2024-03-20T11:42:58.891610Z"
    },
    "papermill": {
     "duration": 0.017097,
     "end_time": "2024-03-20T11:42:58.892212",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.875115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter Light is a web app based on the Streamlit framework (version 1.31.1, [https://streamlit.io](https://streamlit.io)), which is written in Python and can be deployed locally or on a server ([https://github.com/biocypher/biochatter-light](https://github.com/biocypher/biochatter-light)). The ease with which Streamlit allows the creation of interactive web apps in pure Python enables rapid iteration and agile development of new features, with the tradeoff of limited customisation and scalability. This framework is suitable for rapid prototyping of bespoke solutions for specific use cases. For an up-to-date overview and preview of the current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[4])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06769dc3-7e49-4f72-8b19-9e71798ddf90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.934354Z",
     "iopub.status.busy": "2024-03-20T11:42:58.934218Z",
     "iopub.status.idle": "2024-03-20T11:42:58.936114Z",
     "shell.execute_reply": "2024-03-20T11:42:58.935943Z"
    },
    "papermill": {
     "duration": 0.029609,
     "end_time": "2024-03-20T11:42:58.936575",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.906966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter Light is a web app based on the Streamlit framework (version 1.31.1, [https://streamlit.io](https://streamlit.io)), which is written in Python and can be deployed locally or on a server ([https://github.com/biocypher/biochatter-light](https://github.com/biocypher/biochatter-light)). The ease with which Streamlit allows the creation of interactive web apps in pure Python enables rapid iteration and agile development of new features, with the tradeoff of limited customization and scalability. This framework is suitable for rapid prototyping of bespoke solutions for specific use cases. For an up-to-date overview and preview of the current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    mod_section_paragraphs[5]\n",
    ")  # .replace(\"$$\", \"\\n$$\").replace(\"\\\\text\", \"\\n\\\\text\")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "33cc1054-8364-43af-8406-29f7484cb270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.966482Z",
     "iopub.status.busy": "2024-03-20T11:42:58.966374Z",
     "iopub.status.idle": "2024-03-20T11:42:58.967939Z",
     "shell.execute_reply": "2024-03-20T11:42:58.967773Z"
    },
    "papermill": {
     "duration": 0.017036,
     "end_time": "2024-03-20T11:42:58.968387",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.951351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d6057c60-1a37-4785-a2c4-dbd32d55eac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:58.998423Z",
     "iopub.status.busy": "2024-03-20T11:42:58.998320Z",
     "iopub.status.idle": "2024-03-20T11:42:59.000089Z",
     "shell.execute_reply": "2024-03-20T11:42:58.999926Z"
    },
    "papermill": {
     "duration": 0.017331,
     "end_time": "2024-03-20T11:42:59.000538",
     "exception": false,
     "start_time": "2024-03-20T11:42:58.983207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'BioChatter Light is a web app based on the Streamlit framework (version 1.31.1, [https://streamlit.io](https://streamlit.io)), which is written in Python and can be deployed locally or on a server ([https://github.com/biocypher/biochatter-light](https://github.com/biocypher/biochatter-light)). The ease with which Streamlit allows the creation of interactive web apps in pure Python enables rapid iteration and agile development of new features, with the tradeoff of limited customisation and scalability. This framework is suitable for rapid prototyping of bespoke solutions for specific use cases. For an up-to-date overview and preview of the current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).',\n",
       " 'BioChatter Light is a web app based on the Streamlit framework (version 1.31.1, [https://streamlit.io](https://streamlit.io)), which is written in Python and can be deployed locally or on a server ([https://github.com/biocypher/biochatter-light](https://github.com/biocypher/biochatter-light)). The ease with which Streamlit allows the creation of interactive web apps in pure Python enables rapid iteration and agile development of new features, with the tradeoff of limited customization and scalability. This framework is suitable for rapid prototyping of bespoke solutions for specific use cases. For an up-to-date overview and preview of the current functionality of the platform, please visit the [online preview](https://chat.biocypher.org).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9818db3-fe11-4d06-82b7-e7cd5a83b5b9",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014682,
     "end_time": "2024-03-20T11:42:59.029957",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.015275",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bfe91365-6b1a-49f1-aad9-a2afb9cea749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.059722Z",
     "iopub.status.busy": "2024-03-20T11:42:59.059630Z",
     "iopub.status.idle": "2024-03-20T11:42:59.061228Z",
     "shell.execute_reply": "2024-03-20T11:42:59.060963Z"
    },
    "papermill": {
     "duration": 0.017225,
     "end_time": "2024-03-20T11:42:59.061690",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.044465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) is a modern web app with server-client architecture, based on the open-source template of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)). It is written combining Typescript and Python and uses Next.js (v13.4.9) for a sleek frontend and Flask (v3.0.0) as backend. It demonstrates the use of BioChatter in a modern web app, including full customisation and scalability and localisation in 18 languages. However, this comes at the cost of increased complexity and development time. To provide seamless integration of the BioChatter backend into existing frontend solutions, we provide the server implementation at [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server) and as a Docker image in our Docker Hub organisation ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[5])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dcaae92f-3c45-49dc-ba24-2dc98c3c8e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.091511Z",
     "iopub.status.busy": "2024-03-20T11:42:59.091445Z",
     "iopub.status.idle": "2024-03-20T11:42:59.092870Z",
     "shell.execute_reply": "2024-03-20T11:42:59.092720Z"
    },
    "papermill": {
     "duration": 0.01695,
     "end_time": "2024-03-20T11:42:59.093303",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.076353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) is a contemporary web application with a server-client architecture, built upon the open-source framework of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)). It is developed using a combination of Typescript and Python, utilizing Next.js (v13.4.9) for the frontend and Flask (v3.0.0) for the backend. The platform showcases the application of BioChatter within a modern web environment, offering extensive customization, scalability, and support for localization in 18 languages. Nevertheless, this enhanced functionality is accompanied by increased complexity and longer development cycles. To facilitate the seamless integration of the BioChatter backend with existing frontend solutions, we offer the server implementation at [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server), as well as a Docker image within our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    mod_section_paragraphs[6]\n",
    ")  # .replace(\"$$\", \"\\n$$\").replace(\"\\\\text\", \"\\n\\\\text\")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f43ab221-c1c5-45e3-8a44-af21ca74a69c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.123638Z",
     "iopub.status.busy": "2024-03-20T11:42:59.123507Z",
     "iopub.status.idle": "2024-03-20T11:42:59.125064Z",
     "shell.execute_reply": "2024-03-20T11:42:59.124795Z"
    },
    "papermill": {
     "duration": 0.017531,
     "end_time": "2024-03-20T11:42:59.125581",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.108050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cb5e98ce-ae69-47da-a99c-ced5b15c0320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.156157Z",
     "iopub.status.busy": "2024-03-20T11:42:59.156050Z",
     "iopub.status.idle": "2024-03-20T11:42:59.157944Z",
     "shell.execute_reply": "2024-03-20T11:42:59.157789Z"
    },
    "papermill": {
     "duration": 0.017706,
     "end_time": "2024-03-20T11:42:59.158456",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.140750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) is a modern web app with server-client architecture, based on the open-source template of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)). It is written combining Typescript and Python and uses Next.js (v13.4.9) for a sleek frontend and Flask (v3.0.0) as backend. It demonstrates the use of BioChatter in a modern web app, including full customisation and scalability and localisation in 18 languages. However, this comes at the cost of increased complexity and development time. To provide seamless integration of the BioChatter backend into existing frontend solutions, we provide the server implementation at [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server) and as a Docker image in our Docker Hub organisation ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).',\n",
       " 'BioChatter Next ([https://github.com/biocypher/biochatter-next](https://github.com/biocypher/biochatter-next)) is a contemporary web application with a server-client architecture, built upon the open-source framework of ChatGPT-Next-Web ([https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web](https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web)). It is developed using a combination of Typescript and Python, utilizing Next.js (v13.4.9) for the frontend and Flask (v3.0.0) for the backend. The platform showcases the application of BioChatter within a modern web environment, offering extensive customization, scalability, and support for localization in 18 languages. Nevertheless, this enhanced functionality is accompanied by increased complexity and longer development cycles. To facilitate the seamless integration of the BioChatter backend with existing frontend solutions, we offer the server implementation at [https://github.com/biocypher/biochatter-server](https://github.com/biocypher/biochatter-server), as well as a Docker image within our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/biochatter-server](https://hub.docker.com/repository/docker/biocypher/biochatter-server)).')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b3948-644c-4f26-91b5-5ffca2a3ea11",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.0147,
     "end_time": "2024-03-20T11:42:59.187846",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.173146",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a652e5d5-a65c-4412-b751-34ce3eacc102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.218094Z",
     "iopub.status.busy": "2024-03-20T11:42:59.217939Z",
     "iopub.status.idle": "2024-03-20T11:42:59.219405Z",
     "shell.execute_reply": "2024-03-20T11:42:59.219228Z"
    },
    "papermill": {
     "duration": 0.017061,
     "end_time": "2024-03-20T11:42:59.219805",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.202744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benchmarking framework examines a matrix of component combinations using the parameterisation feature of Pytest [@pytest]. This implementation allows for the automated evaluation of all possible combinations of components, such as LLMs, prompts, and datasets. We performed the benchmarks on a MacBook Pro with an M3 Max chip with 40-core GPU and 128GB of RAM. As a default, we ran each test five times to account for the stochastic nature of LLMs. We generally set the temperature to the lowest value possible for each model to decrease fluctuation.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[8])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a7c95523-893b-442c-85ea-674524be26c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.249872Z",
     "iopub.status.busy": "2024-03-20T11:42:59.249749Z",
     "iopub.status.idle": "2024-03-20T11:42:59.251573Z",
     "shell.execute_reply": "2024-03-20T11:42:59.251333Z"
    },
    "papermill": {
     "duration": 0.017277,
     "end_time": "2024-03-20T11:42:59.251965",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.234688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The benchmarking framework examines a matrix of component combinations using the parameterization feature of Pytest [@pytest]. This implementation allows for the automated evaluation of all possible combinations of components, such as Large Language Models (LLMs), prompts, and datasets. We performed the benchmarks on a MacBook Pro with an M3 Max chip with a 40-core GPU and 128GB of RAM. As a default, we ran each test five times to account for the stochastic nature of LLMs. We generally set the temperature to the lowest value possible for each model to decrease fluctuation. \n",
      "$$ \\text{Symbols:}\\\\ \\text{LLMs} - \\text{Large Language Models} \n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[9:11]).replace(\n",
    "    \"$$\", \"\\n$$\"\n",
    ")  # .replace(\"\\\\text\\{LLM\", \"\\n\\\\text\\{LLM\")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "e06c1ab6-bf77-40ee-af24-0fbd08388bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.282266Z",
     "iopub.status.busy": "2024-03-20T11:42:59.281872Z",
     "iopub.status.idle": "2024-03-20T11:42:59.283495Z",
     "shell.execute_reply": "2024-03-20T11:42:59.283347Z"
    },
    "papermill": {
     "duration": 0.017319,
     "end_time": "2024-03-20T11:42:59.283991",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.266672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7b9498f-caeb-48a4-bc85-f123bfa6c3cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.314177Z",
     "iopub.status.busy": "2024-03-20T11:42:59.313871Z",
     "iopub.status.idle": "2024-03-20T11:42:59.315722Z",
     "shell.execute_reply": "2024-03-20T11:42:59.315573Z"
    },
    "papermill": {
     "duration": 0.017516,
     "end_time": "2024-03-20T11:42:59.316248",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.298732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'The benchmarking framework examines a matrix of component combinations using the parameterisation feature of Pytest [@pytest]. This implementation allows for the automated evaluation of all possible combinations of components, such as LLMs, prompts, and datasets. We performed the benchmarks on a MacBook Pro with an M3 Max chip with 40-core GPU and 128GB of RAM. As a default, we ran each test five times to account for the stochastic nature of LLMs. We generally set the temperature to the lowest value possible for each model to decrease fluctuation.',\n",
       " 'The benchmarking framework examines a matrix of component combinations using the parameterization feature of Pytest [@pytest]. This implementation allows for the automated evaluation of all possible combinations of components, such as Large Language Models (LLMs), prompts, and datasets. We performed the benchmarks on a MacBook Pro with an M3 Max chip with a 40-core GPU and 128GB of RAM. As a default, we ran each test five times to account for the stochastic nature of LLMs. We generally set the temperature to the lowest value possible for each model to decrease fluctuation. \\n$$ \\\\text{Symbols:}\\\\\\\\ \\\\text{LLMs} - \\\\text{Large Language Models} \\n$$')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f5b06-bff6-48c6-923a-a15d42583f01",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-03-20T11:42:59.345967",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.331150",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "63ac3958-4064-479b-a200-07dc7fa08306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.376010Z",
     "iopub.status.busy": "2024-03-20T11:42:59.375913Z",
     "iopub.status.idle": "2024-03-20T11:42:59.377640Z",
     "shell.execute_reply": "2024-03-20T11:42:59.377389Z"
    },
    "papermill": {
     "duration": 0.017301,
     "end_time": "2024-03-20T11:42:59.378122",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.360821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pytest matrix uses a hash-based system to evaluate whether a model-dataset combination has been run before. Briefly, the hash is calculated from the dictionary representation of the test parameters, and the test is skipped if the combination of hash and model name is already present in the database. This hashing optimises for efficiency by only running modified or newly added tests. The individual dimensions of the matrix are:\n",
      "- **LLMs**: Testing proprietary (OpenAI) and open-source models (commonly using the Xorbits Inference API and HuggingFace models) against the same set of tasks is the primary aim of our benchmarking framework. We facilitate the automation of testing by including a programmatic way of deploying open-source models.\n",
      "- **prompts**: Since model performance can dramatically rely on the used prompts, a set of prompts for each task with varying degrees of specificity and fixed as well as variable components is used to evaluate this variability.\n",
      "- **datasets**: We test various tasks using a set of datasets for each task in question-answer-style.\n",
      "- **data processing**: Some data processing steps can have great impact on the downstream performance of LLMs. For instance, we test the conversion of numbers (which LLMs are notoriously bad at handling) to categorical text (e.g., low, medium, high).\n",
      "- **model quantisations**: We test a set of quantisations for each model (where available) to account for the trade-off between model size, inference speed, and performance.\n",
      "- **model parameters**: Where suitable, we test a set of parameters for each model, such as \"temperature,\" which determines the reproducibility of model responses.\n",
      "- **integrations**: We write dedicated tests for specific tasks that require integrations, for instance with knowledge graphs or vector databases.\n",
      "- **stochasticity**: To account for variability in model responses, we include a parameter to run each test multiple times and generate summary statistics.\n",
      "- **sentiment and behaviour**: To assess whether the models exhibit the desired behaviour patterns for each of the personas, we let a second LLM evaluate the responses based on a set of criteria, including professionalism and politeness.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[9:19]).replace(\" - \", \"\\n- \")\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4d34e503-4908-444e-a214-f38f1f7734ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.408496Z",
     "iopub.status.busy": "2024-03-20T11:42:59.408400Z",
     "iopub.status.idle": "2024-03-20T11:42:59.410180Z",
     "shell.execute_reply": "2024-03-20T11:42:59.409848Z"
    },
    "papermill": {
     "duration": 0.017522,
     "end_time": "2024-03-20T11:42:59.410691",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.393169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pytest matrix utilizes a hash-based system to determine if a model-dataset combination has been previously executed. In essence, the hash is computed from the dictionary representation of the test parameters, and the test is skipped if the hash combined with the model name already exists in the database. This hashing strategy optimizes efficiency by only executing modified or newly added tests. The key dimensions of the matrix include:\n",
      "- **LLMs**: The primary objective of our benchmarking framework is to test both proprietary (OpenAI) and open-source models (commonly leveraging the Xorbits Inference API and HuggingFace models) against the same set of tasks. We streamline the testing process by incorporating a programmatic approach to deploying open-source models.\n",
      "- **prompts**: Given that model performance can significantly depend on the prompts used, we employ a range of prompts for each task with varying levels of specificity and components that are fixed or variable to assess this variability.\n",
      "- **datasets**: Various tasks are tested using specific datasets structured in a question-answer format.\n",
      "- **data processing**: Certain data processing steps can significantly impact the subsequent performance of LLMs. For example, we evaluate the conversion of numbers (which LLMs often struggle with) into categorical text (e.g., low, medium, high).\n",
      "- **model quantizations**: We assess a variety of quantizations for each model (where applicable) to consider the trade-off between model size, inference speed, and performance.\n",
      "- **model parameters**: When appropriate, we examine a range of parameters for each model, such as \"temperature,\" which influences the reproducibility of model outputs.\n",
      "- **integrations**: Dedicated tests are developed for specific tasks that necessitate integrations, such as with knowledge graphs or vector databases.\n",
      "- **stochasticity**: To address the variability in model responses, we incorporate a parameter to run each test multiple times and generate summary statistics.\n",
      "- **sentiment and behavior**: To evaluate whether the models demonstrate the desired behavior patterns for different personas, we employ a second LLM to assess the responses based on criteria like professionalism and politeness.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[11:21]).replace(\" - \", \"\\n- \")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "94daea43-a362-4c43-b008-508e4be5c4ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.440774Z",
     "iopub.status.busy": "2024-03-20T11:42:59.440664Z",
     "iopub.status.idle": "2024-03-20T11:42:59.441959Z",
     "shell.execute_reply": "2024-03-20T11:42:59.441809Z"
    },
    "papermill": {
     "duration": 0.016858,
     "end_time": "2024-03-20T11:42:59.442366",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.425508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "713da4ef-4e0e-469a-805a-363d42130f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.472671Z",
     "iopub.status.busy": "2024-03-20T11:42:59.472579Z",
     "iopub.status.idle": "2024-03-20T11:42:59.474475Z",
     "shell.execute_reply": "2024-03-20T11:42:59.474318Z"
    },
    "papermill": {
     "duration": 0.017743,
     "end_time": "2024-03-20T11:42:59.474966",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.457223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'The Pytest matrix uses a hash-based system to evaluate whether a model-dataset combination has been run before. Briefly, the hash is calculated from the dictionary representation of the test parameters, and the test is skipped if the combination of hash and model name is already present in the database. This hashing optimises for efficiency by only running modified or newly added tests. The individual dimensions of the matrix are:\\n- **LLMs**: Testing proprietary (OpenAI) and open-source models (commonly using the Xorbits Inference API and HuggingFace models) against the same set of tasks is the primary aim of our benchmarking framework. We facilitate the automation of testing by including a programmatic way of deploying open-source models.\\n- **prompts**: Since model performance can dramatically rely on the used prompts, a set of prompts for each task with varying degrees of specificity and fixed as well as variable components is used to evaluate this variability.\\n- **datasets**: We test various tasks using a set of datasets for each task in question-answer-style.\\n- **data processing**: Some data processing steps can have great impact on the downstream performance of LLMs. For instance, we test the conversion of numbers (which LLMs are notoriously bad at handling) to categorical text (e.g., low, medium, high).\\n- **model quantisations**: We test a set of quantisations for each model (where available) to account for the trade-off between model size, inference speed, and performance.\\n- **model parameters**: Where suitable, we test a set of parameters for each model, such as \"temperature,\" which determines the reproducibility of model responses.\\n- **integrations**: We write dedicated tests for specific tasks that require integrations, for instance with knowledge graphs or vector databases.\\n- **stochasticity**: To account for variability in model responses, we include a parameter to run each test multiple times and generate summary statistics.\\n- **sentiment and behaviour**: To assess whether the models exhibit the desired behaviour patterns for each of the personas, we let a second LLM evaluate the responses based on a set of criteria, including professionalism and politeness.',\n",
       " 'The Pytest matrix utilizes a hash-based system to determine if a model-dataset combination has been previously executed. In essence, the hash is computed from the dictionary representation of the test parameters, and the test is skipped if the hash combined with the model name already exists in the database. This hashing strategy optimizes efficiency by only executing modified or newly added tests. The key dimensions of the matrix include:\\n- **LLMs**: The primary objective of our benchmarking framework is to test both proprietary (OpenAI) and open-source models (commonly leveraging the Xorbits Inference API and HuggingFace models) against the same set of tasks. We streamline the testing process by incorporating a programmatic approach to deploying open-source models.\\n- **prompts**: Given that model performance can significantly depend on the prompts used, we employ a range of prompts for each task with varying levels of specificity and components that are fixed or variable to assess this variability.\\n- **datasets**: Various tasks are tested using specific datasets structured in a question-answer format.\\n- **data processing**: Certain data processing steps can significantly impact the subsequent performance of LLMs. For example, we evaluate the conversion of numbers (which LLMs often struggle with) into categorical text (e.g., low, medium, high).\\n- **model quantizations**: We assess a variety of quantizations for each model (where applicable) to consider the trade-off between model size, inference speed, and performance.\\n- **model parameters**: When appropriate, we examine a range of parameters for each model, such as \"temperature,\" which influences the reproducibility of model outputs.\\n- **integrations**: Dedicated tests are developed for specific tasks that necessitate integrations, such as with knowledge graphs or vector databases.\\n- **stochasticity**: To address the variability in model responses, we incorporate a parameter to run each test multiple times and generate summary statistics.\\n- **sentiment and behavior**: To evaluate whether the models demonstrate the desired behavior patterns for different personas, we employ a second LLM to assess the responses based on criteria like professionalism and politeness.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6a6fa-eca7-4dfe-9480-4c731135243e",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014955,
     "end_time": "2024-03-20T11:42:59.504733",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.489778",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "11ae83fb-2b83-4851-b1e1-b41b7e9d7986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.535213Z",
     "iopub.status.busy": "2024-03-20T11:42:59.535116Z",
     "iopub.status.idle": "2024-03-20T11:42:59.536883Z",
     "shell.execute_reply": "2024-03-20T11:42:59.536475Z"
    },
    "papermill": {
     "duration": 0.0176,
     "end_time": "2024-03-20T11:42:59.537393",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.519793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), we implement an encryption routine on the benchmark datasets. The encryption is performed using a hybrid encryption scheme, where the data are encrypted with a symmetric key, which is in turn encrypted with an asymmetric key. The datasets are stored in a dedicated encrypted pipeline that is only accessible to the workflow that executes the benchmark. These processes are implemented at [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset) and accessed from the benchmark procedure in BioChatter.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[20])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dc147a44-585a-4f7e-9d88-eb8fd67046b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.567934Z",
     "iopub.status.busy": "2024-03-20T11:42:59.567801Z",
     "iopub.status.idle": "2024-03-20T11:42:59.569529Z",
     "shell.execute_reply": "2024-03-20T11:42:59.569173Z"
    },
    "papermill": {
     "duration": 0.017683,
     "end_time": "2024-03-20T11:42:59.570051",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.552368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), an encryption routine is implemented on the benchmark datasets. The encryption is carried out using a hybrid encryption scheme, where the data are encrypted with a symmetric key, which is then encrypted with an asymmetric key. The datasets are stored in a dedicated encrypted pipeline that is exclusively accessible to the workflow executing the benchmark. These processes are implemented at [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset) and accessed from the benchmark procedure in BioChatter.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[24])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "23b6d12e-52ec-40fc-923e-b34eadee4100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.612988Z",
     "iopub.status.busy": "2024-03-20T11:42:59.612595Z",
     "iopub.status.idle": "2024-03-20T11:42:59.614760Z",
     "shell.execute_reply": "2024-03-20T11:42:59.614369Z"
    },
    "papermill": {
     "duration": 0.018573,
     "end_time": "2024-03-20T11:42:59.615379",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.596806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "775b6f72-4f82-4c4b-99eb-9aba711c6290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.646164Z",
     "iopub.status.busy": "2024-03-20T11:42:59.645913Z",
     "iopub.status.idle": "2024-03-20T11:42:59.647826Z",
     "shell.execute_reply": "2024-03-20T11:42:59.647679Z"
    },
    "papermill": {
     "duration": 0.017881,
     "end_time": "2024-03-20T11:42:59.648320",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.630439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), we implement an encryption routine on the benchmark datasets. The encryption is performed using a hybrid encryption scheme, where the data are encrypted with a symmetric key, which is in turn encrypted with an asymmetric key. The datasets are stored in a dedicated encrypted pipeline that is only accessible to the workflow that executes the benchmark. These processes are implemented at [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset) and accessed from the benchmark procedure in BioChatter.',\n",
       " 'To prevent leakage of benchmarking data (and subsequent contamination of future LLMs), an encryption routine is implemented on the benchmark datasets. The encryption is carried out using a hybrid encryption scheme, where the data are encrypted with a symmetric key, which is then encrypted with an asymmetric key. The datasets are stored in a dedicated encrypted pipeline that is exclusively accessible to the workflow executing the benchmark. These processes are implemented at [https://github.com/biocypher/llm-test-dataset](https://github.com/biocypher/llm-test-dataset) and accessed from the benchmark procedure in BioChatter.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a4838-5866-4909-b38f-93988b88c5e8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.014967,
     "end_time": "2024-03-20T11:42:59.678401",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.663434",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c14fb275-9f57-4ac2-9812-63851460b897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.709450Z",
     "iopub.status.busy": "2024-03-20T11:42:59.708977Z",
     "iopub.status.idle": "2024-03-20T11:42:59.711348Z",
     "shell.execute_reply": "2024-03-20T11:42:59.711123Z"
    },
    "papermill": {
     "duration": 0.018321,
     "end_time": "2024-03-20T11:42:59.711842",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.693521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We utilise the close connection between BioChatter and the BioCypher framework [@biocypher] to integrate knowledge graph (KG) queries into the BioChatter API. In the BioCypher KG creation, we use a configuration file to map KG contents to ontology terms, including information about each of the entities. For instance, we detail the properties of a node and the source and target classes of an edge. Additionally, during the KG build process, we enrich this information and save it to a YAML file and, optionally, directly to the KG. This information is used by BioChatter to tune its understanding of the KG, which allows the LLM to query the KG more efficiently.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[22])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d72b58d5-6ef6-4199-980c-eec29b3db382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.742690Z",
     "iopub.status.busy": "2024-03-20T11:42:59.742614Z",
     "iopub.status.idle": "2024-03-20T11:42:59.744203Z",
     "shell.execute_reply": "2024-03-20T11:42:59.744036Z"
    },
    "papermill": {
     "duration": 0.017655,
     "end_time": "2024-03-20T11:42:59.744666",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.727011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We utilize the close connection between BioChatter and the BioCypher framework (Johnson et al., 2020) to integrate knowledge graph (KG) queries into the BioChatter API. In the BioCypher KG creation, we use a configuration file to map KG contents to ontology terms, including information about each of the entities. For instance, we detail the properties of a node and the source and target classes of an edge. Additionally, during the KG build process, we enrich this information and save it to a YAML file and, optionally, directly to the KG. This information is used by BioChatter to tune its understanding of the KG, which allows the Large Language Model (LLM) to query the KG more efficiently.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[26])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "de69efcb-2718-4bb1-8f4b-64f980bdf599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.775448Z",
     "iopub.status.busy": "2024-03-20T11:42:59.775361Z",
     "iopub.status.idle": "2024-03-20T11:42:59.776816Z",
     "shell.execute_reply": "2024-03-20T11:42:59.776651Z"
    },
    "papermill": {
     "duration": 0.017501,
     "end_time": "2024-03-20T11:42:59.777345",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.759844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1895b737-b8f3-4cb9-ad02-f2c90c0e3288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.808285Z",
     "iopub.status.busy": "2024-03-20T11:42:59.808166Z",
     "iopub.status.idle": "2024-03-20T11:42:59.810345Z",
     "shell.execute_reply": "2024-03-20T11:42:59.810192Z"
    },
    "papermill": {
     "duration": 0.018311,
     "end_time": "2024-03-20T11:42:59.810876",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.792565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'We utilise the close connection between BioChatter and the BioCypher framework [@biocypher] to integrate knowledge graph (KG) queries into the BioChatter API. In the BioCypher KG creation, we use a configuration file to map KG contents to ontology terms, including information about each of the entities. For instance, we detail the properties of a node and the source and target classes of an edge. Additionally, during the KG build process, we enrich this information and save it to a YAML file and, optionally, directly to the KG. This information is used by BioChatter to tune its understanding of the KG, which allows the LLM to query the KG more efficiently.',\n",
       " 'We utilize the close connection between BioChatter and the BioCypher framework (Johnson et al., 2020) to integrate knowledge graph (KG) queries into the BioChatter API. In the BioCypher KG creation, we use a configuration file to map KG contents to ontology terms, including information about each of the entities. For instance, we detail the properties of a node and the source and target classes of an edge. Additionally, during the KG build process, we enrich this information and save it to a YAML file and, optionally, directly to the KG. This information is used by BioChatter to tune its understanding of the KG, which allows the Large Language Model (LLM) to query the KG more efficiently.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ec80c-0cd6-463f-86be-01d67b534293",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01518,
     "end_time": "2024-03-20T11:42:59.841282",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.826102",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "82850024-e286-412c-8957-e9ff2c640375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.872046Z",
     "iopub.status.busy": "2024-03-20T11:42:59.871869Z",
     "iopub.status.idle": "2024-03-20T11:42:59.873387Z",
     "shell.execute_reply": "2024-03-20T11:42:59.873228Z"
    },
    "papermill": {
     "duration": 0.017594,
     "end_time": "2024-03-20T11:42:59.873893",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.856299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By understanding the context of the KG, the exact contents, and the exact spelling of all identifiers and properties, we effectively support the LLM in generating correct queries. The query generation process is broken up into multiple steps by BioChatter: recognising entities and relationships according to the user's question, estimating properties to be used in the query, and generating a syntactically correct query in the query language of the database, based on the results from the previous steps and constraints given by the KG schema information. This procedure is implemented in the `prompts.py` module. To evaluate the quality of this process, we dedicate a module in the benchmark to the query generation process with a range of questions and KG schemata.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[23])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "87751798-732e-4cc5-9102-e900ab03ea0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.905080Z",
     "iopub.status.busy": "2024-03-20T11:42:59.904845Z",
     "iopub.status.idle": "2024-03-20T11:42:59.906492Z",
     "shell.execute_reply": "2024-03-20T11:42:59.906349Z"
    },
    "papermill": {
     "duration": 0.017857,
     "end_time": "2024-03-20T11:42:59.906977",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.889120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$$ \n",
      "\\text{BioCypher KG creation process:} \n",
      "$$ {#kg_creation}\n",
      "The BioCypher KG creation process involves using a configuration file to map KG contents to ontology terms. This includes detailing the properties of a node and the source and target classes of an edge. The enriched information is saved to a YAML file and, if desired, directly to the KG. This information is crucial for BioChatter to optimize its understanding of the KG, enabling the LLM to query the KG more effectively. By understanding the context of the knowledge graph (KG), the exact contents, and the exact spelling of all identifiers and properties, we effectively support the Large Language Model (LLM) in generating correct queries. The query generation process is broken up into multiple steps by BioChatter: recognizing entities and relationships according to the user's question, estimating properties to be used in the query, and generating a syntactically correct query in the query language of the database, based on the results from the previous steps and constraints given by the KG schema information. This procedure is implemented in the `prompts.py` module. $$ \\text{To evaluate the quality of this process, we dedicate a module in the benchmark to the query generation process with a range of questions and KG schemata.} $$\n"
     ]
    }
   ],
   "source": [
    "par1 = (\n",
    "    process_paragraph(mod_section_paragraphs[27:28])\n",
    "    .replace(\"$$\", \"\\n$$\")\n",
    "    .replace(\"\\\\text\", \"\\n\\\\text\")\n",
    "    + \"\\n\"\n",
    "    + process_paragraph(mod_section_paragraphs[28:31])\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "508cd603-49b6-4ba1-af1d-11e5bf2ed5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.938186Z",
     "iopub.status.busy": "2024-03-20T11:42:59.938083Z",
     "iopub.status.idle": "2024-03-20T11:42:59.939405Z",
     "shell.execute_reply": "2024-03-20T11:42:59.939274Z"
    },
    "papermill": {
     "duration": 0.017375,
     "end_time": "2024-03-20T11:42:59.939851",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.922476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e2fb1189-ea4f-488a-bf0c-3e8e4abf8e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:42:59.970722Z",
     "iopub.status.busy": "2024-03-20T11:42:59.970607Z",
     "iopub.status.idle": "2024-03-20T11:42:59.972515Z",
     "shell.execute_reply": "2024-03-20T11:42:59.972286Z"
    },
    "papermill": {
     "duration": 0.017856,
     "end_time": "2024-03-20T11:42:59.973012",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.955156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " \"By understanding the context of the KG, the exact contents, and the exact spelling of all identifiers and properties, we effectively support the LLM in generating correct queries. The query generation process is broken up into multiple steps by BioChatter: recognising entities and relationships according to the user's question, estimating properties to be used in the query, and generating a syntactically correct query in the query language of the database, based on the results from the previous steps and constraints given by the KG schema information. This procedure is implemented in the `prompts.py` module. To evaluate the quality of this process, we dedicate a module in the benchmark to the query generation process with a range of questions and KG schemata.\",\n",
       " \"\\n$$ \\n\\\\text{BioCypher KG creation process:} \\n$$ {#kg_creation}\\nThe BioCypher KG creation process involves using a configuration file to map KG contents to ontology terms. This includes detailing the properties of a node and the source and target classes of an edge. The enriched information is saved to a YAML file and, if desired, directly to the KG. This information is crucial for BioChatter to optimize its understanding of the KG, enabling the LLM to query the KG more effectively. By understanding the context of the knowledge graph (KG), the exact contents, and the exact spelling of all identifiers and properties, we effectively support the Large Language Model (LLM) in generating correct queries. The query generation process is broken up into multiple steps by BioChatter: recognizing entities and relationships according to the user's question, estimating properties to be used in the query, and generating a syntactically correct query in the query language of the database, based on the results from the previous steps and constraints given by the KG schema information. This procedure is implemented in the `prompts.py` module. $$ \\\\text{To evaluate the quality of this process, we dedicate a module in the benchmark to the query generation process with a range of questions and KG schemata.} $$\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f18a6-ad27-4d63-97cd-b0a6b47fad61",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01529,
     "end_time": "2024-03-20T11:43:00.003515",
     "exception": false,
     "start_time": "2024-03-20T11:42:59.988225",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "66500345-b007-47cc-94f3-4b422cd13529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.034735Z",
     "iopub.status.busy": "2024-03-20T11:43:00.034658Z",
     "iopub.status.idle": "2024-03-20T11:43:00.036731Z",
     "shell.execute_reply": "2024-03-20T11:43:00.036514Z"
    },
    "papermill": {
     "duration": 0.018336,
     "end_time": "2024-03-20T11:43:00.037262",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.018926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To illustrate the usage of this feature, we provide a demonstration repository at [https://github.com/biocypher/pole](https://github.com/biocypher/pole) including a KG build procedure and an instance of BioChatter Light, which can be run using a single Docker Compose command. The pole KG can also be used in conjunction with the BioChatter Next app by using the `docker-compose.yaml` file to build the application locally. A demonstration of this use case is available in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/)).\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[24])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "08baadce-7877-4b62-bebc-de9013d6a16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.068257Z",
     "iopub.status.busy": "2024-03-20T11:43:00.068134Z",
     "iopub.status.idle": "2024-03-20T11:43:00.069853Z",
     "shell.execute_reply": "2024-03-20T11:43:00.069710Z"
    },
    "papermill": {
     "duration": 0.01755,
     "end_time": "2024-03-20T11:43:00.070229",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.052679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This evaluation process helps us understand how well the LLM can generate queries based on the information provided by the KG and the user's input. It allows us to assess the accuracy and efficiency of the query generation process in different scenarios and with various types of KG schemata. To demonstrate the utility of this feature, we have created a demonstration repository available at [https://github.com/biocypher/pole](https://github.com/biocypher/pole). This repository includes a procedure for building a knowledge graph (KG) and an instance of BioChatter Light, which can be easily run using a single Docker Compose command. Furthermore, the pole KG can be utilized in combination with the BioChatter Next application by utilizing the `docker-compose.yaml` file to locally build the application. A demonstration showcasing this use case can be found in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and is also accessible on our website at [https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/).\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    [\n",
    "        process_paragraph(mod_section_paragraphs[31:32])\n",
    "        .replace(\"$$\", \"\\n$$\")\n",
    "        .replace(\"\\\\text\", \"\\n\\\\text\"),\n",
    "        process_paragraph(mod_section_paragraphs[32:33]),\n",
    "    ]\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "41782770-bcc9-451d-ab67-4fc0c17bd7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.101179Z",
     "iopub.status.busy": "2024-03-20T11:43:00.101001Z",
     "iopub.status.idle": "2024-03-20T11:43:00.102455Z",
     "shell.execute_reply": "2024-03-20T11:43:00.102237Z"
    },
    "papermill": {
     "duration": 0.017475,
     "end_time": "2024-03-20T11:43:00.102907",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.085432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "446db340-f8eb-4410-a0ef-8ada3f8eb95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.134311Z",
     "iopub.status.busy": "2024-03-20T11:43:00.134190Z",
     "iopub.status.idle": "2024-03-20T11:43:00.136163Z",
     "shell.execute_reply": "2024-03-20T11:43:00.135929Z"
    },
    "papermill": {
     "duration": 0.018443,
     "end_time": "2024-03-20T11:43:00.136645",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.118202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'To illustrate the usage of this feature, we provide a demonstration repository at [https://github.com/biocypher/pole](https://github.com/biocypher/pole) including a KG build procedure and an instance of BioChatter Light, which can be run using a single Docker Compose command. The pole KG can also be used in conjunction with the BioChatter Next app by using the `docker-compose.yaml` file to build the application locally. A demonstration of this use case is available in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and on our website ([https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/)).',\n",
       " \"This evaluation process helps us understand how well the LLM can generate queries based on the information provided by the KG and the user's input. It allows us to assess the accuracy and efficiency of the query generation process in different scenarios and with various types of KG schemata. To demonstrate the utility of this feature, we have created a demonstration repository available at [https://github.com/biocypher/pole](https://github.com/biocypher/pole). This repository includes a procedure for building a knowledge graph (KG) and an instance of BioChatter Light, which can be easily run using a single Docker Compose command. Furthermore, the pole KG can be utilized in combination with the BioChatter Next application by utilizing the `docker-compose.yaml` file to locally build the application. A demonstration showcasing this use case can be found in [Supplementary Note 1: Knowledge Graph Retrieval-Augmented Generation] and is also accessible on our website at [https://biochatter.org/vignette-kg/](https://biochatter.org/vignette-kg/).\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0f1f4-9bea-45e5-80a8-da1b848b2bb7",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.016066,
     "end_time": "2024-03-20T11:43:00.168192",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.152126",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a6c5b537-549a-4af8-bfc7-ed3ad060e0f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.199786Z",
     "iopub.status.busy": "2024-03-20T11:43:00.199290Z",
     "iopub.status.idle": "2024-03-20T11:43:00.201356Z",
     "shell.execute_reply": "2024-03-20T11:43:00.201163Z"
    },
    "papermill": {
     "duration": 0.018317,
     "end_time": "2024-03-20T11:43:00.201844",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.183527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While current LLMs possess extensive internal general knowledge, they may not know how to prioritise very specific scientific results, or they may not have had access to some research articles in their training data (e.g., due to their recency or licensing issues). To bridge this gap, we can provide additional information from relevant publications to the model via the prompt. However, we frequently cannot add entire publications to the prompt, since the input length of current models is still restricted; we need to isolate the information that is specifically relevant to the question given by the user. To find this information, we perform a semantic similarity search between the user’s question and the contents of user-provided scientific articles (or other texts). The most efficient way to do this mapping is by using a vector database [@doi:10.48550/arxiv.2308.07107].\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[26])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ebee10e-5858-4850-954b-876b323a2735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.233131Z",
     "iopub.status.busy": "2024-03-20T11:43:00.232977Z",
     "iopub.status.idle": "2024-03-20T11:43:00.234437Z",
     "shell.execute_reply": "2024-03-20T11:43:00.234290Z"
    },
    "papermill": {
     "duration": 0.017476,
     "end_time": "2024-03-20T11:43:00.234847",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.217371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While current Large Language Models (LLMs) possess extensive internal general knowledge, they may not prioritize very specific scientific results or have access to all research articles in their training data, possibly due to recency or licensing issues. To address this limitation, we can enhance the model's knowledge by providing additional information from relevant publications through the prompt. However, due to the current input length restrictions of LLMs, we cannot include entire publications in the prompt. Therefore, we must identify and isolate the information that is directly pertinent to the user's query. To achieve this, we conduct a semantic similarity search between the user's question and the content of user-provided scientific articles or other texts. Utilizing a vector database is the most effective method for this mapping process [@doi:10.48550/arxiv.2308.07107].\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    [\n",
    "        process_paragraph(\n",
    "            mod_section_paragraphs[34:35]\n",
    "        ),  # .replace(\"$$\", \"\\n$$\").replace(\"\\\\text\", \"\\n\\\\text\"),\n",
    "        # process_paragraph(mod_section_paragraphs[32:33]),\n",
    "    ]\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "821518fe-5d6b-45e4-81d0-a69cbcd696a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.266158Z",
     "iopub.status.busy": "2024-03-20T11:43:00.266070Z",
     "iopub.status.idle": "2024-03-20T11:43:00.267373Z",
     "shell.execute_reply": "2024-03-20T11:43:00.267220Z"
    },
    "papermill": {
     "duration": 0.017442,
     "end_time": "2024-03-20T11:43:00.267750",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.250308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c7298e1c-f3dc-4128-969b-7500ec2ef2fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.311531Z",
     "iopub.status.busy": "2024-03-20T11:43:00.311434Z",
     "iopub.status.idle": "2024-03-20T11:43:00.313524Z",
     "shell.execute_reply": "2024-03-20T11:43:00.313383Z"
    },
    "papermill": {
     "duration": 0.030787,
     "end_time": "2024-03-20T11:43:00.313960",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.283173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'While current LLMs possess extensive internal general knowledge, they may not know how to prioritise very specific scientific results, or they may not have had access to some research articles in their training data (e.g., due to their recency or licensing issues). To bridge this gap, we can provide additional information from relevant publications to the model via the prompt. However, we frequently cannot add entire publications to the prompt, since the input length of current models is still restricted; we need to isolate the information that is specifically relevant to the question given by the user. To find this information, we perform a semantic similarity search between the user’s question and the contents of user-provided scientific articles (or other texts). The most efficient way to do this mapping is by using a vector database [@doi:10.48550/arxiv.2308.07107].',\n",
       " \"While current Large Language Models (LLMs) possess extensive internal general knowledge, they may not prioritize very specific scientific results or have access to all research articles in their training data, possibly due to recency or licensing issues. To address this limitation, we can enhance the model's knowledge by providing additional information from relevant publications through the prompt. However, due to the current input length restrictions of LLMs, we cannot include entire publications in the prompt. Therefore, we must identify and isolate the information that is directly pertinent to the user's query. To achieve this, we conduct a semantic similarity search between the user's question and the content of user-provided scientific articles or other texts. Utilizing a vector database is the most effective method for this mapping process [@doi:10.48550/arxiv.2308.07107].\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f16b0-fc6c-4500-a6fa-09f9cd6f6a7a",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015505,
     "end_time": "2024-03-20T11:43:00.345145",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.329640",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1a27d782-282c-4ec1-bf8d-ce5b29bd3ce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.376578Z",
     "iopub.status.busy": "2024-03-20T11:43:00.376483Z",
     "iopub.status.idle": "2024-03-20T11:43:00.378590Z",
     "shell.execute_reply": "2024-03-20T11:43:00.378300Z"
    },
    "papermill": {
     "duration": 0.018564,
     "end_time": "2024-03-20T11:43:00.379093",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.360529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contextual background information provided by the user (e.g., by uploading a scientific article of prior work related to the experiment to be interpreted) is split into pieces suitable to be digested by the LLM, which are individually embedded by the model. These embeddings (represented by vectors) are used to store the text fragments in a vector database; the storage as vectors allows fast and efficient retrieval of similar entities via the comparison of individual vectors. For example, the two sentences “Amyloid beta levels are associated with Alzheimer’s Disease stage.” and “One of the most important clinical markers of AD progression is the amount of deposited A-beta 42.” would be closely associated in a vector database (given the embedding model is of sufficient quality, i.e., similar to GPT-3 or better), while traditional text-based similarity metrics probably would not identify them as highly similar.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[27])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8d0ccfde-80db-4579-8b53-865e22114499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.410524Z",
     "iopub.status.busy": "2024-03-20T11:43:00.410449Z",
     "iopub.status.idle": "2024-03-20T11:43:00.412615Z",
     "shell.execute_reply": "2024-03-20T11:43:00.412451Z"
    },
    "papermill": {
     "duration": 0.018671,
     "end_time": "2024-03-20T11:43:00.413177",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.394506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$ \n",
      "LLM: Large Language Model\n",
      "$$ \n",
      "The contextual background information provided by the user, such as a scientific article related to the experiment to be interpreted, is divided into segments suitable for processing by the Large Language Model (LLM). Each segment is individually embedded by the model. These embeddings, represented as vectors, are utilized to store the text fragments in a vector database. Storing the information as vectors enables rapid and efficient retrieval of similar entities through the comparison of individual vectors. For instance, the sentences \"Amyloid beta levels are associated with Alzheimer's Disease stage.\" and \"One of the most crucial clinical markers of AD progression is the amount of deposited A-beta 42.\" would be closely linked in a vector database if the embedding model is of high quality, similar to GPT-3 or superior. In contrast, traditional text-based similarity metrics might not recognize them as highly similar.\n"
     ]
    }
   ],
   "source": [
    "par1 = (\n",
    "    process_paragraph(\n",
    "        [\n",
    "            process_paragraph(mod_section_paragraphs[35:36]),\n",
    "            process_paragraph(mod_section_paragraphs[36:37]),\n",
    "        ]\n",
    "    )\n",
    "    .replace(\"Model $$\", \"Model\\n$$\")\n",
    "    .replace(\"LLM:\", \"\\nLLM:\")\n",
    "    .replace(\"The contextual\", \"\\nThe contextual\")\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "eb8cb2e1-fc13-49a0-8b34-812b56570b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.444356Z",
     "iopub.status.busy": "2024-03-20T11:43:00.444278Z",
     "iopub.status.idle": "2024-03-20T11:43:00.446096Z",
     "shell.execute_reply": "2024-03-20T11:43:00.445870Z"
    },
    "papermill": {
     "duration": 0.017959,
     "end_time": "2024-03-20T11:43:00.446551",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.428592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "cbf82494-1147-44af-800a-81a6677b3076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.477913Z",
     "iopub.status.busy": "2024-03-20T11:43:00.477823Z",
     "iopub.status.idle": "2024-03-20T11:43:00.479798Z",
     "shell.execute_reply": "2024-03-20T11:43:00.479595Z"
    },
    "papermill": {
     "duration": 0.018229,
     "end_time": "2024-03-20T11:43:00.480264",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.462035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'The contextual background information provided by the user (e.g., by uploading a scientific article of prior work related to the experiment to be interpreted) is split into pieces suitable to be digested by the LLM, which are individually embedded by the model. These embeddings (represented by vectors) are used to store the text fragments in a vector database; the storage as vectors allows fast and efficient retrieval of similar entities via the comparison of individual vectors. For example, the two sentences “Amyloid beta levels are associated with Alzheimer’s Disease stage.” and “One of the most important clinical markers of AD progression is the amount of deposited A-beta 42.” would be closely associated in a vector database (given the embedding model is of sufficient quality, i.e., similar to GPT-3 or better), while traditional text-based similarity metrics probably would not identify them as highly similar.',\n",
       " '$$ \\nLLM: Large Language Model\\n$$ \\nThe contextual background information provided by the user, such as a scientific article related to the experiment to be interpreted, is divided into segments suitable for processing by the Large Language Model (LLM). Each segment is individually embedded by the model. These embeddings, represented as vectors, are utilized to store the text fragments in a vector database. Storing the information as vectors enables rapid and efficient retrieval of similar entities through the comparison of individual vectors. For instance, the sentences \"Amyloid beta levels are associated with Alzheimer\\'s Disease stage.\" and \"One of the most crucial clinical markers of AD progression is the amount of deposited A-beta 42.\" would be closely linked in a vector database if the embedding model is of high quality, similar to GPT-3 or superior. In contrast, traditional text-based similarity metrics might not recognize them as highly similar.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85459dcb-67ad-4e49-8ca5-8e11ca5aaeb5",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015313,
     "end_time": "2024-03-20T11:43:00.510967",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.495654",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fce240b5-e631-4acb-acd5-d30d9c0ae4d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.541968Z",
     "iopub.status.busy": "2024-03-20T11:43:00.541906Z",
     "iopub.status.idle": "2024-03-20T11:43:00.543559Z",
     "shell.execute_reply": "2024-03-20T11:43:00.543339Z"
    },
    "papermill": {
     "duration": 0.017729,
     "end_time": "2024-03-20T11:43:00.543988",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.526259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background. Even better, we can first use an LLM to generate an answer to the user's question and then use this answer to query the vector database for relevant information. Regardless of whether the initial answer is correct, it is likely that the \"fake answer\" is more semantically similar to the relevant pieces of information than the user's question [@doi:10.48550/arXiv.2308.07107]. Semantic search results (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt. In this way, the model can learn from additional context without the need for retraining or fine-tuning. This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[28])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "18477820-18f8-48fe-baa1-2acf2b7cb069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.575152Z",
     "iopub.status.busy": "2024-03-20T11:43:00.575090Z",
     "iopub.status.idle": "2024-03-20T11:43:00.576562Z",
     "shell.execute_reply": "2024-03-20T11:43:00.576422Z"
    },
    "papermill": {
     "duration": 0.017823,
     "end_time": "2024-03-20T11:43:00.577154",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.559331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background. Even better, we can first use a Large Language Model (LLM) to generate an answer to the user's question and then use this answer to query the vector database for relevant information. Regardless of whether the initial answer is correct, it is likely that the \"fake answer\" is more semantically similar to the relevant pieces of information than the user's question [@doi:10.48550/arXiv.2308.07107]. Semantic search results (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt. In this way, the model can learn from additional context without the need for retraining or fine-tuning. This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[38])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b1daa321-bb72-4218-bbdb-170bf52af562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.608593Z",
     "iopub.status.busy": "2024-03-20T11:43:00.608514Z",
     "iopub.status.idle": "2024-03-20T11:43:00.609988Z",
     "shell.execute_reply": "2024-03-20T11:43:00.609845Z"
    },
    "papermill": {
     "duration": 0.017799,
     "end_time": "2024-03-20T11:43:00.610550",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.592751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7a0d3bf4-1317-46ba-94e7-54e869c11c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.641878Z",
     "iopub.status.busy": "2024-03-20T11:43:00.641797Z",
     "iopub.status.idle": "2024-03-20T11:43:00.643606Z",
     "shell.execute_reply": "2024-03-20T11:43:00.643399Z"
    },
    "papermill": {
     "duration": 0.018115,
     "end_time": "2024-03-20T11:43:00.644174",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.626059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background. Even better, we can first use an LLM to generate an answer to the user\\'s question and then use this answer to query the vector database for relevant information. Regardless of whether the initial answer is correct, it is likely that the \"fake answer\" is more semantically similar to the relevant pieces of information than the user\\'s question [@doi:10.48550/arXiv.2308.07107]. Semantic search results (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt. In this way, the model can learn from additional context without the need for retraining or fine-tuning. This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].',\n",
       " 'By comparing the user’s question to prior knowledge in the vector database, we can extract the relevant pieces of information from the entire background. Even better, we can first use a Large Language Model (LLM) to generate an answer to the user\\'s question and then use this answer to query the vector database for relevant information. Regardless of whether the initial answer is correct, it is likely that the \"fake answer\" is more semantically similar to the relevant pieces of information than the user\\'s question [@doi:10.48550/arXiv.2308.07107]. Semantic search results (for instance, single sentences directly related to the topic of the question) are then sufficiently small to be added to the prompt. In this way, the model can learn from additional context without the need for retraining or fine-tuning. This method is sometimes described as in-context learning [@doi:10.48550/arxiv.2303.17580] or retrieval-augmented generation [@rag].')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5a166-fa52-40e7-a0e0-996a10f3c848",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015337,
     "end_time": "2024-03-20T11:43:00.675049",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.659712",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "661dafe8-4ec6-4d73-983a-8aa008072e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.706270Z",
     "iopub.status.busy": "2024-03-20T11:43:00.706198Z",
     "iopub.status.idle": "2024-03-20T11:43:00.707855Z",
     "shell.execute_reply": "2024-03-20T11:43:00.707629Z"
    },
    "papermill": {
     "duration": 0.017877,
     "end_time": "2024-03-20T11:43:00.708337",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.690460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide access to this functionality in BioChatter, we implement classes for the connection to, and management of, vector database systems (in the `vectorstore.py` module), and for performing semantic search on the vector database and injecting the results into the prompt (in the `vectorstore_agent.py` module). An analogous implementation for KG retrieval is available in the `database_agent.py` module. Both retrieval mechanisms are integrated and provided to the BioChatter API via the `rag_agent.py` module. To demonstrate the use of the API, we add a “Retrieval-Augmented Generation” tab to the preview apps that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model. This contextual information is transparently displayed. Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus service, including a way to start the service in conjunction with a BioCypher knowledge graph and the BioChatter Light app in one Docker Compose workflow.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[29])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bd1fe885-db17-4e1a-b6a6-f488bc1637cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.739545Z",
     "iopub.status.busy": "2024-03-20T11:43:00.739416Z",
     "iopub.status.idle": "2024-03-20T11:43:00.740874Z",
     "shell.execute_reply": "2024-03-20T11:43:00.740734Z"
    },
    "papermill": {
     "duration": 0.017696,
     "end_time": "2024-03-20T11:43:00.741437",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.723741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide access to this functionality in BioChatter, we implement classes for the connection to, and management of, vector database systems (in the `vectorstore.py` module), and for performing semantic search on the vector database and injecting the results into the prompt (in the `vectorstore_agent.py` module). An analogous implementation for Knowledge Graph (KG) retrieval is available in the `database_agent.py` module. Both retrieval mechanisms are integrated and provided to the BioChatter API via the `rag_agent.py` module. To demonstrate the use of the API, we add a “Retrieval-Augmented Generation” tab to the preview apps that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model. This contextual information is transparently displayed. Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus service, including a way to start the service in conjunction with a BioCypher knowledge graph and the BioChatter Light app in one Docker Compose workflow.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(mod_section_paragraphs[40:43])\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ed501ecc-588f-45af-b49c-02a74d682535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.773188Z",
     "iopub.status.busy": "2024-03-20T11:43:00.772904Z",
     "iopub.status.idle": "2024-03-20T11:43:00.774354Z",
     "shell.execute_reply": "2024-03-20T11:43:00.774212Z"
    },
    "papermill": {
     "duration": 0.018006,
     "end_time": "2024-03-20T11:43:00.774921",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.756915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "85983bcb-b02c-4cda-a9de-61dfc1d23741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.807567Z",
     "iopub.status.busy": "2024-03-20T11:43:00.807434Z",
     "iopub.status.idle": "2024-03-20T11:43:00.809465Z",
     "shell.execute_reply": "2024-03-20T11:43:00.809307Z"
    },
    "papermill": {
     "duration": 0.01902,
     "end_time": "2024-03-20T11:43:00.809991",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.790971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'To provide access to this functionality in BioChatter, we implement classes for the connection to, and management of, vector database systems (in the `vectorstore.py` module), and for performing semantic search on the vector database and injecting the results into the prompt (in the `vectorstore_agent.py` module). An analogous implementation for KG retrieval is available in the `database_agent.py` module. Both retrieval mechanisms are integrated and provided to the BioChatter API via the `rag_agent.py` module. To demonstrate the use of the API, we add a “Retrieval-Augmented Generation” tab to the preview apps that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model. This contextual information is transparently displayed. Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus service, including a way to start the service in conjunction with a BioCypher knowledge graph and the BioChatter Light app in one Docker Compose workflow.',\n",
       " 'To provide access to this functionality in BioChatter, we implement classes for the connection to, and management of, vector database systems (in the `vectorstore.py` module), and for performing semantic search on the vector database and injecting the results into the prompt (in the `vectorstore_agent.py` module). An analogous implementation for Knowledge Graph (KG) retrieval is available in the `database_agent.py` module. Both retrieval mechanisms are integrated and provided to the BioChatter API via the `rag_agent.py` module. To demonstrate the use of the API, we add a “Retrieval-Augmented Generation” tab to the preview apps that allows the upload of text documents to be added to a vector database, which then can be queried to add contextual information to the prompt sent to the primary model. This contextual information is transparently displayed. Since this functionality requires a connection to a vector database system, we provide connectivity to a Milvus service, including a way to start the service in conjunction with a BioCypher knowledge graph and the BioChatter Light app in one Docker Compose workflow.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268afe6a-8550-4c0e-a667-4f5b4d3e5cee",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015515,
     "end_time": "2024-03-20T11:43:00.841107",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.825592",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d0597d98-6dbf-4eb8-ab9b-2a93ac15cd2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.872578Z",
     "iopub.status.busy": "2024-03-20T11:43:00.872474Z",
     "iopub.status.idle": "2024-03-20T11:43:00.874093Z",
     "shell.execute_reply": "2024-03-20T11:43:00.873931Z"
    },
    "papermill": {
     "duration": 0.018014,
     "end_time": "2024-03-20T11:43:00.874590",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.856576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To facilitate access to open-source models, we adopt a flexible deployment framework based on the Xorbits Inference API [@{https://github.com/xorbitsai/inference}]. Xorbits Inference includes a large number of open-source models out of the box, and new models from Hugging Face Hub [@{https://huggingface.co/}] can be added using the intuitive graphical user interface. We used Xorbits Inference version 0.8.4 to deploy the benchmarked models, and we provide a Docker Compose repository to deploy the app on a Linux server with Nvidia GPUs ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)). This Compose uses the multi-architecture image (for ARM64 and AMD64 chips) we provide on our Docker Hub organisation ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)). On Mac OS with Apple Silicon chips, Docker does not have access to the GPU driver, and as such, Xinference needs to be deployed natively.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[32])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d0747d6a-3c0e-47bb-b688-3cd618530e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.906644Z",
     "iopub.status.busy": "2024-03-20T11:43:00.906292Z",
     "iopub.status.idle": "2024-03-20T11:43:00.908605Z",
     "shell.execute_reply": "2024-03-20T11:43:00.908313Z"
    },
    "papermill": {
     "duration": 0.018913,
     "end_time": "2024-03-20T11:43:00.909132",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.890219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To facilitate access to open-source models, we adopted a flexible deployment framework based on the Xorbits Inference API [@{https://github.com/xorbitsai/inference}]. Xorbits Inference includes a large number of open-source models out of the box, and new models from Hugging Face Hub [@{https://huggingface.co/}] can be added using the intuitive graphical user interface. We utilized Xorbits Inference version 0.8.4 to deploy the benchmarked models, and we provide a Docker Compose repository to deploy the app on a Linux server with Nvidia GPUs ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)). This Compose uses the multi-architecture image (for ARM64 and AMD64 chips) we provide on our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)).\n",
      "\n",
      "$$\n",
      "X = Y + Z \n",
      "$$ {#eq1} \n",
      "\n",
      "On Mac OS with Apple Silicon chips, Docker does not have access to the GPU driver, and as such, Xinference needs to be deployed natively.\n"
     ]
    }
   ],
   "source": [
    "par1 = (\n",
    "    process_paragraph(\n",
    "        [\n",
    "            process_paragraph(mod_section_paragraphs[45:46]),\n",
    "            mod_section_paragraphs[46],\n",
    "            process_paragraph(mod_section_paragraphs[47]),\n",
    "        ]\n",
    "    )\n",
    "    .replace(\" $$ X\", \"\\n\\n$$\\nX\")\n",
    "    .replace(\"$$ {\", \"\\n$$ {\")\n",
    "    .replace(\"On Mac OS\", \"\\n\\nOn Mac OS\")\n",
    ")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d6b097a7-aec7-4984-95bb-1ce7f5d5c476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.941015Z",
     "iopub.status.busy": "2024-03-20T11:43:00.940910Z",
     "iopub.status.idle": "2024-03-20T11:43:00.942176Z",
     "shell.execute_reply": "2024-03-20T11:43:00.942044Z"
    },
    "papermill": {
     "duration": 0.017839,
     "end_time": "2024-03-20T11:43:00.942743",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.924904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ee20b3ab-dc07-4d49-acb7-46a72d9d4531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:00.974416Z",
     "iopub.status.busy": "2024-03-20T11:43:00.974240Z",
     "iopub.status.idle": "2024-03-20T11:43:00.975820Z",
     "shell.execute_reply": "2024-03-20T11:43:00.975689Z"
    },
    "papermill": {
     "duration": 0.018045,
     "end_time": "2024-03-20T11:43:00.976302",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.958257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'To facilitate access to open-source models, we adopt a flexible deployment framework based on the Xorbits Inference API [@{https://github.com/xorbitsai/inference}]. Xorbits Inference includes a large number of open-source models out of the box, and new models from Hugging Face Hub [@{https://huggingface.co/}] can be added using the intuitive graphical user interface. We used Xorbits Inference version 0.8.4 to deploy the benchmarked models, and we provide a Docker Compose repository to deploy the app on a Linux server with Nvidia GPUs ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)). This Compose uses the multi-architecture image (for ARM64 and AMD64 chips) we provide on our Docker Hub organisation ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)). On Mac OS with Apple Silicon chips, Docker does not have access to the GPU driver, and as such, Xinference needs to be deployed natively.',\n",
       " 'To facilitate access to open-source models, we adopted a flexible deployment framework based on the Xorbits Inference API [@{https://github.com/xorbitsai/inference}]. Xorbits Inference includes a large number of open-source models out of the box, and new models from Hugging Face Hub [@{https://huggingface.co/}] can be added using the intuitive graphical user interface. We utilized Xorbits Inference version 0.8.4 to deploy the benchmarked models, and we provide a Docker Compose repository to deploy the app on a Linux server with Nvidia GPUs ([https://github.com/biocypher/xinference-docker-builtin/](https://github.com/biocypher/xinference-docker-builtin/)). This Compose uses the multi-architecture image (for ARM64 and AMD64 chips) we provide on our Docker Hub organization ([https://hub.docker.com/repository/docker/biocypher/xinference-builtin](https://hub.docker.com/repository/docker/biocypher/xinference-builtin)).\\n\\n$$\\nX = Y + Z \\n$$ {#eq1} \\n\\nOn Mac OS with Apple Silicon chips, Docker does not have access to the GPU driver, and as such, Xinference needs to be deployed natively.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae437b3-4af8-4e4f-9b50-1228b8449d3c",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.029223,
     "end_time": "2024-03-20T11:43:01.021134",
     "exception": false,
     "start_time": "2024-03-20T11:43:00.991911",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ea7f45d7-9c41-49f3-93a0-6b386ea81137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.053733Z",
     "iopub.status.busy": "2024-03-20T11:43:01.053613Z",
     "iopub.status.idle": "2024-03-20T11:43:01.055200Z",
     "shell.execute_reply": "2024-03-20T11:43:01.055052Z"
    },
    "papermill": {
     "duration": 0.018447,
     "end_time": "2024-03-20T11:43:01.055625",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.037178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ability of LLMs to control external software, including other LLMs, opens up a wide range of possibilities for the orchestration of complex tasks. A simple example is the implementation of a correcting agent, which receives the output of the primary model and checks it for factual correctness. If the agent detects an error, it can prompt the primary model to correct its output, or forward this correction to the user directly. Since this relies on the internal knowledge base of the correcting agent, the same caveats apply, as the correcting agent may confabulate as well. However, since the agent is independent of the primary model (being set up with dedicated prompts), it is less likely to confabulate in the same way.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[34])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1deca811-bc29-4fea-ae02-d1ce3698d2ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.087939Z",
     "iopub.status.busy": "2024-03-20T11:43:01.087829Z",
     "iopub.status.idle": "2024-03-20T11:43:01.089313Z",
     "shell.execute_reply": "2024-03-20T11:43:01.089151Z"
    },
    "papermill": {
     "duration": 0.018318,
     "end_time": "2024-03-20T11:43:01.089918",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.071600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capability of Large Language Models (LLMs) to interact with external software, including other LLMs, presents a multitude of opportunities for orchestrating intricate tasks. An illustrative instance involves the deployment of a correction agent, which receives the primary model's output and verifies its factual accuracy. In case an error is identified, the agent can either prompt the primary model to rectify its output or directly convey the correction to the user. This process is contingent upon the internal knowledge repository of the correction agent, thereby subject to similar limitations, as the agent may also produce incorrect information. Nevertheless, due to the agent's autonomy from the primary model (established with specific prompts), it is less susceptible to generating erroneous data in the same manner.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    [\n",
    "        process_paragraph(mod_section_paragraphs[49]),\n",
    "        # mod_section_paragraphs[46],\n",
    "        # process_paragraph(mod_section_paragraphs[47])\n",
    "    ]\n",
    ")  # .replace(\" $$ X\", \"\\n\\n$$\\nX\").replace(\"$$ {\", \"\\n$$ {\").replace(\"On Mac OS\", \"\\n\\nOn Mac OS\")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0d6a4164-6116-49f3-9b50-6d107204a8a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.122125Z",
     "iopub.status.busy": "2024-03-20T11:43:01.121958Z",
     "iopub.status.idle": "2024-03-20T11:43:01.123237Z",
     "shell.execute_reply": "2024-03-20T11:43:01.123079Z"
    },
    "papermill": {
     "duration": 0.017772,
     "end_time": "2024-03-20T11:43:01.123632",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.105860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0cb7edaf-ec14-4280-9811-276e22df8289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.155990Z",
     "iopub.status.busy": "2024-03-20T11:43:01.155866Z",
     "iopub.status.idle": "2024-03-20T11:43:01.157939Z",
     "shell.execute_reply": "2024-03-20T11:43:01.157751Z"
    },
    "papermill": {
     "duration": 0.019038,
     "end_time": "2024-03-20T11:43:01.158448",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.139410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'The ability of LLMs to control external software, including other LLMs, opens up a wide range of possibilities for the orchestration of complex tasks. A simple example is the implementation of a correcting agent, which receives the output of the primary model and checks it for factual correctness. If the agent detects an error, it can prompt the primary model to correct its output, or forward this correction to the user directly. Since this relies on the internal knowledge base of the correcting agent, the same caveats apply, as the correcting agent may confabulate as well. However, since the agent is independent of the primary model (being set up with dedicated prompts), it is less likely to confabulate in the same way.',\n",
       " \"The capability of Large Language Models (LLMs) to interact with external software, including other LLMs, presents a multitude of opportunities for orchestrating intricate tasks. An illustrative instance involves the deployment of a correction agent, which receives the primary model's output and verifies its factual accuracy. In case an error is identified, the agent can either prompt the primary model to rectify its output or directly convey the correction to the user. This process is contingent upon the internal knowledge repository of the correction agent, thereby subject to similar limitations, as the agent may also produce incorrect information. Nevertheless, due to the agent's autonomy from the primary model (established with specific prompts), it is less susceptible to generating erroneous data in the same manner.\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc7183-77da-4a67-801b-8a5f92687ee0",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015928,
     "end_time": "2024-03-20T11:43:01.190459",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.174531",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "####  Paragraph 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ddf1d0d3-7018-4ef2-a2e5-9fce257a19c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.222848Z",
     "iopub.status.busy": "2024-03-20T11:43:01.222748Z",
     "iopub.status.idle": "2024-03-20T11:43:01.224341Z",
     "shell.execute_reply": "2024-03-20T11:43:01.224157Z"
    },
    "papermill": {
     "duration": 0.018484,
     "end_time": "2024-03-20T11:43:01.224812",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.206328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This approach can be extended to a more complex model chain, where the correcting agent, for example, can query a knowledge graph or a vector database to ground its responses in prior knowledge. These chains are easy to implement, and some are available out of the box in the LangChain framework [@langchain]. However, they can behave unpredictably, which increases with the number of links in the chain and, as such, should be tightly controlled. They also add to the computational burden of the system, which is particularly relevant for deployments on end-user devices.\n"
     ]
    }
   ],
   "source": [
    "par0 = process_paragraph(orig_section_paragraphs[35])\n",
    "print(par0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e1ef7cfb-439e-4eea-bab1-34e900a6c424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.257088Z",
     "iopub.status.busy": "2024-03-20T11:43:01.256957Z",
     "iopub.status.idle": "2024-03-20T11:43:01.258415Z",
     "shell.execute_reply": "2024-03-20T11:43:01.258275Z"
    },
    "papermill": {
     "duration": 0.0182,
     "end_time": "2024-03-20T11:43:01.258992",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.240792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This approach can be extended to a more complex model chain, where the correcting agent, for example, can query a knowledge graph or a vector database to ground its responses in prior knowledge. These chains are easy to implement, and some are available out of the box in the LangChain framework (Doe et al., 2020). However, they can behave unpredictably, which increases with the number of links in the chain and, as such, should be tightly controlled. They also add to the computational burden of the system, which is particularly relevant for deployments on end-user devices.\n"
     ]
    }
   ],
   "source": [
    "par1 = process_paragraph(\n",
    "    [\n",
    "        process_paragraph(mod_section_paragraphs[51]),\n",
    "        # mod_section_paragraphs[46],\n",
    "        # process_paragraph(mod_section_paragraphs[47])\n",
    "    ]\n",
    ")  # .replace(\" $$ X\", \"\\n\\n$$\\nX\").replace(\"$$ {\", \"\\n$$ {\").replace(\"On Mac OS\", \"\\n\\nOn Mac OS\")\n",
    "print(par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c9363e84-d8b7-47df-97f6-863584381112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.291138Z",
     "iopub.status.busy": "2024-03-20T11:43:01.291031Z",
     "iopub.status.idle": "2024-03-20T11:43:01.292475Z",
     "shell.execute_reply": "2024-03-20T11:43:01.292145Z"
    },
    "papermill": {
     "duration": 0.018235,
     "end_time": "2024-03-20T11:43:01.293052",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.274817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "paragraph_matches.append(\n",
    "    (\n",
    "        section_name,\n",
    "        par0,\n",
    "        par1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c2c5d2b5-aa4a-4efd-9c4f-8edfbf70f5a8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.325531Z",
     "iopub.status.busy": "2024-03-20T11:43:01.325315Z",
     "iopub.status.idle": "2024-03-20T11:43:01.327163Z",
     "shell.execute_reply": "2024-03-20T11:43:01.327019Z"
    },
    "papermill": {
     "duration": 0.018605,
     "end_time": "2024-03-20T11:43:01.327554",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.308949",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('methods',\n",
       " 'This approach can be extended to a more complex model chain, where the correcting agent, for example, can query a knowledge graph or a vector database to ground its responses in prior knowledge. These chains are easy to implement, and some are available out of the box in the LangChain framework [@langchain]. However, they can behave unpredictably, which increases with the number of links in the chain and, as such, should be tightly controlled. They also add to the computational burden of the system, which is particularly relevant for deployments on end-user devices.',\n",
       " 'This approach can be extended to a more complex model chain, where the correcting agent, for example, can query a knowledge graph or a vector database to ground its responses in prior knowledge. These chains are easy to implement, and some are available out of the box in the LangChain framework (Doe et al., 2020). However, they can behave unpredictably, which increases with the number of links in the chain and, as such, should be tightly controlled. They also add to the computational burden of the system, which is particularly relevant for deployments on end-user devices.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(paragraph_matches[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45b8eb-9a57-4d0a-b311-b7ce8f6b4ceb",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015767,
     "end_time": "2024-03-20T11:43:01.359045",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.343278",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Close connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "91fcd4bb-131b-42b2-b64f-127441bd608b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.390980Z",
     "iopub.status.busy": "2024-03-20T11:43:01.390886Z",
     "iopub.status.idle": "2024-03-20T11:43:01.392634Z",
     "shell.execute_reply": "2024-03-20T11:43:01.392482Z"
    },
    "papermill": {
     "duration": 0.018589,
     "end_time": "2024-03-20T11:43:01.393352",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.374763",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b2259-6562-4d79-a0d2-a594c44a7fa8",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.015791,
     "end_time": "2024-03-20T11:43:01.424892",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.409101",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6b7e80bd-f678-4047-b0f6-3c4e55de4767",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.457008Z",
     "iopub.status.busy": "2024-03-20T11:43:01.456834Z",
     "iopub.status.idle": "2024-03-20T11:43:01.458347Z",
     "shell.execute_reply": "2024-03-20T11:43:01.458215Z"
    },
    "papermill": {
     "duration": 0.018266,
     "end_time": "2024-03-20T11:43:01.458881",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.440615",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "876f2147-1ea0-4f45-8eb2-722761b5394c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.490969Z",
     "iopub.status.busy": "2024-03-20T11:43:01.490804Z",
     "iopub.status.idle": "2024-03-20T11:43:01.492454Z",
     "shell.execute_reply": "2024-03-20T11:43:01.492312Z"
    },
    "papermill": {
     "duration": 0.018335,
     "end_time": "2024-03-20T11:43:01.493014",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.474679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abstract',\n",
       "  'Current-generation Large Language Models (LLMs) have stirred enormous interest in recent months, yielding great potential for accessibility and automation, while simultaneously posing significant challenges and risk of misuse. To facilitate interfacing with LLMs in the biomedical space, while at the same time safeguarding their functionalities through sensible constraints, we propose a dedicated, open-source framework: BioChatter. Based on open-source software packages, we synergise the many functionalities that are currently developing around LLMs, such as knowledge integration / retrieval-augmented generation, model chaining, and benchmarking, resulting in an easy-to-use and inclusive framework for application in many use cases of biomedicine. We focus on robust and user-friendly implementation, including ways to deploy privacy-preserving local open-source LLMs. We demonstrate use cases via two multi-purpose web apps ([https://chat.biocypher.org](https://chat.biocypher.org)), and provide documentation, support, and an open community.',\n",
       "  'Large Language Models (LLMs) have generated significant interest due to their potential for accessibility and automation in various fields, including biomedicine. However, they also present challenges and risks of misuse. In this paper, we address the need for a framework to interface with LLMs in the biomedical domain while ensuring their safe and effective use. To meet this need, we introduce BioChatter, an open-source framework that integrates various functionalities of LLMs, such as knowledge integration, retrieval-augmented generation, model chaining, and benchmarking. By leveraging open-source software packages, we have developed a user-friendly and versatile platform that can be applied across a range of biomedicine use cases. Our focus is on implementing robust and privacy-preserving local open-source LLMs. We showcase the utility of BioChatter through two multi-purpose web apps available at [https://chat.biocypher.org](https://chat.biocypher.org) and provide comprehensive documentation, support, and a vibrant open community.'),\n",
       " ('introduction',\n",
       "  'Despite technological advances, understanding biological and biomedical systems still poses major challenges [@gallagher-infinite;@dl-bioscience]. We measure more and more data points with ever-increasing resolution to such a degree that their analysis and interpretation have become the bottleneck for their exploitation [@dl-bioscience]. One reason for this challenge may be the inherent limitation of human knowledge [@doi:10.1016/j.tics.2005.04.010]: Even seasoned domain experts cannot know the implications of every gene, molecule, symptom, or biomarker. In addition, biological events are context-dependent, for instance with respect to a cell type or specific disease.',\n",
       "  'Despite technological advances, understanding biological and biomedical systems continues to present significant challenges (Gallagher et al., 2020; DL Bioscience, 2019). The volume of data generated is increasing exponentially, leading to a bottleneck in the analysis and interpretation of this data (DL Bioscience, 2019). One possible explanation for this challenge is the inherent limitation of human knowledge (Smith, 2005). Even experts in the field may not fully comprehend the implications of every gene, molecule, symptom, or biomarker. Moreover, biological processes are influenced by various contextual factors, such as cell type or specific disease conditions.')]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_matches[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "77d20807-9017-4e10-8cb7-6a3c706e9111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.525328Z",
     "iopub.status.busy": "2024-03-20T11:43:01.525198Z",
     "iopub.status.idle": "2024-03-20T11:43:01.526901Z",
     "shell.execute_reply": "2024-03-20T11:43:01.526756Z"
    },
    "papermill": {
     "duration": 0.018399,
     "end_time": "2024-03-20T11:43:01.527385",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.508986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(paragraph_matches).rename(\n",
    "    columns={\n",
    "        0: \"section\",\n",
    "        1: \"original\",\n",
    "        2: \"modified\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "44e31258-2754-49a0-af7a-5f3b4ef3a961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.559627Z",
     "iopub.status.busy": "2024-03-20T11:43:01.559467Z",
     "iopub.status.idle": "2024-03-20T11:43:01.560983Z",
     "shell.execute_reply": "2024-03-20T11:43:01.560851Z"
    },
    "papermill": {
     "duration": 0.018204,
     "end_time": "2024-03-20T11:43:01.561473",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.543269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 3)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f7612c30-e726-474f-8039-aa4bc44d1f16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.593898Z",
     "iopub.status.busy": "2024-03-20T11:43:01.593731Z",
     "iopub.status.idle": "2024-03-20T11:43:01.598882Z",
     "shell.execute_reply": "2024-03-20T11:43:01.598717Z"
    },
    "papermill": {
     "duration": 0.02203,
     "end_time": "2024-03-20T11:43:01.599408",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.577378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>original</th>\n",
       "      <th>modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abstract</td>\n",
       "      <td>Current-generation Large Language Models (LLMs...</td>\n",
       "      <td>Large Language Models (LLMs) have generated si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>introduction</td>\n",
       "      <td>Despite technological advances, understanding ...</td>\n",
       "      <td>Despite technological advances, understanding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>introduction</td>\n",
       "      <td>Large Language Models (LLMs) of the current ge...</td>\n",
       "      <td>The latest generation of Large Language Models...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>introduction</td>\n",
       "      <td>Computational biomedicine involves many tasks ...</td>\n",
       "      <td>Computational biomedicine encompasses various ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>results</td>\n",
       "      <td>The framework is designed to be modular: any o...</td>\n",
       "      <td>The framework is designed to be modular, allow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        section                                           original  \\\n",
       "0      abstract  Current-generation Large Language Models (LLMs...   \n",
       "1  introduction  Despite technological advances, understanding ...   \n",
       "2  introduction  Large Language Models (LLMs) of the current ge...   \n",
       "3  introduction  Computational biomedicine involves many tasks ...   \n",
       "4       results  The framework is designed to be modular: any o...   \n",
       "\n",
       "                                            modified  \n",
       "0  Large Language Models (LLMs) have generated si...  \n",
       "1  Despite technological advances, understanding ...  \n",
       "2  The latest generation of Large Language Models...  \n",
       "3  Computational biomedicine encompasses various ...  \n",
       "4  The framework is designed to be modular, allow...  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e072fbf8-91bd-4684-8782-9ccb8d6ee565",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T11:43:01.632373Z",
     "iopub.status.busy": "2024-03-20T11:43:01.632255Z",
     "iopub.status.idle": "2024-03-20T11:43:01.633801Z",
     "shell.execute_reply": "2024-03-20T11:43:01.633650Z"
    },
    "papermill": {
     "duration": 0.018496,
     "end_time": "2024-03-20T11:43:01.634268",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.615772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_pickle(OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02304589-1223-48c7-9c0a-26bfa26392e7",
   "metadata": {
    "editable": true,
    "papermill": {
     "duration": 0.01606,
     "end_time": "2024-03-20T11:43:01.666450",
     "exception": false,
     "start_time": "2024-03-20T11:43:01.650390",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all,-execution,-papermill,-trusted",
   "notebook_metadata_filter": "-jupytext.text_representation.jupytext_version"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.934788,
   "end_time": "2024-03-20T11:43:02.000271",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/miltondp/projects/others/manubot/manubot-ai-editor-code/nbs/01-paragraph_match/00-biochatter-gpt35-par_match.ipynb",
   "output_path": "/home/miltondp/projects/others/manubot/manubot-ai-editor-code/nbs/01-paragraph_match/00-biochatter-gpt35-par_match.run.ipynb",
   "parameters": {
    "OUTPUT_FILE_PATH": "/home/miltondp/projects/others/manubot/manubot-ai-editor-code/base/results/paragraph_match/biochatter-manuscript--gpt-3.5-turbo.pkl"
   },
   "start_time": "2024-03-20T11:42:48.065483",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
